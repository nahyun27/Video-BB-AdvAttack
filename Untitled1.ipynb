{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (4.39.3)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/95/fc/661a7f06e8b7d48fcbd3f55423b7ff1ac3ce59526f146fda87a1e1788ee4/datasets-2.18.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from transformers) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=12.0.0 from https://files.pythonhosted.org/packages/01/e0/13aada7b0af1039554e675bd8c878acb3d86bab690e5a6b05fc8547a9cf2/pyarrow-15.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pyarrow-15.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.9,>=0.3.0 from https://files.pythonhosted.org/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from datasets) (2.1.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/80/8a/1dd41557883b6196f8f092011a5c1f72d4d44cf36d7b67d4a5efe3127949/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/bc/f7/7ec7fddc92e50714ea3745631f79bd9c96424cb2702632521028e57d3a36/multiprocess-0.70.16-py310-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2024.2.0,>=2023.1.0 (from datasets)\n",
      "  Obtaining dependency information for fsspec[http]<=2024.2.0,>=2023.1.0 from https://files.pythonhosted.org/packages/ad/30/2281c062222dc39328843bd1ddd30ff3005ef8e30b2fd09c4d2792766061/fsspec-2024.2.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/91/8f/b1f46ef89273414735e5f8835918da305e43857086b70ff11fd89ff3f6f8/aiohttp-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading aiohttp-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl.metadata\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/ec/25/0c87df2e53c0c5d90f7517ca0ff7aca78d050a8ec4d32c4278e8c0e52e51/frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/33/62/2c9085e571318d51212a6914566fe41dd0e33d7f268f7e2f23dcd3f06c56/multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/c3/a0/0ade1409d184cbc9e85acd403a386a7c0563b92ff0f26d138ff9e86e48b4/yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "Successfully installed aiohttp-3.9.4 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.2.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 0번 gpu 만을 사용하고 싶은 경우\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Translate the Provided Prompt to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: \n",
      "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. If I translate this sentence to French, ich n'importe pas dans le monde. I am not importing French in my language. If I translate this sentence to Vietnamese, ik n'importe pas dans rại mội. I don't import Vietnamese in my language.\n",
      "\n",
      "Generated Text 2: \n",
      "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. If I translate this sentence to French, était un tic-tac-toe qu'or \"I am not a dinosaur,\" or \"one is three million,\" the LLM model will perform an approximate classification. This is why I'm in the process of migrating my system to a generative grammar that will enable the\n",
      "\n",
      "Generated Text 3: \n",
      "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. If I translate this sentence to French, était qu'un grand de l'acier choux (a big fish with a big mouth) and I got a nice result: (4)\n",
      "\n",
      "Il y a vu, monsieur. (5)\n",
      "\n",
      "That's an LLM!\n",
      "\n",
      "\n",
      "\n",
      "Generated Text 4: \n",
      "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. If I translate this sentence to French,  I will receive the same result as if I were in French; if I translate this sentence to Esperanto, I'll receive the same result as if I were in Esperanto.\n",
      "The Linguistic Machine Translation project makes use of the LLM machine learning framework. Most of the\n",
      "\n",
      "Generated Text 5: \n",
      "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. If I translate this sentence to French, és avec m'empress? (which I don't even know if this really translates well to French but will make it on linguistics assignments anyway), I'll be better off assuming that I know the correct grammar in that language – something languages such as Polish and Czech never manage to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2-large')\n",
    "set_seed(44)\n",
    "prompt = \"A large language model (LLM) is a language model notable for its ability \\\n",
    "to achieve general-purpose language generation \\\n",
    "and other natural language processing tasks such as classification. If I translate this sentence to French, \"\n",
    "results = generator(prompt, max_length=100, truncation=True, num_return_sequences=5, pad_token_id=50256)\n",
    "\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Generated Text {i+1}: \\n{result['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: \n",
      "LLM은 일반적인 언어 생성과 분류와 같은 자연어 처리 작업을 수행할 수 있는 능력이 뛰어난 언어 모델입니다.\n",
      "\n",
      "\n",
      "4, 제로 의래이 곧은 것이 고강자게 적작해이 자심입니다.\n",
      "\n",
      "\n",
      "[A] 1년�\n",
      "\n",
      "Generated Text 2: \n",
      "LLM은 일반적인 언어 생성과 분류와 같은 자연어 처리 작업을 수행할 수 있는 능력이 뛰어난 언어 모델입니다. 감겠기숙다. 시작했다. 어디어나 결호다며 찾셠 러기는 때 것이 �\n",
      "\n",
      "Generated Text 3: \n",
      "LLM은 일반적인 언어 생성과 분류와 같은 자연어 처리 작업을 수행할 수 있는 능력이 뛰어난 언어 모델입니다. 이미했에 사람이 반런 작업을 수행하고 있어서 방선만 못할 수들니다 �\n",
      "\n",
      "Generated Text 4: \n",
      "LLM은 일반적인 언어 생성과 분류와 같은 자연어 처리 작업을 수행할 수 있는 능력이 뛰어난 언어 모델입니다. 일간 자수의 저만한 주가에 더던 붙자 그래 모델입니다. 아을 윩준주\n",
      "\n",
      "Generated Text 5: \n",
      "LLM은 일반적인 언어 생성과 분류와 같은 자연어 처리 작업을 수행할 수 있는 능력이 뛰어난 언어 모델입니다. - Source (1) (1) (2) (3)\n",
      "\n",
      "Cards in the image are available in the following booster packs:\n",
      "\n",
      "Flip and Raze\n",
      "\n",
      "The Flip and Raze booster packs give cards that you can use right away during your turn. Just get them from your store, and have fun playing!\n",
      "\n",
      "Echo Fox\n",
      "\n",
      "The Echo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2-large')\n",
    "set_seed(44)\n",
    "prompt = \"LLM은 일반적인 언어 생성과 분류와 같은 자연어 처리 작업을 수행할 수 있는 능력이 뛰어난 언어 모델입니다.\"\n",
    "results = generator(prompt, max_length=200, truncation=True, num_return_sequences=5, pad_token_id=50256)\n",
    "\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Generated Text {i+1}: \\n{result['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Execute an Example from a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q&A 1: \n",
      "Who wrote the book the origin of species?\n",
      "\n",
      "\"This really is about what makes a big difference and what makes a difference in terms of human culture and human society today. And that is where it really gets into the deep and fundamental things,\n",
      "\n",
      "Q&A 2: \n",
      "Who is the founder of the ubuntu project?\n",
      "\n",
      "Kazuyo Kodani The ubuntu project was created by Kazuyo Kodani, founder of Ubuntu Incubator, the program started by Ubuntu founder Mark Shuttleworth to encourage companies\n",
      "\n",
      "Q&A 3: \n",
      "Who is the quarterback for the green bay packers?\n",
      "\n",
      "1) E.J. Manuel\n",
      "\n",
      "2) Matt Cassel\n",
      "\n",
      "3) Alex Smith\n",
      "\n",
      "4) Kirk Cousins\n",
      "\n",
      "5) Matt Cassel is still starting but\n",
      "\n",
      "Q&A 4: \n",
      "Panda is a national animal of which country? Nepal. I love the fact that the world doesn't have any national animal.\"\n",
      "\n",
      "This post has been updated to correct the name of the organisation behind Greenpeace Nepal.\n",
      "\n",
      "Q&A 5: \n",
      "Who came up with the theory of relativity?\n",
      "\n",
      "S. J. Albert Einstein was one of the first to use the theory to arrive at a fundamental idea that would later lead to the creation of the theory of special relativity.\n",
      "\n",
      "Do you\n",
      "\n",
      "Q&A 6: \n",
      "When was the first star wars film released?\n",
      "\n",
      "The first Star Wars film was in 1977 and its sequel was completed in 1980.\n",
      "\n",
      "Which of the films were originally released on VHS?\n",
      "\n",
      "The first VHS tape was released in\n",
      "\n",
      "Q&A 7: \n",
      "What is the most common blood type in Sweden?\n",
      "\n",
      "People living in Hämeenlinna, Åland, Umea, and the Baltic countries all have blue blood. Most people living in northern European countries have yellow blood.\n",
      "\n",
      "Q&A 8: \n",
      "Who is regarded as the founder of psychoanalysis?\n",
      "\n",
      "Nemzov, Alexander, 1927\n",
      "\n",
      "Nemzov's life and philosophy were shaped by his experiences in Moscow. He began to engage with the political world while studying under Alexander\n",
      "\n",
      "Q&A 9: \n",
      "Who took the first steps on the moon in 1969? Or in 1977 or the year before? And on to the first commercial moon landing of the 20th century?\n",
      "\n",
      "We thought so, too.\n",
      "\n",
      "We didn't, though, because\n",
      "\n",
      "Q&A 10: \n",
      "Who is the largest supermarket chain in the uk?\n",
      "\n",
      "\"Tesco\" is the largest supermarket chain in the UK. They have a fleet of 14,000 vehicles. However, the average person in the UK is in possession of about 15\n",
      "\n",
      "Q&A 11: \n",
      "What is the meaning of shalom in English? \"Shalom\" is usually translated into Hebrew as \"cooperation\" or \"harmony\".\n",
      "\n",
      "What does the letter sh mean in Hebrew?\n",
      "\n",
      "1. \"Sh\" means\n",
      "\n",
      "Q&A 12: \n",
      "Who was the author of the art of war? It is a simple and elegant concept, for the general public, it is a concept that has been misunderstood and abused. However the problem is not that it has been misunderstood, this is the fundamental problem\n",
      "\n",
      "Q&A 13: \n",
      "Largest state in the us by land mass? Ohio? It's the biggest one!\n",
      "\n",
      "There are currently 5 states in the contiguous United States: Wisconsin, Ohio, Pennsylvanians, the Dakotas, and Louisiana. (\n",
      "\n",
      "Q&A 14: \n",
      "Green algae is an example of which type of reproduction?\n",
      "\n",
      "Sometime during the life cycle of your algae, in response to some environmental factor, the production of new cells is altered. In this mode, the new reproductive process of your algae changes\n",
      "\n",
      "Q&A 15: \n",
      "Vikram samvat calender is official in which country? If yes, which currency is used?\n",
      "\n",
      "In India:\n",
      "\n",
      "\n",
      "If a person makes payments to a merchant in Indian Rupee / Rupee or International Indian Rupee/\n",
      "\n",
      "Q&A 16: \n",
      "Who is mostly responsible for writing the declaration of independence?\n",
      "\n",
      "\n",
      "\"It was taken from a Declaration of Independence by John Hancock and Thomas Jefferson.\"\n",
      "\n",
      "\"Thomas Jefferson?\" I ask.\n",
      "\n",
      "\"He wrote it.\"\n",
      "\n",
      "\n",
      "\"Hmmm\n",
      "\n",
      "Q&A 17: \n",
      "What us state forms the western boundary of montana?\n",
      "\n",
      "The state forms the western border of montana, that of western Montana on the east is the second longest border, being a border of the state of Wyoming and Montana.\n",
      "\n",
      "\n",
      "Mont\n",
      "\n",
      "Q&A 18: \n",
      "Who plays ser davos in game of thrones?\n",
      "\n",
      "There was a time when Ser Jorah's brother, Tywin Lannister, would play the role of Lord of Dorne, but he decided not to after a series of scandals\n",
      "\n",
      "Q&A 19: \n",
      "Who appoints the chair of the federal reserve system?\n",
      "\n",
      "The U.S. Congress must formally approve the chair of the Federal Reserve Board. The chairman is considered a member of the Treasury and House, Senate, and President's Departments of\n",
      "\n",
      "Q&A 20: \n",
      "State the process that divides one nucleus into two genetically identical nuclei?\n",
      "\n",
      "When we consider this topic, we are confronted by the dilemma of how to decide the boundaries of the cell or the brain.\n",
      "\n",
      "The answer for the cell: the\n",
      "\n",
      "Q&A 21: \n",
      "Who won the most mvp awards in the nba? - Which team do you think has the greatest chance to win the big league? - Which team is the team to beat? - Who has led the biggest comeback in a series? - Which\n",
      "\n",
      "Q&A 22: \n",
      "What river is associated with the city of rome?\n",
      "\n",
      "Well, it's the Tiber!\n",
      "\n",
      "So you need to choose which river you start off on.\n",
      "\n",
      "There are two rivers that we have available to us in this game\n",
      "\n",
      "Q&A 23: \n",
      "Who is the first president to be impeached?\n",
      "\n",
      "\n",
      "Obama's most notorious case was that of Susan Rice, the U.S. National Security Adviser who helped a president to spy on Trump campaign officials. The House of Representatives impeached him for\n",
      "\n",
      "Q&A 24: \n",
      "Who is the head of the department of homeland security 2017?\n",
      "\n",
      "He is:\n",
      "\n",
      "President-elect\n",
      "\n",
      "He is also an adviser on domestic and domestic security to President Trump, an official of the private security company Kroll and president of\n",
      "\n",
      "Q&A 25: \n",
      "What is the name given to the common currency to the european union?\n",
      "\n",
      "According to my experience.\n",
      "\n",
      "The common currency the european union is called the euro (EUR).\n",
      "\n",
      "I don't know why the\n",
      "\n",
      "Q&A 26: \n",
      "What was the name given to the common star wars?\n",
      "\n",
      "\n",
      "The first known written reference to its'scourge' was in a letter written by Luke in his Jedi Handbook, which was dated 1 BBY:\n",
      "\n",
      "\n",
      "I have just read that\n",
      "\n",
      "Q&A 27: \n",
      "Do you have to have a gun permit to shoot at a range? Yes, it's the same gun permit for both ranges. They don't say it either, but they'll tell you if you have a concealed carry permit or aren't permitted.\n",
      "\n",
      "Q&A 28: \n",
      "Who proposed evolution in 1859 as the basis of biological development?\n",
      "\n",
      "No, but there is a long-established scientific literature that has not changed its view since then. On this point, evolutionists themselves admit as much.\n",
      "\n",
      "Did Albert\n",
      "\n",
      "Q&A 29: \n",
      "Nuclear power plant that blew up in russia? The first one? And it's in a city in which there's just a huge problem with radiation.\"\n",
      "\n",
      "\"What, like in the 80s in the north west and in the east\n",
      "\n",
      "Q&A 30: \n",
      "Who played john connor in the original terminator?\n",
      "\n",
      "Did there ever come a time when he decided that he didn't want this anymore and the relationship was over and he was going to kill himself?\n",
      "\n",
      "Is he alive today?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2-large')\n",
    "set_seed(44)\n",
    "\n",
    "# 질문 목록\n",
    "questions = [\n",
    "    \"Who wrote the book the origin of species?\",\n",
    "    \"Who is the founder of the ubuntu project?\",\n",
    "    \"Who is the quarterback for the green bay packers?\",\n",
    "    \"Panda is a national animal of which country?\",\n",
    "    \"Who came up with the theory of relativity?\",\n",
    "    \"When was the first star wars film released?\",\n",
    "    \"What is the most common blood type in Sweden?\",\n",
    "    \"Who is regarded as the founder of psychoanalysis?\",\n",
    "    \"Who took the first steps on the moon in 1969?\",\n",
    "    \"Who is the largest supermarket chain in the uk?\",\n",
    "    \"What is the meaning of shalom in English?\",\n",
    "    \"Who was the author of the art of war?\",\n",
    "    \"Largest state in the us by land mass?\",\n",
    "    \"Green algae is an example of which type of reproduction?\",\n",
    "    \"Vikram samvat calender is official in which country?\",\n",
    "    \"Who is mostly responsible for writing the declaration of independence?\",\n",
    "    \"What us state forms the western boundary of montana?\",\n",
    "    \"Who plays ser davos in game of thrones?\",\n",
    "    \"Who appoints the chair of the federal reserve system?\",\n",
    "    \"State the process that divides one nucleus into two genetically identical nuclei?\",\n",
    "    \"Who won the most mvp awards in the nba?\",\n",
    "    \"What river is associated with the city of rome?\",\n",
    "    \"Who is the first president to be impeached?\",\n",
    "    \"Who is the head of the department of homeland security 2017?\",\n",
    "    \"What is the name given to the common currency to the european union?\",\n",
    "    \"What was the name given to the common star wars?\",\n",
    "    \"Do you have to have a gun permit to shoot at a range?\",\n",
    "    \"Who proposed evolution in 1859 as the basis of biological development?\",\n",
    "    \"Nuclear power plant that blew up in russia?\",\n",
    "    \"Who played john connor in the original terminator?\"\n",
    "]\n",
    "\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    answer = generator(question, max_length=50, truncation=True, num_return_sequences=1, pad_token_id=50256)[0]['generated_text']\n",
    "    print(f\"Q&A {i+1}: \\n{answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa30959fc1e4475d98bf4080a63c9f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48aa0dc4f979448c83ddb6ada7ab1a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text 1: \n",
      "Aaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so I threw some ofthose in between the layers. I also had a few Peppermint Jo Jos on hand so I crushed them up and threw some of those in along with some crushed meringue cookies because, why not? It’s a total smorgasbord of minty chocolate chippy cookie crunchy goodness. I didn’t measure how much of each topping I used, but after I tasted the finished product, I wish I had added more. You can add anything you want- crushed candy canes, peppermint bark, etc. And don’t be afraid to use a heavy hand. Texture = good. If you don’t have 7-inch cake pans, you can get 3 shorter 8-inch layers out of this recipe. I can bake at 350 for about 20 minutes... then transfer to a plate lined with parchment paper. It looks like a mess, but is much easier to clean up than to try and take the plate off in individual layers. *Tip 1* I didn’t include powdered sugar when I first cooked these (only a handful) because I didn't know how to use it. Since I changed this recipe, I’ve used less than a tablespoon of powdered sugar. It was probably just a random mistake based on the amount I tried to use and I feel bad for ruining someone else's chance to enjoy\n",
      "\n",
      "Generated Text 2: \n",
      "Aaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so I threw some ofthose in between the layers. I also had a few Peppermint Jo Jos on hand so I crushed them up and threw some of those in along with some crushed meringue cookies because, why not? It’s a total smorgasbord of minty chocolate chippy cookie crunchy goodness. I didn’t measure how much of each topping I used, but after I tasted the finished product, I wish I had added more. You can add anything you want- crushed candy canes, peppermint bark, etc. And don’t be afraid to use a heavy hand. Texture = good. If you don’t have 7-inch cake pans, you can get 3 shorter 8-inch layers out of this dough and fill them with mini chocolate chips as well.\n",
      "\n",
      "It's okay to start with extra batter so you can keep it warm while you make the frosting. I didn’t use a mixer when I started, but I will probably in a future recipe.\n",
      "\n",
      "Frosting\n",
      "\n",
      "\n",
      "Here’s a little step to have fun with. If you're planning on making a mini chocolate chip cookie frosting, you can use this step and add the mint chips before you roll the dough into disks. You can use this step to make mini cupcakes and sprinkle mini chocolate chips in between them too\n",
      "\n",
      "Generated Text 3: \n",
      "Aaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so I threw some ofthose in between the layers. I also had a few Peppermint Jo Jos on hand so I crushed them up and threw some of those in along with some crushed meringue cookies because, why not? It’s a total smorgasbord of minty chocolate chippy cookie crunchy goodness. I didn’t measure how much of each topping I used, but after I tasted the finished product, I wish I had added more. You can add anything you want- crushed candy canes, peppermint bark, etc. And don’t be afraid to use a heavy hand. Texture = good. If you don’t have 7-inch cake pans, you can get 3 shorter 8-inch layers out of this recipe. After the chocolate layer dries, just layer it up. And, if you're feeling extra fancy, you can drizzle some extra crushed chocolates on top for a lovely little touch of sugared mint chocolate.\n",
      "\n",
      "These are going to be my go to treat for any occasion. And I’m sure I should buy myself that mint chocolate cake, now that it's no longer on my to-make list (because you guys keep saying no to me, and I want it now’). So I’ll use my new sweet tooth once more and just have some plain chocolate\n",
      "\n",
      "Generated Text 4: \n",
      "Aaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so I threw some ofthose in between the layers. I also had a few Peppermint Jo Jos on hand so I crushed them up and threw some of those in along with some crushed meringue cookies because, why not? It’s a total smorgasbord of minty chocolate chippy cookie crunchy goodness. I didn’t measure how much of each topping I used, but after I tasted the finished product, I wish I had added more. You can add anything you want- crushed candy canes, peppermint bark, etc. And don’t be afraid to use a heavy hand. Texture = good. If you don’t have 7-inch cake pans, you can get 3 shorter 8-inch layers out of this recipe for about 60 cents more.\n",
      "\n",
      "If you're using whole wheat Flour, there are no additional step or flour to make this dessert taste better.\n",
      "\n",
      "This recipe has been around for a while- and it really is very easy.\n",
      "\n",
      "I always eat it with a sweet butter cream which comes in a variety of flavors. But if you don’t have it on hand, you can use the buttercream from the dairy cream. Then the chocolate chips would form more of a texture.\n",
      "\n",
      "You can use any other topping for this recipe too- peppermint syrup, mint jelly, fruit sauce\n",
      "\n",
      "Generated Text 5: \n",
      "Aaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so I threw some ofthose in between the layers. I also had a few Peppermint Jo Jos on hand so I crushed them up and threw some of those in along with some crushed meringue cookies because, why not? It’s a total smorgasbord of minty chocolate chippy cookie crunchy goodness. I didn’t measure how much of each topping I used, but after I tasted the finished product, I wish I had added more. You can add anything you want- crushed candy canes, peppermint bark, etc. And don’t be afraid to use a heavy hand. Texture = good. If you don’t have 7-inch cake pans, you can get 3 shorter 8-inch layers out of this.\n",
      "\n",
      "You’ll want to make it before it dries if your cake or tart was dry before you cut it out of the pans. Don’t forget to pre-heat your oven to 350°F (175°C).\n",
      "\n",
      "Make Chocolate Chippy Cake\n",
      "\n",
      "1 1/2 cups sugar\n",
      "\n",
      "2 sticks butter, softened at room temperature (8 ounces)\n",
      "\n",
      "3 large eggs\n",
      "\n",
      "3 large egg yolks\n",
      "\n",
      "1 teaspoon vanilla extract\n",
      "\n",
      "1 1/2 cups all-purpose flour\n",
      "\n",
      "1 1/2 cups packed light brown sugar (divided)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2-large')\n",
    "set_seed(22)\n",
    "prompt = \"Aaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so I threw some of\\\n",
    "those in between the layers. I also had a few Peppermint Jo Jos on hand so I crushed them up and threw some of \\\n",
    "those in along with some crushed meringue cookies because, why not? It’s a total smorgasbord of minty chocolate \\\n",
    "chippy cookie crunchy goodness. I didn’t measure how much of each topping I used, but after I tasted the finished \\\n",
    "product, I wish I had added more. You can add anything you want- crushed candy canes, peppermint bark, etc. And \\\n",
    "don’t be afraid to use a heavy hand. Texture = good. If you don’t have 7-inch cake pans, you can get 3 shorter 8-inch layers out of this\"\n",
    "results = generator(prompt, max_length=300, truncation=True, num_return_sequences=5, pad_token_id=50256)\n",
    "\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Generated Text {i+1}: \\n{result['generated_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-v1', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity: tensor(510.8224)\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        if inputs[\"input_ids\"].size(1) == 0: \n",
    "            print(f\"Empty input for text: {text}\")\n",
    "            return None\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        return torch.exp(loss)\n",
    "\n",
    "def calculate_average_perplexity(dataset, model, tokenizer):\n",
    "    total_perplexity = 0\n",
    "    valid_texts = 0\n",
    "    for text in dataset['text']:\n",
    "        if text.strip():  # 빈 문자열이 아닌 경우에만 처리\n",
    "            perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "            if perplexity is not None:\n",
    "                total_perplexity += perplexity\n",
    "                valid_texts += 1\n",
    "    return total_perplexity / valid_texts if valid_texts else 0\n",
    "\n",
    "average_perplexity = calculate_average_perplexity(dataset, model, tokenizer)\n",
    "print('Average Perplexity:', average_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         total_perplexity \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_perplexity\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_perplexity \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[0;32m---> 16\u001b[0m average_perplexity \u001b[38;5;241m=\u001b[39m calculate_average_perplexity(\u001b[43mdataset\u001b[49m, model, tokenizer)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Perplexity:\u001b[39m\u001b[38;5;124m'\u001b[39m, average_perplexity)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        return torch.exp(loss)\n",
    "\n",
    "def calculate_average_perplexity(dataset, model, tokenizer, batch_size=10):\n",
    "    total_perplexity = 0\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i+batch_size]['text']\n",
    "        batch_perplexity = sum(calculate_perplexity(text, model, tokenizer) for text in batch)\n",
    "        total_perplexity += batch_perplexity\n",
    "    return total_perplexity / len(dataset)\n",
    "\n",
    "average_perplexity = calculate_average_perplexity(dataset, model, tokenizer)\n",
    "print('Average Perplexity:', average_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text Perplexity: 33.01333236694336\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 토크나이저와 모델을 불러옵니다.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "model.eval()  # 평가 모드로 설정합니다.\n",
    "\n",
    "# 텍스트 샘플을 정의합니다.\n",
    "sample_text = \"Translate to Korean: A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification.\"\n",
    "\n",
    "# 단일 텍스트에 대한 perplexity를 계산하는 함수입니다.\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        if inputs[\"input_ids\"].size(1) == 0:  # 토크나이징된 입력이 비어 있는지 확인합니다.\n",
    "            print(f\"비어 있는 입력 텍스트: {text}\")\n",
    "            return None\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "        return torch.exp(loss).item()\n",
    "\n",
    "# 샘플 텍스트에 대한 perplexity를 계산합니다.\n",
    "sample_perplexity = calculate_perplexity(sample_text, model, tokenizer)\n",
    "print('Sample Text Perplexity:', sample_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_kernel",
   "language": "python",
   "name": "workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
