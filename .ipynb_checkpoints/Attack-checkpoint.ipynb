{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오류 발생: Command '['ffmpeg', '-i', '/data_1/seclab_nahyun/UCF101_subset/train/SkyDiving/v_SkyDiving_g17_c05.avi', '-c:v', 'mpeg4', '-f', 'rawvideo', '-y', './resized_video.mp4']' returned non-zero exit status 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "/data_1/seclab_nahyun/UCF101_subset/train/SkyDiving/v_SkyDiving_g17_c05.avi: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "def convert_resize_video(input_path, output_path, target_resolution=(256, 256), bitrate='1000k'):\n",
    "    try:\n",
    "        # 임시 파일 경로 설정\n",
    "        temp_mp4_path = 'temp.mp4'\n",
    "\n",
    "        # Step 1: .avi를 .mp4로 변환\n",
    "        cmd1 = [\n",
    "            'ffmpeg',\n",
    "            '-i', input_path,       \n",
    "            # 입력 파일 경로\n",
    "            '-c:v', 'mpeg4',          # 비디오 코덱 설정 \n",
    "            '-f', 'rawvideo',          \n",
    "#             '-b:v', bitrate,            # 비트레이트 설정\n",
    "            '-y',                       # 덮어쓰기 허용\n",
    "            output_path\n",
    "#             temp_mp4_path                # 임시 출력 파일 경로 (.mp4)\n",
    "        ]\n",
    "\n",
    "        # FFmpeg 명령어 실행 (Step 1)\n",
    "        subprocess.run(cmd1, check=True)\n",
    "\n",
    "#         # Step 2: 리사이징 및 재인코딩\n",
    "#         cmd2 = [\n",
    "#             'ffmpeg',\n",
    "#             '-i', temp_mp4_path,                 # 입력 파일 경로 (임시 .mp4 파일)\n",
    "#             '-vf', f'scale={target_resolution[0]}:{target_resolution[1]}',  # 리사이징\n",
    "#             '-c:v', 'mpeg4',                   # 비디오 코덱 설정 \n",
    "#             '-f', 'rawvideo',          \n",
    "# #             '-b:v', bitrate,                     # 비트레이트 설정\n",
    "#             '-y',                                # 덮어쓰기 허용\n",
    "#             output_path                          # 출력 파일 경로\n",
    "#         ]\n",
    "\n",
    "#         # FFmpeg 명령어 실행 (Step 2)\n",
    "#         subprocess.run(cmd2, check=True)\n",
    "\n",
    "#         # 임시 파일 삭제\n",
    "#         subprocess.run(['rm', temp_mp4_path])\n",
    "\n",
    "        print(f'동영상 변환 및 리사이징이 완료되었습니다. 저장 경로: {output_path}')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f'오류 발생: {e}')\n",
    "    except Exception as e:\n",
    "        print(f'알 수 없는 오류 발생: {e}')\n",
    "\n",
    "# 사용 예시\n",
    "input_video_path = '/data_1/seclab_nahyun/UCF101_subset/train/SkyDiving/v_SkyDiving_g17_c05.avi'\n",
    "output_video_path = './resized_video.mp4'\n",
    "convert_resize_video(input_video_path, output_video_path)\n",
    "# !ls ./~/jupyter_notebook/UCF101_subset/train/SkyDiving/v_SkyDiving_g17_c05.avi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "import os\n",
    "# 0번 gpu 만을 사용하고 싶은 경우\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('./dist/coviar-0.1-py3.9-linux-x86_64.egg')\n",
    "from coviar import load, get_num_frames\n",
    "import os\n",
    "\n",
    "def visualize_motion_vector(mv, stride=20):\n",
    "    Y, X, _ = mv.shape\n",
    "    x, y = np.meshgrid(np.arange(X), np.arange(Y))\n",
    "    u = mv[..., 0]\n",
    "    v = mv[..., 1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.quiver(x[::stride, ::stride], y[::stride, ::stride], u[::stride, ::stride], v[::stride, ::stride], angles='xy', scale_units='xy', scale=1, color='r')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "def compute_motion_vectors_and_weight_maps(video_path, GOP_SIZE, height, width, num_frame):\n",
    "    representation_type = 1  # 모션 벡터를 위한 값\n",
    "    accumulate = True  # 누적된 표현을 위한 값\n",
    "\n",
    "    \"\"\"\n",
    "    비디오에서 모션 벡터와 가중치 맵을 계산하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: 비디오 파일의 경로\n",
    "    - GOP_SIZE: GOP의 크기\n",
    "\n",
    "    Returns:\n",
    "    - all_motion_vectors: 각 프레임의 모션 벡터 텐서\n",
    "    - weight_maps: 각 GOP의 가중치 맵 텐서\n",
    "    \"\"\"\n",
    "\n",
    "    total_frames = num_frame\n",
    "#     print('total_frames!', total_frames)\n",
    "#     print('video_path!', video_path, height, width)\n",
    "    \n",
    "    if num_frame % GOP_SIZE == 0:\n",
    "        num_iframe = num_frame // GOP_SIZE\n",
    "    else:\n",
    "        num_iframe = num_frame // GOP_SIZE + 1\n",
    "    \n",
    "    # 각 프레임에 대한 가중치 맵과 모션 벡터 텐서 초기화\n",
    "    weight_maps_tensor = torch.zeros((num_iframe, 3, height, width))\n",
    "    all_motion_vectors_tensor = torch.zeros((total_frames, height, width, 2))\n",
    "    gop_start = 0\n",
    "    frame_index = 0\n",
    "\n",
    "    for gop_start in range(0, total_frames, GOP_SIZE):\n",
    "        weight_map = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "        for offset in range(1, GOP_SIZE):  # I 프레임 제외\n",
    "            frame_index = gop_start + offset\n",
    "            \n",
    "            if frame_index >= total_frames:\n",
    "                break\n",
    "            \n",
    "#             print('frame_index:', frame_index)\n",
    "            try:\n",
    "#                 print(video_path, frame_index // GOP_SIZE, frame_index % GOP_SIZE, representation_type, accumulate)\n",
    "                mv = load(video_path, frame_index // GOP_SIZE, frame_index % GOP_SIZE, representation_type, accumulate)\n",
    "#                 visualize_motion_vector(mv)\n",
    "                # mv의 유효성 확인\n",
    "                if mv is None or mv.size == 0:\n",
    "                    raise ValueError(f\"Invalid motion vector data at frame {frame_index}\")\n",
    "\n",
    "                # mv의 각 채널을 float32로 변환하고 개별적으로 리사이즈\n",
    "                mv_x = cv2.resize(mv[..., 0].astype(np.float32), (height, width))\n",
    "                mv_y = cv2.resize(mv[..., 1].astype(np.float32), (height, width))\n",
    "\n",
    "                # 리사이즈된 채널들을 다시 합치기\n",
    "                mv = np.stack([mv_x, mv_y], axis=-1)\n",
    "\n",
    "                # 모션 벡터의 크기 (움직임의 거리)를 계산하고 가중치 맵에 누적합니다.\n",
    "                magnitude = np.sqrt(mv[:,:,0]**2 + mv[:,:,1]**2)\n",
    "                weight_map += magnitude\n",
    "\n",
    "                # 모션 벡터 저장\n",
    "                all_motion_vectors_tensor[frame_index] = torch.tensor(mv)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred at frame {frame_index}:\", e)\n",
    "                mv = np.zeros((height, width, 2), dtype=np.int32)  # 0벡터 할당\n",
    "\n",
    "        # RGB 채널 각각에 동일한 가중치 맵을 복사\n",
    "        for channel in range(3):\n",
    "            weight_maps_tensor[gop_start // GOP_SIZE, channel] = torch.tensor(weight_map)\n",
    "    \n",
    "#     print('weight_maps_tensor', weight_maps_tensor.shape, 'all_motion_vectors_tensor', all_motion_vectors_tensor.shape)\n",
    "    \n",
    "#     visualize_weight_map(weight_maps_tensor)\n",
    "#     visualize_motion_vectors(all_motion_vectors_tensor)\n",
    "    return all_motion_vectors_tensor, weight_maps_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def perform_segmentation_with_interval(segmentation_module, original_tensor, interval=12, device='cuda:1'):\n",
    "    \n",
    "    # Define the image transform\n",
    "    transform_img = T.Compose([\n",
    "        T.Resize(512),\n",
    "        T.ToTensor(), \n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Ensure the tensor is on the GPU\n",
    "    original_tensor = original_tensor.to(device)\n",
    "\n",
    "    # Initialize the scores tensor on CPU\n",
    "    num_frames = original_tensor.size(0)\n",
    "    selected_frames = list(range(0, num_frames, interval))\n",
    "    scores = torch.zeros(len(selected_frames), 150, original_tensor.size(2), original_tensor.size(3))\n",
    "\n",
    "    # Perform segmentation for selected frames\n",
    "    with torch.no_grad():  # Use no_grad to save GPU memory\n",
    "        for i, frame_idx in enumerate(selected_frames):\n",
    "            img = original_tensor[frame_idx].cpu().numpy()\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            img = Image.fromarray(np.uint8(img))\n",
    "            img = img.convert('RGB')\n",
    "            segSize = (img.size[1], img.size[0])\n",
    "            \n",
    "            # 이미지를 텐서로 변환\n",
    "            img_transformed = transform_img(img)\n",
    "            img_transformed = img_transformed.unsqueeze(0).to(device)\n",
    "\n",
    "            feed_dict = {'img_data': img_transformed}\n",
    "            pred_tmp = segmentation_module(feed_dict, segSize=segSize)\n",
    "            scores[i] = pred_tmp\n",
    "\n",
    "    # Move the scores tensor to GPU if needed\n",
    "    scores = scores.to(device)\n",
    "\n",
    "    _, pred = torch.max(scores, dim=1)\n",
    "\n",
    "    return pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def create_mask_for_stuff_pixels(segmentation_tensor, color_encoding_path='./color_coding_semantic_segmentation_classes.csv'):\n",
    "    \"\"\"\n",
    "    Create a mask to remove pixels with 'Stuff' values in the segmentation tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - segmentation_tensor: A tensor of shape (num_frames, height, width) containing the segmentation results\n",
    "    - color_encoding_path: Path to the CSV file containing color encodings for semantic segmentation classes\n",
    "\n",
    "    Returns:\n",
    "    - masks: A tensor of shape (num_frames, height, width) containing the masks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the color encoding CSV\n",
    "    color_df = pd.read_csv(color_encoding_path) \n",
    "\n",
    "    # Extract the Idx values where Stuff is 1\n",
    "    stuff_pixel_values = color_df[color_df['Stuff'] == 1]['Idx'].tolist()\n",
    "\n",
    "    # Create a mask where pixels with stuff values are set to 0, others are set to 1\n",
    "    masks = np.ones_like(segmentation_tensor)\n",
    "    for value in stuff_pixel_values:\n",
    "        masks[segmentation_tensor == value - 1] = 0\n",
    "\n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    masks_tensor = torch.tensor(masks, dtype=torch.float32)\n",
    "\n",
    "    return masks_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def visualize_tensor_image(image_tensor):\n",
    "    \"\"\"\n",
    "    Visualize a given image tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - image_tensor: A tensor of shape [3, H, W]\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the tensor is on GPU, if yes, move it to CPU\n",
    "    if image_tensor.device != torch.device('cpu'):\n",
    "        image_tensor = image_tensor.cpu()\n",
    "\n",
    "    # Convert the tensor to numpy array\n",
    "    image_array = image_tensor.numpy()\n",
    "\n",
    "    # Transpose the array to shape [H, W, 3] for visualization\n",
    "    image_array = image_array.transpose(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image_array)\n",
    "    plt.axis('off')  # Hide axes for better visualization\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_weight_map(weight_maps_tensor, stride=5):\n",
    "    num_iframes, num_channels, height, width = weight_maps_tensor.shape\n",
    "    \n",
    "    cols = 7  # 한 행에 7개의 프레임\n",
    "    rows = math.ceil(num_iframes*num_channels / cols)  # 필요한 행의 수 계산\n",
    "\n",
    "    # figsize의 값을 조절하여 전체 그림의 크기를 늘립니다.\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))  # 각 프레임의 크기를 대략 3x3으로 설정\n",
    "\n",
    "    \n",
    "\n",
    "    for iframe in range(num_iframes):\n",
    "        for channel in range(num_channels):\n",
    "            plt.subplot(rows, cols, iframe * num_channels + channel + 1)\n",
    "            weight_map = weight_maps_tensor[iframe, channel].numpy()\n",
    "            plt.imshow(weight_map, cmap='viridis')\n",
    "            plt.title(f'I-Frame {iframe + 1}, Channel {channel + 1}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def visualize_masks(masks_tensor):\n",
    "    \"\"\"\n",
    "    Visualize the masks.\n",
    "\n",
    "    Parameters:\n",
    "    - masks_tensor: A tensor of shape (num_frames, height, width) containing the masks\n",
    "    \"\"\"\n",
    "    \n",
    "    num_frames, height, width = masks_tensor.shape\n",
    "    \n",
    "    cols = 7  # 한 행에 7개의 프레임\n",
    "    rows = math.ceil(num_frames / cols)  # 필요한 행의 수 계산\n",
    "\n",
    "    # figsize의 값을 조절하여 전체 그림의 크기를 늘립니다.\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))  # 각 프레임의 크기를 대략 3x3으로 설정\n",
    "\n",
    "    \n",
    "    for frame_idx in range(num_frames):\n",
    "        plt.subplot(rows, cols, frame_idx+1)\n",
    "        mask = masks_tensor[frame_idx].numpy()\n",
    "\n",
    "#         plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(mask, cmap='gray')  # You can choose a different colormap\n",
    "        plt.title(f'Mask - Frame {frame_idx}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_motion_vectors(motion_tensor):\n",
    "    \"\"\"\n",
    "    Visualize motion vectors for each GOP and frame.\n",
    "\n",
    "    Parameters:\n",
    "    - motion_tensor: A tensor of shape [num_frames, 3, H, W]\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(motion_tensor.shape)\n",
    "    num_frames, _, _, _ = motion_tensor.shape\n",
    "    \n",
    "    cols = 7  # 한 행에 7개의 프레임\n",
    "    rows = math.ceil(num_frames / cols)  # 필요한 행의 수 계산\n",
    "\n",
    "    # figsize의 값을 조절하여 전체 그림의 크기를 늘립니다.\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))  # 각 프레임의 크기를 대략 3x3으로 설정\n",
    "\n",
    "    stride = 20\n",
    "\n",
    "    # Check if the tensor is on GPU, if yes, move it to CPU\n",
    "    if motion_tensor.device != torch.device('cpu'):\n",
    "        motion_tensor = motion_tensor.cpu()\n",
    "\n",
    "    for frame_idx in range(num_frames):\n",
    "        mv = motion_tensor[frame_idx]  # .numpy()\n",
    "        Y, X, _ = mv.shape\n",
    "        x, y = np.meshgrid(np.arange(X), np.arange(Y))\n",
    "        u = mv[..., 0]\n",
    "        v = mv[..., 1]\n",
    "\n",
    "        plt.subplot(rows, cols, frame_idx + 1)\n",
    "        plt.quiver(x[::stride, ::stride], y[::stride, ::stride], u[::stride, ::stride], v[::stride, ::stride],\n",
    "                   angles='xy', scale_units='xy', scale=1, color='r')\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_motion_vector(mv, stride=20):\n",
    "    Y, X, _ = mv.shape\n",
    "    x, y = np.meshgrid(np.arange(X), np.arange(Y))\n",
    "    u = mv[..., 0]\n",
    "    v = mv[..., 1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.quiver(x[::stride, ::stride], y[::stride, ::stride], u[::stride, ::stride], v[::stride, ::stride], angles='xy', scale_units='xy', scale=1, color='r')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "# 최종 노이즈 시각화\n",
    "def visualize_warped_frames(warped_frames):\n",
    "    \"\"\"\n",
    "    Visualize the given warped frames (noise).\n",
    "\n",
    "    Parameters:\n",
    "    - warped_frames: Tensor of shape [num_frames, C, H, W]\n",
    "    \"\"\"\n",
    "    \n",
    "    num_frames, C, H, W = warped_frames.shape\n",
    "    \n",
    "    # Ensure the tensor is on CPU for visualization\n",
    "    warped_frames = warped_frames.cpu()\n",
    "\n",
    "    cols = 7  # 한 행에 7개의 프레임\n",
    "    rows = math.ceil(num_frames / cols)  # 필요한 행의 수 계산\n",
    "\n",
    "    # figsize의 값을 조절하여 전체 그림의 크기를 늘립니다.\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))  # 각 프레임의 크기를 대략 3x3으로 설정\n",
    "\n",
    "#     for idx in range(num_frames):\n",
    "    for idx in range(7):\n",
    "        plt.subplot(rows, cols, idx+1)\n",
    "        frame = warped_frames[idx].numpy()\n",
    "        \n",
    "        frame = warped_frames[idx].numpy()\n",
    "        \n",
    "        # Transpose the array to shape [H, W, 3] for visualization\n",
    "        frame = frame.transpose(1, 2, 0)\n",
    "\n",
    "        # Clip the values to the range [0, 1]\n",
    "        frame = np.clip(frame, 0, 1)\n",
    "\n",
    "        # Normalize the frame for visualization\n",
    "#         frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
    "        \n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Frame {idx}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def visualize_video_frames_tf(video_frames):\n",
    "    \"\"\"\n",
    "    동영상 프레임을 시각화하는 함수\n",
    "\n",
    "    Parameters:\n",
    "    - video_frames: 동영상 프레임의 텐서 (프레임수, 높이, 너비, 채널)\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    print(\"!!!!, \", video_frames.shape)\n",
    "    # 텐서를 넘파이 배열로 변환\n",
    "    frames_np = video_frames.numpy()\n",
    "\n",
    "    # 프레임수\n",
    "    num_frames = frames_np.shape[0]\n",
    "    cols = 7  # 한 행에 7개의 프레임\n",
    "    rows = math.ceil(num_frames / cols)  # 필요한 행의 수 계산\n",
    "\n",
    "    # figsize의 값을 조절하여 전체 그림의 크기를 늘립니다.\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))\n",
    "\n",
    "    # 각 프레임을 시각화\n",
    "#     for i in range(num_frames):\n",
    "    for i in range(7):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(frames_np[i])\n",
    "#         plt.title(f'프레임 {i+1}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "def save_video_frames_tf(video_frames, save_path='./adversarial_examples/'):\n",
    "    \"\"\"\n",
    "    동영상 프레임을 이미지 파일로 저장하는 함수\n",
    "\n",
    "    Parameters:\n",
    "    - video_frames: 동영상 프레임의 텐서 (프레임수, 높이, 너비, 채널)\n",
    "    - save_path: 이미지를 저장할 경로\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # 텐서를 넘파이 배열로 변환\n",
    "    frames_np = video_frames.numpy()\n",
    "    \n",
    "    \n",
    "    # 데이터 타입을 uint8로 변환\n",
    "    frames_np = (frames_np * 255).astype(np.uint8)\n",
    "\n",
    "    # 프레임수\n",
    "    num_frames = frames_np.shape[0]\n",
    "\n",
    "    # 저장 경로 확인 및 생성\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # 각 프레임을 이미지 파일로 저장\n",
    "    for i in range(num_frames):\n",
    "        frame_image = Image.fromarray(frames_np[i])\n",
    "        frame_image.save(os.path.join(save_path, f'frame_{i:03d}.png'))\n",
    "    \n",
    "    \n",
    "def visualize_video(video_array):\n",
    "    \"\"\"\n",
    "    주어진 numpy.ndarray 동영상을 시각화하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "    - video_array: (num_frame, height, width, 3) shape의 numpy.ndarray 동영상\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    num_frames, height, width, _ = video_array.shape\n",
    "\n",
    "    for frame_idx in range(num_frames):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(video_array[frame_idx])\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Frame {frame_idx + 1}/{num_frames}\")\n",
    "        plt.show()\n",
    "#         input(\"Press Enter to continue to the next frame...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Motion Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as FF\n",
    "\n",
    "def normalize_grid(grid):\n",
    "    # 그리드의 최대값과 최소값을 가져옵니다.\n",
    "    max_val = grid.max()\n",
    "    min_val = grid.min()\n",
    "\n",
    "    # 그리드 값을 [-1, 1] 범위로 정규화\n",
    "    normalized_grid = 2 * (grid - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "    return normalized_grid\n",
    "\n",
    "# 이 함수는 exp_noise를 각 GOP의 첫 번째 프레임에 설정하고, 나머지 프레임들에 대해 해당 프레임의 모션 벡터를 사용하여 노이즈를 변형합니다. \n",
    "# 결과적으로, 각 GOP의 모든 프레임에 대한 노이즈가 생성됩니다.\n",
    "def apply_motion_vector_to_iframe_tensor(iframe, motion_vector, scale_factor, grid_density):\n",
    "    C, H, W = iframe.shape\n",
    "#     print(iframe.shape, motion_vector.shape)\n",
    "#     visualize_tensor_image(iframe)\n",
    "\n",
    "    # Create a dense grid\n",
    "    x = torch.linspace(0, W-1, W*grid_density).unsqueeze(0).repeat(H*grid_density, 1)\n",
    "    y = torch.linspace(0, H-1, H*grid_density).unsqueeze(1).repeat(1, W*grid_density)\n",
    "\n",
    "    # Resize motion vectors to match the dense grid\n",
    "    mv_x = FF.interpolate(motion_vector[..., 0].unsqueeze(0).unsqueeze(0), size=(H*grid_density, W*grid_density), mode='bilinear', align_corners=True).squeeze()\n",
    "    mv_y = FF.interpolate(motion_vector[..., 1].unsqueeze(0).unsqueeze(0), size=(H*grid_density, W*grid_density), mode='bilinear', align_corners=True).squeeze()\n",
    "\n",
    "    # Apply motion vectors to the grid\n",
    "    x_new = x - scale_factor * mv_x\n",
    "    y_new = y - scale_factor * mv_y\n",
    "\n",
    "    grid = torch.stack((x_new, y_new), dim=2).unsqueeze(0)\n",
    "    \n",
    "    # 그리드 값을 확인하고 필요한 경우 정규화\n",
    "#     print(\"Before normalization:\", grid.min(), grid.max())\n",
    "    grid = normalize_grid(grid)\n",
    "#     print(\"After normalization:\", grid.min(), grid.max())\n",
    "    \n",
    "    # Move the grid tensor to the same device as iframe\n",
    "    grid = grid.to(iframe.device)\n",
    "    \n",
    "    # Use grid_sample to warp the frame\n",
    "    warped_frame = FF.grid_sample(iframe.unsqueeze(0), grid, mode='bilinear', padding_mode='reflection', align_corners=True).squeeze()\n",
    "    \n",
    "\n",
    "    # Resize the warped frame to the original resolution\n",
    "    warped_frame = FF.interpolate(warped_frame.unsqueeze(0), size=(H, W), mode='bilinear', align_corners=True).squeeze()\n",
    "\n",
    "#     visualize_tensor_image(warped_frame)\n",
    "    return warped_frame\n",
    "\n",
    "\n",
    "def apply_motion_vectors_to_all_iframes(iframes, motion_vectors, scale_factor=1.0, grid_density=4):\n",
    "    \"\"\"\n",
    "    Apply motion vectors to all I-frames.\n",
    "\n",
    "    Parameters:\n",
    "    - iframes: Tensor of shape [num_gops, C, H, W]\n",
    "    - motion_vectors: Tensor of shape [total_frames, H, W, 2]\n",
    "\n",
    "    Returns:\n",
    "    - warped_frames: Tensor of shape [total_frames, C, H, W]\n",
    "    \"\"\"\n",
    "    \n",
    "    num_gops, C, H, W = iframes.shape\n",
    "    total_frames = motion_vectors.shape[0]\n",
    "    GOP_SIZE = total_frames // num_gops\n",
    "    \n",
    "    # Initialize the result tensor\n",
    "    warped_frames = torch.zeros((total_frames, C, H, W), device=iframes.device)\n",
    "\n",
    "    frame_counter = 0\n",
    "    for gop_idx in range(num_gops):\n",
    "        # Set the first frame of each GOP as the I-frame\n",
    "        warped_frames[frame_counter] = iframes[gop_idx]\n",
    "        frame_counter += 1\n",
    "        \n",
    "        for _ in range(GOP_SIZE - 1):\n",
    "            if frame_counter < total_frames:  # Ensure we don't exceed the total number of frames\n",
    "                mv = motion_vectors[frame_counter]\n",
    "                iframe = iframes[gop_idx]\n",
    "                \n",
    "                warped = apply_motion_vector_to_iframe_tensor(iframe, mv, scale_factor, grid_density)\n",
    "                warped_frames[frame_counter] = warped\n",
    "                frame_counter += 1\n",
    "\n",
    "    return warped_frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "sys.path.append('/home/ksknh7/.local/bin/pip')\n",
    "# import mxnet as mx\n",
    "# from mxnet import nd\n",
    "import torch\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "import pdb\n",
    "# from gluoncv.data.transforms import video\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import imageio\n",
    "import subprocess\n",
    "\n",
    "# transform_post = video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "def preprocess_images(frames):\n",
    "    processed_frames = []\n",
    "\n",
    "    for i in range(frames.shape[0]):\n",
    "        frame = frames[i]\n",
    "\n",
    "        # Transpose to move the channel dimension to the last axis\n",
    "        frame = tf.transpose(frame, perm=[1, 2, 0])\n",
    "\n",
    "        # Convert to PIL Image equivalent\n",
    "        frame = tf.image.convert_image_dtype(frame, dtype=tf.uint8) # 얘를 안하면 이미지 출력이 안됨 \n",
    "#         frame = tf.cast(frame, dtype=tf.uint8)\n",
    "        \n",
    "        # Resize\n",
    "#         frame = tf.image.resize(frame, (224, 224))\n",
    "        \n",
    "        # Normalize\n",
    "        frame = tf.cast(frame, dtype=tf.float32) / 255.0\n",
    "#         frame = frame / 255.0\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        frame = (frame - mean) / std\n",
    "        \n",
    "        frame = tf.transpose(frame, perm=[2, 0, 1])\n",
    "\n",
    "        processed_frames.append(frame)\n",
    "\n",
    "    return tf.stack(processed_frames)\n",
    "\n",
    "def inverse_preprocess_images(frames, device):\n",
    "    processed_frames = []\n",
    "\n",
    "    for i in range(frames.shape[0]):\n",
    "        frame = frames[i]\n",
    "\n",
    "        # Transpose to move the channel dimension to the last axis\n",
    "        frame = frame.permute(1, 2, 0)\n",
    "        \n",
    "#         start_time = time.time()\n",
    "\n",
    "        # Denormalize\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).to('cpu')\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).to('cpu')\n",
    "        frame = frame.to('cpu')\n",
    "#         end_time = time.time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "#         print(f\"  -CPU옮기는 시간: {elapsed_time}초\")\n",
    "#         start_time = time.time()\n",
    "\n",
    "\n",
    "#         mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "#         std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "#         frame = frame.to(device)\n",
    "        frame = ((frame * std) + mean) * 255.0\n",
    "        \n",
    "        frame = tf.cast(frame, dtype=tf.uint8)\n",
    "#         frame = frame.byte()\n",
    "        frame = tf.image.convert_image_dtype(frame, dtype=tf.float32)\n",
    "#         frame = frame.to(torch.float32)\n",
    "#         frame = tf.image.resize(frame, (256, 256))\n",
    "\n",
    "        # convert to JPEG \n",
    "#         frame = tf.cast(frame, dtype=tf.uint8)\n",
    "#         frame = tf.cast(frame, dtype=tf.float32)\n",
    "#         frame = tf.image.convert_image_dtype(frame , dtype=tf.uint8)\n",
    "#         frame = tf.image.encode_jpeg(frame)\n",
    "#         frame = tf.image.decode_jpeg(frame, channels=3)\n",
    "#         end_time = time.time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "#         print(f\"  -나머지 시간: {elapsed_time}초\")\n",
    "\n",
    "        \n",
    "        processed_frames.append(frame)\n",
    "        \n",
    "    return tf.stack(processed_frames)\n",
    "\n",
    "\n",
    "# def transform_input_to_mxnet(input_tensor):\n",
    "# #     # PyTorch 텐서를 MXNet NDArray로 변환\n",
    "# #     input_ndarray = mgs_ = transform_post(input_tensor)\n",
    "# #     data = input_tensor.reshape((-1,) + (args.new_length, 3, args.input_size, args.input_size))\n",
    "# #     data = data.reshape((-1,) + (imgs.shape[0] , 3, 256, 256))\n",
    "\n",
    "#     # 입력 텐서의 shape를 가져옵니다.\n",
    "#     clip_length, num_channels, height, width = input_tensor.shape\n",
    "    \n",
    "#     # 배치 차원과 깊이 차원을 추가합니다.\n",
    "#     input_data = input_tensor.reshape((1, num_channels, clip_length, height, width))\n",
    "    \n",
    "#     min_val = np.min(input_data, axis=(2, 3, 4), keepdims=True)\n",
    "#     max_val = np.max(input_data, axis=(2, 3, 4), keepdims=True)\n",
    "#     normalized_data = (input_data - min_val) / (max_val - min_val)\n",
    "    \n",
    "#     return normalized_data\n",
    "\n",
    "def save_images_with_imageio(num_frame, original_images, adv_images, save_path):\n",
    "    for i in range(num_frame):\n",
    "        original_image = (original_images[i] * 255).astype('uint8')\n",
    "        adv_image = (adv_images[i] * 255).astype('uint8')\n",
    "        \n",
    "        original_image_path = f\"{save_path}/original_{i}.png\"\n",
    "        adv_image_path = f\"{save_path}/adv_{i}.png\"\n",
    "        \n",
    "        imageio.imwrite(original_image_path, original_image)\n",
    "        imageio.imwrite(adv_image_path, adv_image)\n",
    "        \n",
    "def normalization(imgs):\n",
    "    batch_size = imgs.shape[0]\n",
    "    input_data = mx.nd.array(imgs)\n",
    "    data = np.stack(imgs, axis=0)\n",
    "    data = data.reshape((-1,) + (imgs.shape[0] , 3, 256, 256))\n",
    "    data = np.transpose(data, (0, 2, 1, 3, 4))\n",
    "    print(\"normalized imgs shape:\", imgs.shape)\n",
    "\n",
    "    return data\n",
    "\n",
    "def Cross_Entropy(logits, target, kappa=0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    other, other_class = logits.max(1)\n",
    "    loss = criterion(logits, other_class.long())\n",
    "    #pdb.set_trace()\n",
    "    return loss, loss.item(), loss.item(), loss.item(), loss.item(), loss.item(), loss.item()\n",
    "\n",
    "\n",
    "def CWLoss(logits, target, device, kappa=0):\n",
    "    logits = F.softmax(logits, dim=1)\n",
    "\n",
    "    target_onehot = torch.zeros(1, 101).to(device)\n",
    "    target_onehot[0, target] = 1\n",
    "    real = (target_onehot * logits).sum(1)[0]\n",
    "    tmp_logit = ((1. - target_onehot) * logits - target_onehot*10000.)\n",
    "\n",
    "    other, other_class = logits.max(1)\n",
    "    sort_prob, sort_class = logits.sort()\n",
    "    second_logit = sort_prob[0][-2].unsqueeze(0)\n",
    "    second_class = sort_class[0][-2].unsqueeze(0)\n",
    "    \n",
    "    return torch.clamp(torch.sum(logits)-second_logit, kappa), target.item(), real.item(), other.item(), other_class.item(), second_logit.item(), second_class.item() # untargeted\n",
    "#     return torch.clamp(other-5*real, kappa), target.item(), real.item(), other.item(), other_class.item(), second_logit.item(), second_class.item() # targeted\n",
    "\n",
    "\n",
    "def norm2(x):\n",
    "    assert len(x.shape) == 4\n",
    "    norm_vec = torch.sqrt(x.float().pow(2).sum(dim=[1,2,3])).view(-1, 1, 1, 1)\n",
    "    norm_vec += (norm_vec == 0).float()*1e-8\n",
    "    return norm_vec\n",
    "\n",
    "def _pert_loss(logits, ori_label, target_label, delta_motion, device):\n",
    "    cw_loss = CWLoss\n",
    "    #cw_loss = Cross_Entropy\n",
    "    loss, target, real, other, other_class, second_logit, second_class = cw_loss(logits, target_label, device)\n",
    "    loss = loss.squeeze(0)\n",
    "    return loss, target, real, other, other_class, second_logit, second_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Box Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coviar import get_num_frames\n",
    "from coviar import load\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import pdb\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "# import mxnet.ndarray as FF\n",
    "import random\n",
    "\n",
    "GOP_SIZE = 12\n",
    "\n",
    "def _perturbation_image(model,\n",
    "                      original_image,\n",
    "                      ori_label,\n",
    "                      video_path,\n",
    "                      save_path,\n",
    "                      transform_post,\n",
    "                      args,\n",
    "                      config,\n",
    "                      device, segmentation_module):\n",
    "    \n",
    "\n",
    "    original_image = original_image.to(device)\n",
    "\n",
    "#     total_frames = get_num_frames(video_path)\n",
    "    original_image_ = original_image.clone()\n",
    "    \n",
    "    \n",
    "    num_frame, channel, height, width = original_image.shape\n",
    "    dim = height * width * channel\n",
    "    loop = 0\n",
    "    inner_loop = 0\n",
    "    success = False\n",
    "    num_query = 0\n",
    "    num_pframe = 0\n",
    "\n",
    "    max_query = 60000\n",
    "    base_exploration = 0.1\n",
    "    fd_eta = 0.1\n",
    "    online_lr = 0.1\n",
    "    flow_lr = 0.005\n",
    "    \n",
    "    \n",
    "    ori_label = ori_label.to(device)\n",
    "    target_label = torch.tensor([random.sample(range(101), 1)[0]]).to(device)\n",
    "    while target_label == ori_label:\n",
    "        target_label = torch.tensor([random.sample(range(101), 1)[0]]).to(device)\n",
    "    print('target_label', target_label, 'ori_label', ori_label)\n",
    "    \n",
    "    if num_frame % GOP_SIZE == 0:\n",
    "        num_iframe = num_frame // GOP_SIZE\n",
    "    else:\n",
    "        num_iframe = num_frame // GOP_SIZE + 1\n",
    "\n",
    "    prior = torch.zeros(num_frame, channel, height, width).to(device)\n",
    "    delta = torch.zeros(num_frame, channel, height, width).to(device)\n",
    "    est_grad = torch.zeros(num_frame, channel, height, width).to(device)\n",
    "    adv_img = torch.zeros(3, num_frame, channel, height, width).to(device)\n",
    "    iframe = torch.zeros(num_frame, height, width, channel).to(device)\n",
    "    noise_frames = torch.zeros(num_frame, channel, height, width).to(device)\n",
    "    noise_iframes = torch.zeros(num_frame, channel, height, width).to(device)\n",
    "    segmentation_result = torch.zeros(num_iframe, height, width).to(device)\n",
    "    dynamic_exploration = torch.zeros(height, width).to(device)\n",
    "\n",
    "    index_visual = torch.zeros(num_frame, 2, height, width).to(device)\n",
    "    index_motion = torch.zeros(num_frame, height, width, 2).to(device)\n",
    "    \n",
    "#     print('original_image:', original_image.shape)\n",
    "#     visualize_warped_frames(original_image)\n",
    "    \n",
    "    segmentation_result = perform_segmentation_with_interval(segmentation_module, original_image, interval=12, device=device)\n",
    "#     print('segmentation', segmentation_result.shape)\n",
    "    mask = create_mask_for_stuff_pixels(segmentation_result)\n",
    "#     print('mask', mask.shape)\n",
    "#     visualize_masks(mask)\n",
    "    \n",
    "    motion_vectors, weight_maps = compute_motion_vectors_and_weight_maps(video_path, GOP_SIZE=12, height=height, width=width, num_frame=num_frame)\n",
    "#     print('motion_vectors', motion_vectors.shape, 'weight_maps', weight_maps.shape)\n",
    "    \n",
    "#     # I-frame과 각 프레임 간의 Optical Flow 계산\n",
    "#     motion_vectors = calculate_optical_flow(video_path, cap, gop_size=GOP_SIZE)\n",
    "#     cap.release()\n",
    "\n",
    "    # exploration 값을 동적으로 조절, dynamic_exploration_tensor: torch.Size([9, 3, 256, 256])\n",
    "    dynamic_exploration = base_exploration * weight_maps \n",
    "#     print(dynamic_exploration.shape)\n",
    "    # dynamic_exploration 값을 사용하여 노이즈 프레임의 크기를 조절 노이즈 프레임을 모션 벡터와 결합하여 적대적 섭동을 생성\n",
    "    dynamic_exploration_tensor = dynamic_exploration.clone().detach().to(device)\n",
    "    \n",
    "    while not (num_query > max_query):\n",
    "        pred_adv_logit = list()\n",
    "        start1 = time.time()\n",
    "            \n",
    "        gop_index = loop // GOP_SIZE\n",
    "\n",
    "        # 노이즈 프레임 생성: iframe에 대한 랜덤 노이즈 프레임(noise_frames)을 생성\n",
    "        noise_frames = torch.randn(1, 3, height, width).repeat(num_iframe, 1, 1, 1).to(device)\n",
    "#         visualize_warped_frames(noise_frames)\n",
    "#         print('noise_frames.shape', noise_frames.shape, mask.shape)\n",
    "        \n",
    "        # masks를 noise_frames와 동일한 shape으로 확장 \n",
    "        expanded_masks = mask.unsqueeze(1).expand_as(noise_frames)\n",
    "#         print('mask', mask.shape, expanded_masks.shape)\n",
    "\n",
    "        # noise_frames에 마스크를 적용\n",
    "#         masked_noise_frames = noise_frames * expanded_masks.to(device)\n",
    "        masked_noise_frames = noise_frames\n",
    "\n",
    "#         print('dynamic_exploration_tensor:', dynamic_exploration_tensor.shape, 'masked_noise_frames:', masked_noise_frames.shape)\n",
    "        iframe_noise = dynamic_exploration_tensor * masked_noise_frames\n",
    "#         iframe_noise = dynamic_exploration_tensor * noise_frames # seg X\n",
    "        applied_noise = apply_motion_vectors_to_all_iframes(iframe_noise, motion_vectors)\n",
    "#         print('iframe_noise', iframe_noise.shape, 'applied_noise', applied_noise.shape)\n",
    "        \n",
    "        q1 = prior + applied_noise\n",
    "        q2 = prior - applied_noise\n",
    "#         visualize_warped_frames(q1)\n",
    "\n",
    "        adv_img[0] = original_image + fd_eta*q1/norm2(q1)\n",
    "        adv_img[1] = original_image + fd_eta*q2/norm2(q2)\n",
    "        adv_img[2] = original_image\n",
    "        \n",
    "        for i in range(3):\n",
    "#             data = adv_img[i].clone()\n",
    "            \n",
    "            adv_img_np = adv_img[i].clone().cpu().numpy()\n",
    "            data = np.stack(adv_img_np, axis=0)\n",
    "            data = torch.tensor(np.array(data)).to(device)\n",
    "            \n",
    "#             start_time = time.time()\n",
    "            restored_frames = tf.expand_dims(inverse_preprocess_images(data, device), axis=0)\n",
    "#             restored_frames = inverse_preprocess_images(data, device).unsqueeze(0)\n",
    "#             print('!1!!!!!! data shape: ', restored_frames.shape, '// type', type(restored_frames))\n",
    "            \n",
    "#             end_time = time.time()\n",
    "#             elapsed_time = end_time - start_time\n",
    "#             print(f\"전처리 시간: {elapsed_time}초\")\n",
    "            \n",
    "#             print(restored_frames.device) \n",
    "            \n",
    "#             start_time = time.time()\n",
    "#             with tf.device('/GPU:0'):\n",
    "            pred = model.predict(restored_frames, verbose=0)\n",
    "#             pred = model(restored_frames)\n",
    "#             end_time = time.time()\n",
    "#             elapsed_time = end_time - start_time\n",
    "#             print(f\"모델 쿼리 실행 시간: {elapsed_time}초\")\n",
    "#             start_time = time.time()\n",
    "            \n",
    "#             pred = model(restored_frames)\n",
    "            pred_classes = np.argmax(pred)\n",
    "            pred = torch.tensor(pred).to(device)            \n",
    "            \n",
    "#             print('pred shape: ', pred.shape ,'pred type: ', type(pred) , 'pred_classes: ', pred_classes)\n",
    "            \n",
    "            pred_adv_logit.append(pred)\n",
    "            \n",
    "#             loss = criterion(logits, target_label)  # 손실 함수 정의 필요\n",
    "#             losses.append(loss.item())\n",
    "            \n",
    "#         losses = torch.tensor(losses).to(device)\n",
    "#         print('losses: ', losses, losses.shape)\n",
    "#         weighted_noises = (losses - losses.mean()) / (losses.std() + 1e-10)  # 정규화\n",
    "#         est_grad = torch.mean(weighted_noises.view(population_size, 1, 1, 1, 1) * noises, dim=0)\n",
    "\n",
    "#         # 그래디언트를 사용하여 적대적 이미지 업데이트\n",
    "#         delta = online_lr * est_grad.sign()\n",
    "#         original_image = torch.clamp(original_image + delta, 0, 1)\n",
    "\n",
    "\n",
    "        l1, _, _, _, _, _, _ = _pert_loss(pred_adv_logit[0].to(device), ori_label, target_label, delta, device)\n",
    "        l2, _, _, _, _, _, _ = _pert_loss(pred_adv_logit[1].to(device), ori_label, target_label, delta, device)\n",
    "        loss, target, real, other, other_class, second_logit, second_class = _pert_loss(pred_adv_logit[2].to(device), ori_label, target_label, delta, device)\n",
    "\n",
    "        \n",
    "#         print('loss:', loss, 'other', other, 'other_class', other_class,'second_logit', second_logit, 'second_class', second_class)\n",
    "        \n",
    "        num_query += 3\n",
    "        \n",
    "        est_deriv = (l1-l2)/(fd_eta*base_exploration*base_exploration)\n",
    "        est_grad = est_deriv.item() * applied_noise\n",
    "        prior += online_lr * est_grad\n",
    "        \n",
    "#         visualize_warped_frames(applied_noise)\n",
    "\n",
    "        original_image = original_image - flow_lr*prior.sign()\n",
    "        delta = original_image_ - original_image\n",
    "        original_image = torch.max(torch.min(original_image, original_image_ + 0.03), original_image_ - 0.03)\n",
    "        original_image = torch.clamp(original_image, 0, 1)\n",
    "\n",
    "#         end_time = time.time()\n",
    "#         elapsed_time = end_time - start_time\n",
    "#         print(f\"나머지 시간: {elapsed_time}초\\n\")\n",
    "\n",
    "        # 적대적 공격의 성공 여부 판단:\n",
    "        pred_adv_label = pred_adv_logit[2].argmax()\n",
    "        if (loop % 100 ==0) or (loop == max_query) or pred_adv_label != ori_label: # untargeted attack.\n",
    "#         if (loop % 10 ==0) or (loop == max_query) or pred_adv_label == target_label:\n",
    "            print('[T2]{:.3f}s for [{}]-th loop\\t'\n",
    "                  'Queries {:03d}\\t'\n",
    "                  'Overall loss {:.3f}\\t'\n",
    "                  'est_deriv {:.3f}\\t'\n",
    "                  'Target {}\\t'\n",
    "                  'Target logit {:.3f}\\t'\n",
    "                  'ori logit {:.3f}\\t'\n",
    "                  'ori class {}\\t'\n",
    "                  'second logit {:.3f}\\t'\n",
    "                  'second class {}\\t'.format(time.time() - start1, loop,\n",
    "                                            num_query, loss, est_deriv.item(), target,\n",
    "                                            real, other, other_class, second_logit, second_class))\n",
    "#             print(prior.shape, delta.shape)\n",
    "            visualize_warped_frames(q1)\n",
    "#             visualize_warped_frames(prior) \n",
    "#             visualize_warped_frames(original_image)\n",
    "#             visualize_video_frames_tf(restored_frames)\n",
    "            \n",
    "            \n",
    "        loop += 1\n",
    "        if pred_adv_label != ori_label:  # untargeted attack.\n",
    "#         if pred_adv_label == target_label:\n",
    "            print('Predicted label is {}\\t'.format(pred_adv_label))\n",
    "            diff = adv_img[2] - original_image_\n",
    "            print('diff max {:.3f}, diff min {:.3f}'.format(diff.max(), diff.min()))\n",
    "            success = True\n",
    "#             visualize_warped_frames(original_image_) # 색 이상\n",
    "            \n",
    "#             visualize_video_frames_tf(tf.squeeze(test_frames, axis=0))\n",
    "            save_video_frames_tf(tf.squeeze(test_frames, axis=0))\n",
    "#             visualize_video_frames_tf(tf.squeeze(restored_frames, axis=0)) # 흐림\n",
    "            \n",
    "            #save_images(num_frame, original_image_.cpu().permute(0,2,3,1).numpy(), adv_img[2].cpu().permute(0,2,3,1).numpy(), save_path)\n",
    "            break\n",
    "\n",
    "        if num_query >= max_query:\n",
    "#             visualize_warped_frames(original_image_)\n",
    "            save_video_frames_tf(tf.squeeze(test_frames, axis=0))\n",
    "            #save_images(num_frame, original_image_.cpu().permute(0,2,3,1).numpy(), adv_img[2].cpu().permute(0,2,3,1).numpy(), save_path)\n",
    "            break\n",
    "            \n",
    "    return pred_adv_label, num_query, success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': PosixPath('/data_1/seclab_nahyun/UCF101_subset/train'), 'val': PosixPath('/data_1/seclab_nahyun/UCF101_subset/val'), 'test': PosixPath('/data_1/seclab_nahyun/UCF101_subset/test')}\n",
      "Total videos: 1010\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "download_dir = pathlib.Path('/data_1/seclab_nahyun/UCF101_subset/')\n",
    "subset_paths = {'train': pathlib.Path('/data_1/seclab_nahyun/UCF101_subset/train'), 'val': pathlib.Path('/data_1/seclab_nahyun/UCF101_subset/val'), 'test': pathlib.Path('/data_1/seclab_nahyun/UCF101_subset/test')}\n",
    "print(subset_paths)\n",
    "\n",
    "# video_count_train = len(list(download_dir.glob('train/*/*.avi')))\n",
    "# video_count_val = len(list(download_dir.glob('val/*/*.avi')))\n",
    "video_count_test = len(list(download_dir.glob('test/*/*.avi')))\n",
    "# video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_count_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "def format_frames(frame, output_size):\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    return frame\n",
    "\n",
    "\n",
    "# 이전에 정의한 format_frames 함수와 FrameGenerator 클래스를 그대로 사용합니다.\n",
    "def frames_from_video_file(video_path, output_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    동영상 파일에서 모든 프레임을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        video_path: 동영상 파일 경로.\n",
    "        output_size: 프레임 이미지의 크기.\n",
    "\n",
    "    Returns:\n",
    "        모든 프레임으로 이루어진 NumPy 배열과 video_path.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = src.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = format_frames(frame, output_size)\n",
    "        result.append(frame)\n",
    "\n",
    "    src.release()\n",
    "    result = np.array(result)[..., [2, 1, 0]]\n",
    "#     print(result)\n",
    "\n",
    "    return result, str(video_path)  # video_path도 반환합니다.\n",
    "\n",
    "# 이제 frames_from_video_file 함수를 사용하여 모든 프레임을 생성하도록 FrameGenerator 클래스를 수정합니다.\n",
    "class FrameGenerator:\n",
    "    def __init__(self, path):\n",
    "        \"\"\" 레이블링된 동영상 파일의 프레임을 생성합니다.\n",
    "\n",
    "        Args:\n",
    "            path: 동영상 파일 경로.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "        self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "    def get_files_and_class_names(self):\n",
    "        video_paths = list(self.path.glob('*/*.avi'))\n",
    "        classes = [p.parent.name for p in video_paths]\n",
    "        return video_paths, classes\n",
    "\n",
    "    def __call__(self):\n",
    "        video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "        pairs = list(zip(video_paths, classes))\n",
    "\n",
    "        random.shuffle(pairs)\n",
    "\n",
    "        for path, name in pairs:\n",
    "            video_frames, video_path = frames_from_video_file(path)\n",
    "            label = self.class_ids_for_name[name]  # 레이블 인코딩\n",
    "            yield video_frames, label, video_path  # video_path도 반환합니다.\n",
    "\n",
    "#     def generator_function(self, num_batches):\n",
    "#         video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "#         pairs = list(zip(video_paths, classes))\n",
    "\n",
    "#         random.shuffle(pairs)\n",
    "\n",
    "#         for path, name in pairs:\n",
    "#             video_frames, video_path = frames_from_video_file(path)\n",
    "#             label = self.class_ids_for_name[name]  # 레이블 인코딩\n",
    "#             yield video_frames, label, video_path  # video_path도 반환합니다.\n",
    "\n",
    "#             num_batches -= 1\n",
    "#             if num_batches == 0:\n",
    "#                 break\n",
    "           \n",
    "        \n",
    "# Create the training set\n",
    "output_signature = (tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(), dtype=tf.int16),\n",
    "                    tf.TensorSpec(shape=(), dtype=tf.string))  # video_path를 추가\n",
    "batch_size = 1\n",
    "# Create the test set\n",
    "# frame_generator = FrameGenerator(subset_paths['test'])\n",
    "# generator_function = lambda: frame_generator.generator_function(1)  # 원하는 배치 개수를 인자로 전달\n",
    "# test_ds = tf.data.Dataset.from_generator(generator_function, output_signature=output_signature)\n",
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test']), output_signature=output_signature)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# test_ds = test_ds.shuffle(1000).batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE) 이거 개오래걸림\n",
    "\n",
    "# test_ds = test_ds.take(1).batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('my_model')\n",
    "# loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.get_device_name() # CUDA를 실행하고 있는 기기 이름을 나타낸다.\n",
    "torch.cuda.is_available() # CUDA의 활성 여부를 나타낸다.\n",
    "# print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 97, 256, 256, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 16:47:50.622741: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "test_frames, test_labels, video_path = next(iter(test_ds))\n",
    "print(test_frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for net_encoder\n",
      "Loading weights for net_decoder\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.65 GiB total capacity; 20.64 MiB already allocated; 1.56 MiB free; 32.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m segmentation_module \u001b[38;5;241m=\u001b[39m SegmentationModule(net_encoder, net_decoder, crit)\n\u001b[1;32m     26\u001b[0m segmentation_module\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 27\u001b[0m \u001b[43msegmentation_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# i = 0\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# for test_frames, test_labels, video_path in test_ds:\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     if test_frames is None:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#         continue\u001b[39;00m\n\u001b[1;32m     34\u001b[0m visualize_video_frames_tf(tf\u001b[38;5;241m.\u001b[39msqueeze(test_frames, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 23.65 GiB total capacity; 20.64 MiB already allocated; 1.56 MiB free; 32.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/home/seclab_nahyun/jupyter_notebook/semantic-segmentation-pytorch/')\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1'\n",
    "\n",
    "# 모델 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Load the pre-trained model\n",
    "builder = ModelBuilder()\n",
    "net_encoder = builder.build_encoder(\n",
    "    arch='resnet50dilated',\n",
    "    fc_dim=2048,\n",
    "    weights='/home/seclab_nahyun/jupyter_notebook/semantic-segmentation-pytorch/ckpt/encoder_epoch_20.pth')\n",
    "net_decoder = builder.build_decoder(\n",
    "    arch='ppm_deepsup',\n",
    "    fc_dim=2048,\n",
    "    num_class=150,\n",
    "    weights='/home/seclab_nahyun/jupyter_notebook/semantic-segmentation-pytorch/ckpt/decoder_epoch_20.pth',\n",
    "    use_softmax=True)\n",
    "\n",
    "crit = torch.nn.NLLLoss(ignore_index=-1)\n",
    "segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
    "segmentation_module.eval()\n",
    "segmentation_module.to(device)\n",
    "\n",
    "# i = 0\n",
    "# for test_frames, test_labels, video_path in test_ds:\n",
    "#     if test_frames is None:\n",
    "#         continue\n",
    "    \n",
    "visualize_video_frames_tf(tf.squeeze(test_frames, axis=0))\n",
    "# 각 배치에 대한 예측 수행\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# print(test_frames.shape, type(test_frames), test_labels, video_path)\n",
    "# predictions = loaded_model.predict(test_frames)\n",
    "# predicted_class_index = np.argmax(predictions)\n",
    "# print(predictions, predicted_class_index, )\n",
    "\n",
    "save_path = 'saved_images'  # 저장 경로 \n",
    "video_frames = tf.transpose(test_frames, perm=[0, 1, 4, 2, 3])\n",
    "preprocessed_image = preprocess_images(video_frames[0])\n",
    "#     print(preprocessed_image.shape, type(preprocessed_image))\n",
    "video_frames = torch.tensor(preprocessed_image.numpy())\n",
    "\n",
    "label = torch.tensor(test_labels.numpy())\n",
    "input_video_path = video_path[0].numpy().decode('utf-8')\n",
    "print(video_frames.shape, label, input_video_path)\n",
    "\n",
    "output_video_path = \"./resized_video.mp4\"  # 출력 비디오 파일 경로\n",
    "target_size = (256, 256)  # 목표 크기\n",
    "convert_resize_video(input_video_path, output_video_path)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "_perturbation_image(loaded_model, video_frames, label, output_video_path, save_path, None, None, None, device, segmentation_module)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"모델 쿼리 실행 시간: {elapsed_time}초\")\n",
    "\n",
    "\n",
    "#     i+=1\n",
    "#     if (i == 5) :\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_1 (Rescaling)     (None, None, None, None,  0         \n",
      "                              3)                                 \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, None, None, None,  4049571  \n",
      " ibuted)                      1280)                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, None, None,  129381    \n",
      "                              101)                               \n",
      "                                                                 \n",
      " global_average_pooling3d (G  (None, 101)              0         \n",
      " lobalAveragePooling3D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,178,952\n",
      "Trainable params: 129,381\n",
      "Non-trainable params: 4,049,571\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def make_gradcam_heatmap_video(video_array, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_outputs, preds = grad_model(video_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    heatmaps = []\n",
    "    for i in range(video_array.shape[1]):  # Loop through frames\n",
    "        grads = tape.gradient(class_channel, last_conv_layer_outputs[:, i])\n",
    "\n",
    "        # Assuming 3D input with dimensions (batch_size, frames, height, width, channels)\n",
    "        # Adjust the axis parameter based on the actual dimensions of your input\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(1, 2, 3, 4))\n",
    "\n",
    "        last_conv_layer_output = last_conv_layer_outputs[0, i]\n",
    "        heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "        heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "        heatmaps.append(heatmap.numpy())\n",
    "\n",
    "    return np.array(heatmaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_frames.shape)\n",
    "# video_frames = tf.transpose(test_frames, perm=[0, 1, 4, 2, 3])\n",
    "heatmap = make_gradcam_heatmap(test_frames, loaded_model, 'global_average_pooling3d')\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "        \n",
    "\n",
    "# # def check_normalization(video_tensor):\n",
    "# #     # 주어진 텐서의 크기 확인\n",
    "# #     assert len(video_tensor.size()) == 4, \"텐서는 4차원이어야 합니다.\"\n",
    "\n",
    "# #     # 텐서의 데이터 타입을 torch.float32로 변환\n",
    "# #     video_tensor = video_tensor.float()\n",
    "\n",
    "# #     # 프레임 수, 채널 수, 높이, 너비 추출\n",
    "# #     num_frames, num_channels, height, width = video_tensor.size()\n",
    "\n",
    "# #     # 각 프레임의 최솟값과 최댓값 확인\n",
    "# #     for frame_idx in range(num_frames):\n",
    "# #         frame = video_tensor[frame_idx]\n",
    "# #         min_value = frame.min().item()\n",
    "# #         max_value = frame.max().item()\n",
    "\n",
    "# #         print(f\"Frame {frame_idx + 1}: Min Value = {min_value}, Max Value = {max_value}\")\n",
    "\n",
    "\n",
    "\n",
    "# with tf.device(\"/GPU:3\"):\n",
    "#     for test_frames, test_labels, video_path in test_ds:\n",
    "        \n",
    "#         predictions = loaded_model.predict(test_frames)\n",
    "#         predicted_class_index = np.argmax(predictions)\n",
    "#         print(predictions, predicted_class_index)\n",
    "        \n",
    "# #         print(test_frames.shape, type(test_frames))\n",
    "# #         visualize_video_frames_tf(tf.squeeze(test_frames, axis=0))\n",
    "#         video_frames = tf.transpose(test_frames, perm=[0, 1, 4, 2, 3])\n",
    "#         preprocessed_image = preprocess_images(video_frames[0])\n",
    "#         video_frames = torch.tensor(preprocessed_image.numpy())\n",
    "#         print(video_frames.dtype, type(video_frames))\n",
    "# #         visualize_warped_frames(video_frames)\n",
    "# #         check_normalization(video_frames)\n",
    "        \n",
    "#         adv_img_np = video_frames.clone().cpu().numpy()\n",
    "#         data = np.stack(video_frames, axis=0)\n",
    "#         data = torch.tensor(np.array(data)).to(device)\n",
    "#         restored_frames = inverse_preprocess_images(data, device)\n",
    "#         restored_frames = tf.expand_dims(restored_frames, axis=0)\n",
    "\n",
    "#         predictions = loaded_model.predict(restored_frames)\n",
    "#         predicted_class_index = np.argmax(predictions)\n",
    "#         print(predictions, predicted_class_index)\n",
    "        \n",
    "#         break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import sys\n",
    "sys.path.append('./pytorch-resnet3d/')\n",
    "from models import resnet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "net = resnet.i3_res50(400) # vanilla I3D ResNet50\n",
    "net.eval()\n",
    "# net = resnet.i3_res50_nl() # Nonlocal version\n",
    "inp = {'frames': torch.rand(4, 3, 32, 224, 224)}\n",
    "pred, losses = net(inp)\n",
    "print(pred, losses)\n",
    "print(net)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# resnet50 = models.resnet50(pretrained = True).to(device)\n",
    "# resnet50.eval()\n",
    "# print(resnet50)\n",
    "\n",
    "for name, _ in net.named_modules():\n",
    "    print(name)\n",
    "    \n",
    "def normalize(tensor, mean, std):\n",
    "    if not tensor.ndimension() == 4:\n",
    "        raise TypeError('tensor should be 4D')\n",
    "\n",
    "    mean = torch.FloatTensor(mean).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
    "    std = torch.FloatTensor(std).view(1, 3, 1, 1).expand_as(tensor).to(tensor.device)\n",
    "\n",
    "    return tensor.sub(mean).div(std)\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return self.do(tensor)\n",
    "    \n",
    "    def do(self, tensor):\n",
    "        return normalize(tensor, self.mean, self.std)\n",
    "    \n",
    "    def undo(self, tensor):\n",
    "        return denormalize(tensor, self.mean, self.std)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_frames = test_frames[0]\n",
    "# print(video_frames.shape)\n",
    "# normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# torch_img = torch.from_numpy(np.asarray(video_frames)).permute(0, 3, 1, 2).float().div(255).cuda()\n",
    "# print(torch_img.shape, type(torch_img))\n",
    "# torch_img = F.interpolate(torch_img, size=(224, 224), mode='bilinear', align_corners=False) # (1, 3, 224, 224)\n",
    "# normed_torch_img = normalizer(torch_img) # (1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 저장 위치:  ./2024-02-20-04:32\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now() + datetime.timedelta(hours= 9)\n",
    "current_time = current_time.strftime('%Y-%m-%d-%H:%M')\n",
    "\n",
    "saved_loc = os.path.join('./', current_time)\n",
    "if os.path.exists(saved_loc):\n",
    "    shutil.rmtree(saved_loc)\n",
    "os.mkdir(saved_loc)\n",
    "\n",
    "print(\"결과 저장 위치: \", saved_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class GuidedBackpropReLU(Function):\n",
    "    @staticmethod\n",
    "    def forward(self, input_img):\n",
    "        # input image 기준으로 양수인 부분만 1로 만드는 positive_mask 생성\n",
    "        positive_mask = (input_img > 0).type_as(input_img)\n",
    "        \n",
    "        # torch.addcmul(input, tensor1, tensor2) => output = input + tensor1 x tensor 2\n",
    "        # input image와 동일한 사이즈의 torch.zeros를 만든 뒤, input image와 positive_mask를 곱해서 output 생성\n",
    "        output = torch.addcmul(torch.zeros(input_img.size()).type_as(input_img), input_img, positive_mask)\n",
    "        \n",
    "        # backward에서 사용될 forward의 input이나 output을 저장\n",
    "        self.save_for_backward(input_img, output)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output):\n",
    "        \n",
    "        # forward에서 저장된 saved tensor를 불러오기\n",
    "        input_img, output = self.saved_tensors\n",
    "        grad_input = None\n",
    "\n",
    "        # input image 기준으로 양수인 부분만 1로 만드는 positive_mask 생성\n",
    "        positive_mask_1 = (input_img > 0).type_as(grad_output)\n",
    "        \n",
    "        # 모델의 결과가 양수인 부분만 1로 만드는 positive_mask 생성\n",
    "        positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
    "        \n",
    "        # 먼저 모델의 결과와 positive_mask_1과 곱해주고,\n",
    "        # 다음으로는 positive_mask_2와 곱해줘서 \n",
    "        # 모델의 결과가 양수이면서 input image가 양수인 부분만 남도록 만들어줌\n",
    "        grad_input = torch.addcmul(torch.zeros(input_img.size()).type_as(input_img),\n",
    "                                   torch.addcmul(torch.zeros(input_img.size()).type_as(input_img), grad_output,\n",
    "                                                 positive_mask_1), positive_mask_2)\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackpropReLUModel:\n",
    "    def __init__(self, model, use_cuda):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        def recursive_relu_apply(module_top):\n",
    "            for idx, module in module_top._modules.items():\n",
    "                recursive_relu_apply(module)\n",
    "                if module.__class__.__name__ == 'ReLU':\n",
    "                    module_top._modules[idx] = GuidedBackpropReLU.apply\n",
    "\n",
    "        # replace ReLU with GuidedBackpropReLU\n",
    "        recursive_relu_apply(self.model)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        return self.model(input_img)\n",
    "\n",
    "    def __call__(self, input_img, target_category=None):\n",
    "        if self.cuda:\n",
    "            input_img = input_img.cuda()\n",
    "\n",
    "        input_img = input_img.requires_grad_(True)\n",
    "\n",
    "        output = self.forward(input_img)\n",
    "\n",
    "        if target_category is None:\n",
    "            target_category = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][target_category] = 1\n",
    "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
    "        if self.cuda:\n",
    "            one_hot = one_hot.cuda()\n",
    "\n",
    "        one_hot = torch.sum(one_hot * output)\n",
    "        # 모델이 예측한 결과값을 기준으로 backward 진행\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        # input image의 gradient를 저장\n",
    "        output = input_img.grad.cpu().data.numpy()\n",
    "        output = output[0, :, :, :]\n",
    "        output = output.transpose((1, 2, 0))\n",
    "        return output\n",
    "\n",
    "# pytorch에서는 224x224 짜리 RGB 이미지를 (3, 224, 224)로 저장하게 되는데, numpy에서는 (224, 224, 3)으로 저장\n",
    "def deprocess_image(img):\n",
    "    \"\"\" see https://github.com/jacobgil/keras-grad-cam/blob/master/grad-cam.py#L65 \"\"\"\n",
    "    img = img - np.mean(img)\n",
    "    img = img / (np.std(img) + 1e-5)\n",
    "    img = img * 0.1\n",
    "    img = img + 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return np.uint8(img * 255)\n",
    "\n",
    "# final conv layer name \n",
    "finalconv_name = 'layer4'\n",
    "\n",
    "# activations\n",
    "feature_blobs = []\n",
    "\n",
    "# gradients\n",
    "backward_feature = []\n",
    "\n",
    "# output으로 나오는 feature를 feature_blobs에 append하도록\n",
    "def forward_hook(module, input, output):\n",
    "#     feature_blobs.append(output.cpu().data.numpy())\n",
    "    feature_blobs.append(output.cpu().data)\n",
    "    \n",
    "\n",
    "# Grad-CAM\n",
    "def backward_hook(module, input, output):\n",
    "    backward_feature.append(output[0])\n",
    "#     backward_feature.append(output[0])\n",
    "    \n",
    "\n",
    "# resnet50._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "# resnet50._modules.get(finalconv_name).register_backward_hook(backward_hook)\n",
    "# final_layer = net.layer4[-1]\n",
    "# final_layer.register_forward_hook(forward_hook)\n",
    "# final_layer.register_backward_hook(backward_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get the softmax weight\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mnet\u001b[49m\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m      3\u001b[0m weight_softmax \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(params[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;66;03m# [1000, 512]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(weight_softmax\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# get the softmax weight\n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].cpu().detach().numpy()) # [1000, 512]\n",
    "print(weight_softmax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = logit[:, 30] # 예측값 y^\n",
    "# score = tf.squeeze(score)\n",
    "# score.backward(retain_graph = True) # 예측값 y^c에 대해서 backprop 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I3D, Kinetics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_grad_cam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedDataParallel\n\u001b[1;32m     33\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./pytorch-grad-cam/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_grad_cam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradCAM\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_grad_cam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_targets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassifierOutputTarget\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_grad_cam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_cam_on_image, \\\n\u001b[1;32m     37\u001b[0m     deprocess_image, \\\n\u001b[1;32m     38\u001b[0m     preprocess_image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_grad_cam'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "import collections\n",
    "import torchnet as tnt\n",
    "import sys\n",
    "sys.path.append('./pytorch-resnet3d/')\n",
    "from models import resnet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import torch.nn.functional as FF\n",
    "import torchvision\n",
    "import random\n",
    "from PIL import Image\n",
    "import numbers\n",
    "from utils import util\n",
    "import tensorflow as tf\n",
    "import torchvision.transforms.functional as F\n",
    "sys.path.append('./semantic-segmentation-pytorch/')\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "import time\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "sys.path.append('./pytorch-grad-cam/')\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1, 2, 3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "\n",
    "# visualize_video_frames_tf(tf.squeeze(test_frames, axis=0))\n",
    "\n",
    "\n",
    "# 모델 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "# Load the pre-trained model\n",
    "builder = ModelBuilder()\n",
    "net_encoder = builder.build_encoder(\n",
    "    arch='resnet50dilated',\n",
    "    fc_dim=2048,\n",
    "    weights='./semantic-segmentation-pytorch/ckpt/encoder_epoch_20.pth')\n",
    "net_decoder = builder.build_decoder(\n",
    "    arch='ppm_deepsup',\n",
    "    fc_dim=2048,\n",
    "    num_class=150,\n",
    "    weights='./semantic-segmentation-pytorch/ckpt/decoder_epoch_20.pth',\n",
    "    use_softmax=True)\n",
    "\n",
    "crit = torch.nn.NLLLoss(ignore_index=-1)\n",
    "segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
    "segmentation_module.eval()\n",
    "segmentation_module.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_cuda(batch, device):\n",
    "    _batch = {}\n",
    "    for k, v in batch.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            v = v.to(device)  # 모든 텐서를 cuda:2로 이동\n",
    "        elif isinstance(v, list) and isinstance(v[0], torch.Tensor):\n",
    "            v = [item.to(device) for item in v]  # 리스트 내의 텐서들을 cuda:2로 이동\n",
    "        _batch[k] = v\n",
    "    return _batch\n",
    "\n",
    "class GroupResize(object):\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.worker = torchvision.transforms.Resize(size, interpolation)\n",
    "        \n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "\n",
    "class GroupRandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        if not img_group:\n",
    "            return img_group  # 리스트가 비어 있으면 입력을 반환합니다.\n",
    "\n",
    "        w, h = img_group[0].size\n",
    "        th, tw = self.size\n",
    "\n",
    "        out_images = list()\n",
    "\n",
    "        x1 = random.randint(0, w - tw)\n",
    "        y1 = random.randint(0, h - th)\n",
    "\n",
    "        for img in img_group:\n",
    "            assert(img.size[0] == w and img.size[1] == h)\n",
    "            if w == tw and h == th:\n",
    "                out_images.append(img)\n",
    "            else:\n",
    "                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
    "\n",
    "        return out_images\n",
    "\n",
    "class GroupCenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.worker = torchvision.transforms.CenterCrop(size)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "\n",
    "class GroupRandomHorizontalFlip(object):\n",
    "    def __call__(self, img_group):\n",
    "        if random.random() < 0.5:\n",
    "            img_group = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n",
    "        return img_group\n",
    "\n",
    "class GroupNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor): # (T, 3, 224, 224)\n",
    "        for b in range(tensor.size(0)):\n",
    "            for t, m, s in zip(tensor[b], self.mean, self.std):\n",
    "                t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "class LoopPad(object):\n",
    "\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        length = tensor.size(0)\n",
    "\n",
    "        if length==self.max_len:\n",
    "            return tensor\n",
    "\n",
    "        # repeat the clip as many times as is necessary\n",
    "        n_pad = self.max_len - length\n",
    "        pad = [tensor]*(n_pad//length)\n",
    "        if n_pad%length>0:\n",
    "            pad += [tensor[0:n_pad%length]]\n",
    "\n",
    "        tensor = torch.cat([tensor]+pad, 0)\n",
    "        return tensor\n",
    "\n",
    "# NOTE: Returns [0-255] rather than torchvision's [0-1]\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        self.worker = lambda x: F.to_tensor(x)*255\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        img_group = [self.worker(img) for img in img_group]\n",
    "        return torch.stack(img_group, 0)\n",
    "    \n",
    "def kinetics_mean_std():\n",
    "    mean = [114.75, 114.75, 114.75]\n",
    "    std = [57.375, 57.375, 57.375]\n",
    "    return mean, std\n",
    "\n",
    "def clip_transform(max_len):\n",
    "\n",
    "    mean, std = kinetics_mean_std()\n",
    "    transform = transforms.Compose([\n",
    "                GroupResize(256),\n",
    "                GroupCenterCrop(256),\n",
    "                ToTensor(),\n",
    "                GroupNormalize(mean, std),\n",
    "                LoopPad(max_len),\n",
    "        ])\n",
    "    return transform\n",
    "\n",
    "def extract_frames(video_path, num_frames):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 이미지로 변환\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "        frames.append(img_pil)\n",
    "\n",
    "        if len(frames) == num_frames:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "class KineticsTest(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, clip_len):\n",
    "        super(KineticsTest, self).__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.clip_len = clip_len\n",
    "\n",
    "        if not os.path.exists('/home/seclab_nahyun/jupyter_notebook/pytorch-resnet3d/data/kinetics_data.pth'):\n",
    "            self.parse_annotations()\n",
    "            print('Annotations created!')\n",
    "\n",
    "        annotations = torch.load('/home/seclab_nahyun/jupyter_notebook/pytorch-resnet3d/data/kinetics_data.pth')\n",
    "        self.labels = annotations['labels']\n",
    "        self.test_data = annotations['val_data']\n",
    "        print('%d test clips' % len(self.test_data))\n",
    "        \n",
    "        self.clip_transform = clip_transform(self.clip_len)\n",
    "#         self.loader = lambda fl: Image.open(os.path.join(self.root, fl)).convert('RGB')\n",
    "        self.loader = lambda fl: os.path.join(self.root, fl)\n",
    "    \n",
    "    def parse_annotations(self):\n",
    "\n",
    "        def parse(annotation_csv):\n",
    "            annotations = open(annotation_csv, 'r').read().strip().split('\\n')[1:]\n",
    "            annotations = [line.split(',') for line in annotations]\n",
    "            clip_labels, yt_ids, start_times, end_times, _, _ = zip(*annotations)\n",
    "\n",
    "            labels = map(lambda l: l.strip('\"'), clip_labels)\n",
    "            labels = np.unique(list(labels)).tolist()\n",
    "\n",
    "            data = []\n",
    "            for yt_id, start, end, label in tqdm.tqdm(zip(yt_ids, start_times, end_times, clip_labels), total=len(yt_ids)):\n",
    "                label = label.strip('\"')\n",
    "                file_name = f\"{yt_id}_{int(start):06d}_{int(end):06d}.mp4\"\n",
    "                file_path = os.path.join(frame_dir, file_name)\n",
    "            \n",
    "                # 파일이 존재하는지 확인\n",
    "                if os.path.exists(file_path):\n",
    "                    data.append({'frames': file_path, 'label': labels.index(label)})\n",
    "                # else:\n",
    "                    # print(f\"파일을 찾을 수 없음: {file_path}\")\n",
    "\n",
    "            return data, labels\n",
    "\n",
    "\n",
    "        frame_dir = '%s/frames/' % self.root\n",
    "        val_data, labels = parse('%s/annotations/test.csv' % self.root)\n",
    "        annotations = {'val_data': val_data, 'labels': labels}\n",
    "        torch.save(annotations, '/home/seclab_nahyun/jupyter_notebook/pytorch-resnet3d/data/kinetics_data.pth')\n",
    "\n",
    "\n",
    "    def sample(self, video_path):\n",
    "        # video_path에서 직접 프레임을 추출하여 로드\n",
    "        frames = extract_frames(video_path, num_frames=self.clip_len)\n",
    "#         print(frames)\n",
    "\n",
    "        # 필요한 경우 중앙 정렬 등의 추가적인 프레임 선택 및 조정 로직 적용\n",
    "\n",
    "        # 이미지로드 및 변환\n",
    "#         imgs = [self.loader(frame) for frame in frames]\n",
    "\n",
    "        return frames\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        entry = self.test_data[index]\n",
    "        video_path = entry['frames']  # 비디오 경로 추가\n",
    "        frames = self.sample(video_path)\n",
    "        if frames is None or len(frames) == 0:\n",
    "            return self.__getitem__((index + 1) % len(self.test_data))  # 다음 인덱스로 이동\n",
    "        frames = self.clip_transform(frames)\n",
    "        frames = frames.permute(1, 0, 2, 3)  # (3, T, 224, 224)\n",
    "        instance = {'frames': frames, 'label': entry['label'], 'video_path': video_path}\n",
    "        return instance\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_data)\n",
    "\n",
    "def accuracy(output, target):\n",
    "    \"\"\"Computes the accuracy of the model\"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        assert pred.shape[0] == len(target)\n",
    "        correct = torch.sum(pred == target).item()\n",
    "    return correct / len(target)\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    args.world_size = ngpus_per_node * args.world_size\n",
    "    mp.spawn(main_worker, nprocs=ngpus_per_node, \n",
    "             args=(ngpus_per_node, args))\n",
    "    \n",
    "    \n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    global best_acc1\n",
    "    args.gpu = gpu\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    \n",
    "    print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "    args.rank = args.rank * ngpus_per_node + gpu\n",
    "    dist.init_process_group(backend='nccl', \n",
    "                            init_method='tcp://127.0.0.1:FREEPORT',\n",
    "                            world_size=args.world_size, \n",
    "                            rank=args.rank)\n",
    "    \n",
    "\n",
    "test_dataset = KineticsTest('/home/seclab_nahyun/jupyter_notebook/kinetics-dataset/test_ds', clip_len=32)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=36, pin_memory=True)\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "save_path = 'saved_images'  # 저장 경로 \n",
    "    \n",
    "\n",
    "loaded_model = resnet.i3_res50(400).to(device)  # vanilla I3D ResNet50\n",
    "# final_layer = loaded_model.layer4[-1]\n",
    "# final_layer.register_forward_hook(forward_hook)\n",
    "# final_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "# loaded_model._modules['layer4']._modules.get('2').register_forward_hook(forward_hook)\n",
    "# loaded_model._modules['layer4']._modules.get('2').register_backward_hook(backward_hook)\n",
    "# loaded_model._modules.get('layer4').register_forward_hook(forward_hook)\n",
    "# loaded_model._modules.get('layer4').register_backward_hook(backward_hook)\n",
    "\n",
    "\n",
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "i = 0\n",
    "# with torch.no_grad():\n",
    "loaded_model.eval()\n",
    "total_correct = 0\n",
    "total_images = 0\n",
    "loss_meters = collections.defaultdict(lambda: tnt.meter.AverageValueMeter())\n",
    "\n",
    "\n",
    "for idx, batch in enumerate(test_dataloader):\n",
    "    i += 1\n",
    "    batch = batch_cuda(batch, device)\n",
    "\n",
    "#     batch['frames'] = batch['frames'].requires_grad_(True)\n",
    "#     print(batch['frames'].shape)\n",
    "\n",
    "    pred, loss_dict = loaded_model(batch)\n",
    "    pred = pred.clone().requires_grad_(True)\n",
    "\n",
    "    loss_dict = {k: v.mean() for k, v in loss_dict.items() if v.numel() > 0}\n",
    "    loss = sum(loss_dict.values())\n",
    "\n",
    "    for k, v in loss_dict.items():\n",
    "        loss_meters[k].add(v.item())\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = accuracy(pred, batch['label'])\n",
    "    total_correct += acc\n",
    "    total_images += 1\n",
    "\n",
    "#         # 예측된 레이블과 실제 레이블 출력\n",
    "    _, predicted_labels = torch.max(pred, 1)\n",
    "    print(f\"Batch {idx}:\")\n",
    "    print(f\"Predicted labels: {predicted_labels}\")\n",
    "    print(f\"Actual labels: {batch['label']}\")\n",
    "    \n",
    "    if  predicted_labels != batch['label']:\n",
    "        continue\n",
    "\n",
    "    if i > 10:\n",
    "        break\n",
    "        \n",
    "    print(batch['frames'].shape, batch['label'], batch['video_path'])\n",
    "    video_path = batch['video_path']\n",
    "    test_labels = batch['label'].to('cpu')\n",
    "    test_frames = batch['frames'].to('cpu') # [1, 3, 32, 256, 256]\n",
    "    video_frames = tf.transpose(test_frames, perm=[0,  2,  1, 3, 4]) # [1, 32, 3, 256, 256]\n",
    "    preprocessed_image = preprocess_images(video_frames[0])\n",
    "    #     print(preprocessed_image.shape, type(preprocessed_image))\n",
    "    video_frames = torch.tensor(preprocessed_image.numpy())\n",
    "\n",
    "    label = torch.tensor(test_labels.numpy())\n",
    "    input_video_path = video_path[0]\n",
    "    print(video_frames.shape, label, input_video_path)\n",
    "\n",
    "    output_video_path = \"/home/seclab_nahyun/jupyter_notebook/resized_video.mp4\"  # 출력 비디오 파일 경로\n",
    "    target_size = (256, 256)  # 목표 크기\n",
    "    convert_resize_video(input_video_path, output_video_path)\n",
    "\n",
    "    video_frames=video_frames.to(device)\n",
    "\n",
    "#     _perturbation_image(loaded_model, video_frames, label, output_video_path, save_path, None, None, None, device, segmentation_module)\n",
    "\n",
    "print(total_correct, total_images)\n",
    "print(f\"전체 정확도: {total_correct / total_images}%\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"코드 실행 시간: {elapsed_time}초\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_model(name = 'i3d_resnet50_v1_ucf101', \n",
    "#                       nclass = 101, \n",
    "#                       pretrained = True, \n",
    "#                       num_segments = 2)\n",
    "# model.cast('float32')\n",
    "# # model.hybridize(static_alloc=True, static_shape=True)\n",
    "\n",
    "# model.collect_params().reset_ctx(context)\n",
    "# 모델 로딩\n",
    "model = get_model(name='i3d_resnet50_v1_ucf101',\n",
    "                  nclass=101,\n",
    "                  pretrained=True,\n",
    "                  num_segments=2)\n",
    "\n",
    "# 모델 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_kernel",
   "language": "python",
   "name": "workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
