{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jan 22 22:46:08 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA TITAN RTX                On | 00000000:21:00.0 Off |                  N/A |\n",
      "|  0%   27C    P8               11W / 280W|    107MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2706      G   /usr/lib/xorg/Xorg                           91MiB |\n",
      "|    0   N/A  N/A      2962      G   /usr/bin/gnome-shell                         13MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.datasets import make_regression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "print('시작!')\n",
    "def model_test(model_name, model):\n",
    "    x, y = make_regression(n_samples=100000, n_features=100)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(x, y)\n",
    "    end_time = time.time()\n",
    "    return f'{model_name}: 소요시간: {(end_time - start_time)} 초'\n",
    "\n",
    "xgb = XGBRegressor(n_estimators=1000, \n",
    "                   learning_rate=0.01, \n",
    "                   subsample=0.8, \n",
    "                   colsample_bytree=0.8,\n",
    "                   objective='reg:squarederror', \n",
    "                  )\n",
    "\n",
    "print(model_test('xgb (cpu)', xgb))\n",
    "\n",
    "xgb = XGBRegressor(n_estimators=1000, \n",
    "                   learning_rate=0.01, \n",
    "                   subsample=0.8, \n",
    "                   colsample_bytree=0.8,\n",
    "                   objective='reg:squarederror', \n",
    "                   tree_method='gpu_hist')\n",
    "\n",
    "print(model_test('xgb (gpu)', xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동영상 변환 및 리사이징이 완료되었습니다. 저장 경로: ./resized_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, avi, from '/data_1/seclab_nahyun/UCF101/UCF-101/BaseballPitch/v_BaseballPitch_g11_c04.avi':\n",
      "  Metadata:\n",
      "    encoder         : Lavf52.31.1\n",
      "  Duration: 00:00:01.94, start: 0.000000, bitrate: 489 kb/s\n",
      "    Stream #0:0: Video: mpeg4 (Simple Profile) (xvid / 0x64697678), yuv420p, 320x240 [SAR 1:1 DAR 4:3], 467 kb/s, 29.97 fps, 29.97 tbr, 29.97 tbn, 29.97 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> mpeg4 (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[mpeg4 @ 0x5626cc8b1d40] too many threads/slices (16), reducing to 15\n",
      "Output #0, rawvideo, to 'temp.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0: Video: mpeg4, yuv420p, 320x240 [SAR 1:1 DAR 4:3], q=2-31, 200 kb/s, 29.97 fps, 29.97 tbn, 29.97 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc57.107.100 mpeg4\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
      "frame=   58 fps=0.0 q=4.6 Lsize=     110kB time=00:00:01.93 bitrate= 467.6kbits/s speed=  46x    \n",
      "video:110kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\n",
      "ffmpeg version 3.4.11-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, m4v, from 'temp.mp4':\n",
      "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: mpeg4 (Simple Profile), yuv420p, 320x240 [SAR 1:1 DAR 4:3], 29.97 fps, 29.97 tbr, 1200k tbn, 30k tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> mpeg4 (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, rawvideo, to './resized_video.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0: Video: mpeg4, yuv420p, 256x256 [SAR 4:3 DAR 4:3], q=2-31, 200 kb/s, 29.97 fps, 29.97 tbn, 29.97 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc57.107.100 mpeg4\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: -1\n",
      "frame=   58 fps=0.0 q=7.0 Lsize=     110kB time=00:00:01.93 bitrate= 464.4kbits/s speed=49.1x    \n",
      "video:110kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "def convert_resize_video(input_path, output_path, target_resolution=(256, 256), bitrate='1000k'):\n",
    "    try:\n",
    "        # 임시 파일 경로 설정\n",
    "        temp_mp4_path = 'temp.mp4'\n",
    "\n",
    "        # Step 1: .avi를 .mp4로 변환\n",
    "        cmd1 = [\n",
    "            'ffmpeg',\n",
    "            '-i', input_path,           # 입력 파일 경로\n",
    "            '-c:v', 'mpeg4',          # 비디오 코덱 설정 \n",
    "            '-f', 'rawvideo',          \n",
    "#             '-b:v', bitrate,            # 비트레이트 설정\n",
    "            '-y',                       # 덮어쓰기 허용\n",
    "            temp_mp4_path                # 임시 출력 파일 경로 (.mp4)\n",
    "        ]\n",
    "\n",
    "        # FFmpeg 명령어 실행 (Step 1)\n",
    "        subprocess.run(cmd1, check=True)\n",
    "\n",
    "        # Step 2: 리사이징 및 재인코딩\n",
    "        cmd2 = [\n",
    "            'ffmpeg',\n",
    "            '-i', temp_mp4_path,                 # 입력 파일 경로 (임시 .mp4 파일)\n",
    "            '-vf', f'scale={target_resolution[0]}:{target_resolution[1]}',  # 리사이징\n",
    "            '-c:v', 'mpeg4',                   # 비디오 코덱 설정 \n",
    "            '-f', 'rawvideo',          \n",
    "#             '-b:v', bitrate,                     # 비트레이트 설정\n",
    "            '-y',                                # 덮어쓰기 허용\n",
    "            output_path                          # 출력 파일 경로\n",
    "        ]\n",
    "\n",
    "        # FFmpeg 명령어 실행 (Step 2)\n",
    "        subprocess.run(cmd2, check=True)\n",
    "\n",
    "        # 임시 파일 삭제\n",
    "        subprocess.run(['rm', temp_mp4_path])\n",
    "\n",
    "        print(f'동영상 변환 및 리사이징이 완료되었습니다. 저장 경로: {output_path}')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f'오류 발생: {e}')\n",
    "    except Exception as e:\n",
    "        print(f'알 수 없는 오류 발생: {e}')\n",
    "\n",
    "# 사용 예시\n",
    "input_video_path = '/data_1/seclab_nahyun/UCF101/UCF-101/BaseballPitch/v_BaseballPitch_g11_c04.avi'\n",
    "output_video_path = './resized_video.mp4'\n",
    "convert_resize_video(input_video_path, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 256 256\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/seclab_nahyun/.local/lib/python3.10/site-packages/coviar-0.1-py3.10-linux-x86_64.egg')\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from coviar import load, get_num_frames\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "cap = cv2.VideoCapture(output_video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "cap.release()\n",
    "\n",
    "# 비디오의 총 프레임 수를 구하는 함수 (이전 코드에서 정의되어 있어야 함)\n",
    "total_frames = get_num_frames(output_video_path)\n",
    "print(total_frames, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from coviar import load, get_num_frames\n",
    "import os\n",
    "\n",
    "def visualize_motion_vector(mv, stride=20):\n",
    "    Y, X, _ = mv.shape\n",
    "    x, y = np.meshgrid(np.arange(X), np.arange(Y))\n",
    "    u = mv[..., 0]\n",
    "    v = mv[..., 1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.quiver(x[::stride, ::stride], y[::stride, ::stride], u[::stride, ::stride], v[::stride, ::stride], angles='xy', scale_units='xy', scale=1, color='r')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "def compute_motion_vectors_and_weight_maps(video_path, GOP_SIZE, height, width, num_frame):\n",
    "    representation_type = 1  # 모션 벡터를 위한 값\n",
    "    accumulate = True  # 누적된 표현을 위한 값\n",
    "\n",
    "    \"\"\"\n",
    "    비디오에서 모션 벡터와 가중치 맵을 계산하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "    - video_path: 비디오 파일의 경로\n",
    "    - GOP_SIZE: GOP의 크기\n",
    "\n",
    "    Returns:\n",
    "    - all_motion_vectors: 각 프레임의 모션 벡터 텐서\n",
    "    - weight_maps: 각 GOP의 가중치 맵 텐서\n",
    "    \"\"\"\n",
    "\n",
    "    total_frames = num_frame\n",
    "    print('total_frames!', total_frames)\n",
    "    print('video_path!', video_path, height, width)\n",
    "    \n",
    "    if num_frame % GOP_SIZE == 0:\n",
    "        num_iframe = num_frame // GOP_SIZE\n",
    "    else:\n",
    "        num_iframe = num_frame // GOP_SIZE + 1\n",
    "    \n",
    "    # 각 프레임에 대한 가중치 맵과 모션 벡터 텐서 초기화\n",
    "    weight_maps_tensor = torch.zeros((num_iframe, 3, height, width))\n",
    "    all_motion_vectors_tensor = torch.zeros((total_frames, height, width, 2))\n",
    "\n",
    "    for gop_start in range(0, total_frames, GOP_SIZE):\n",
    "        weight_map = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "        for offset in range(1, GOP_SIZE):  # I 프레임 제외\n",
    "            frame_index = gop_start + offset\n",
    "            \n",
    "            if frame_index >= total_frames:\n",
    "                break\n",
    "            \n",
    "#             print('frame_index:', frame_index)\n",
    "            try:\n",
    "                mv = load(video_path, frame_index // GOP_SIZE, frame_index % GOP_SIZE, representation_type, accumulate)\n",
    "                \n",
    "                # mv의 유효성 확인\n",
    "                if mv is None or mv.size == 0:\n",
    "                    raise ValueError(f\"Invalid motion vector data at frame {frame_index}\")\n",
    "\n",
    "                # mv의 각 채널을 float32로 변환하고 개별적으로 리사이즈\n",
    "                mv_x = cv2.resize(mv[..., 0].astype(np.float32), (height, width))\n",
    "                mv_y = cv2.resize(mv[..., 1].astype(np.float32), (height, width))\n",
    "\n",
    "                # 리사이즈된 채널들을 다시 합치기\n",
    "                mv = np.stack([mv_x, mv_y], axis=-1)\n",
    "\n",
    "                # 모션 벡터의 크기 (움직임의 거리)를 계산하고 가중치 맵에 누적합니다.\n",
    "                magnitude = np.sqrt(mv[:,:,0]**2 + mv[:,:,1]**2)\n",
    "                weight_map += magnitude\n",
    "\n",
    "                # 모션 벡터 저장\n",
    "                all_motion_vectors_tensor[frame_index] = torch.tensor(mv)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred at frame {frame_index}:\", e)\n",
    "                mv = np.zeros((height, width, 2), dtype=np.int32)  # 0벡터 할당\n",
    "\n",
    "        # RGB 채널 각각에 동일한 가중치 맵을 복사\n",
    "        for channel in range(3):\n",
    "            weight_maps_tensor[gop_start // GOP_SIZE, channel] = torch.tensor(weight_map)\n",
    "    print('weight_maps_tensor', weight_maps_tensor.shape, 'all_motion_vectors_tensor', all_motion_vectors_tensor.shape)\n",
    "    \n",
    "    return all_motion_vectors_tensor, weight_maps_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('./semantic-segmentation-pytorch/')\n",
    "\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "from mit_semseg.lib.nn import user_scattered_collate, async_copy_to\n",
    "from mit_semseg.lib.utils import as_numpy\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "def perform_segmentation_with_interval(original_tensor, interval=12, device='cuda'):\n",
    "    # Load the pre-trained model\n",
    "    builder = ModelBuilder()\n",
    "    net_encoder = builder.build_encoder(\n",
    "        arch='resnet50dilated',\n",
    "        fc_dim=2048,\n",
    "        weights='./semantic-segmentation-pytorch/ckpt/encoder_epoch_20.pth')\n",
    "    net_decoder = builder.build_decoder(\n",
    "        arch='ppm_deepsup',\n",
    "        fc_dim=2048,\n",
    "        num_class=150,\n",
    "        weights='./semantic-segmentation-pytorch/ckpt/decoder_epoch_20.pth',\n",
    "        use_softmax=True)\n",
    "\n",
    "    crit = torch.nn.NLLLoss(ignore_index=-1)\n",
    "    segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
    "    segmentation_module.eval()\n",
    "    segmentation_module.to(device)\n",
    "    \n",
    "    # Define the image transform\n",
    "    transform_img = T.Compose([\n",
    "        T.Resize(512),\n",
    "        T.ToTensor(), \n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Ensure the tensor is on the GPU\n",
    "    original_tensor = original_tensor.to(device)\n",
    "\n",
    "    # Initialize the scores tensor on CPU\n",
    "    num_frames = original_tensor.size(0)\n",
    "    selected_frames = list(range(0, num_frames, interval))\n",
    "    scores = torch.zeros(len(selected_frames), 150, original_tensor.size(2), original_tensor.size(3))\n",
    "\n",
    "    # Perform segmentation for selected frames\n",
    "    with torch.no_grad():  # Use no_grad to save GPU memory\n",
    "        for i, frame_idx in enumerate(selected_frames):\n",
    "            img = original_tensor[frame_idx].cpu().numpy()\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "            img = Image.fromarray(np.uint8(img))\n",
    "            img = img.convert('RGB')\n",
    "            segSize = (img.size[1], img.size[0])\n",
    "            \n",
    "            # 이미지를 텐서로 변환\n",
    "            img_transformed = transform_img(img)\n",
    "            img_transformed = img_transformed.unsqueeze(0).cuda()\n",
    "\n",
    "            feed_dict = {'img_data': img_transformed}\n",
    "            pred_tmp = segmentation_module(feed_dict, segSize=segSize)\n",
    "            scores[i] = pred_tmp\n",
    "\n",
    "    # Move the scores tensor to GPU if needed\n",
    "    scores = scores.to(device)\n",
    "\n",
    "    _, pred = torch.max(scores, dim=1)\n",
    "\n",
    "    return pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def create_mask_for_stuff_pixels(segmentation_tensor, color_encoding_path='./color_coding_semantic_segmentation_classes.csv'):\n",
    "    \"\"\"\n",
    "    Create a mask to remove pixels with 'Stuff' values in the segmentation tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - segmentation_tensor: A tensor of shape (num_frames, height, width) containing the segmentation results\n",
    "    - color_encoding_path: Path to the CSV file containing color encodings for semantic segmentation classes\n",
    "\n",
    "    Returns:\n",
    "    - masks: A tensor of shape (num_frames, height, width) containing the masks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the color encoding CSV\n",
    "    color_df = pd.read_csv(color_encoding_path) \n",
    "\n",
    "    # Extract the Idx values where Stuff is 1\n",
    "    stuff_pixel_values = color_df[color_df['Stuff'] == 1]['Idx'].tolist()\n",
    "\n",
    "    # Create a mask where pixels with stuff values are set to 0, others are set to 1\n",
    "    masks = np.ones_like(segmentation_tensor)\n",
    "    for value in stuff_pixel_values:\n",
    "        masks[segmentation_tensor == value - 1] = 0\n",
    "\n",
    "    # Convert the numpy array to a PyTorch tensor\n",
    "    masks_tensor = torch.tensor(masks, dtype=torch.float32)\n",
    "\n",
    "    return masks_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def visualize_tensor_image(image_tensor):\n",
    "    \"\"\"\n",
    "    Visualize a given image tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - image_tensor: A tensor of shape [3, H, W]\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the tensor is on GPU, if yes, move it to CPU\n",
    "    if image_tensor.device != torch.device('cpu'):\n",
    "        image_tensor = image_tensor.cpu()\n",
    "\n",
    "    # Convert the tensor to numpy array\n",
    "    image_array = image_tensor.numpy()\n",
    "\n",
    "    # Transpose the array to shape [H, W, 3] for visualization\n",
    "    image_array = image_array.transpose(1, 2, 0)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image_array)\n",
    "    plt.axis('off')  # Hide axes for better visualization\n",
    "    plt.show()\n",
    "\n",
    "def visualize_motion_vectors(motion_tensor):\n",
    "    \"\"\"\n",
    "    Visualize motion vectors for each GOP and frame.\n",
    "\n",
    "    Parameters:\n",
    "    - motion_tensor: A tensor of shape [num_gops, num_frames, 3, H, W]\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    num_gops, num_frames, _, _, _ = motion_tensor.shape\n",
    "\n",
    "    # Check if the tensor is on GPU, if yes, move it to CPU\n",
    "    if motion_tensor.device != torch.device('cpu'):\n",
    "        motion_tensor = motion_tensor.cpu()\n",
    "\n",
    "    for gop_idx in range(num_gops):\n",
    "        for frame_idx in range(num_frames):\n",
    "            # Convert the tensor to numpy array\n",
    "            image_array = motion_tensor[gop_idx, frame_idx].numpy()\n",
    "\n",
    "            # Transpose the array to shape [H, W, 3] for visualization\n",
    "            image_array = image_array.transpose(1, 2, 0)\n",
    "\n",
    "            # Display the image\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(image_array)\n",
    "            plt.title(f\"GOP {gop_idx + 1}, Frame {frame_idx + 1}\")\n",
    "            plt.axis('off')  # Hide axes for better visualization\n",
    "            plt.show()\n",
    "\n",
    "# 최종 노이즈 시각화\n",
    "def visualize_warped_frames(warped_frames):\n",
    "    \"\"\"\n",
    "    Visualize the given warped frames (noise).\n",
    "\n",
    "    Parameters:\n",
    "    - warped_frames: Tensor of shape [num_frames, C, H, W]\n",
    "    \"\"\"\n",
    "    \n",
    "    num_frames, C, H, W = warped_frames.shape\n",
    "    \n",
    "    # Ensure the tensor is on CPU for visualization\n",
    "    warped_frames = warped_frames.cpu()\n",
    "\n",
    "    cols = 7  # 한 행에 7개의 프레임\n",
    "    rows = math.ceil(num_frames / cols)  # 필요한 행의 수 계산\n",
    "\n",
    "    # figsize의 값을 조절하여 전체 그림의 크기를 늘립니다.\n",
    "    plt.figure(figsize=(cols * 3, rows * 3))  # 각 프레임의 크기를 대략 3x3으로 설정\n",
    "\n",
    "    for idx in range(num_frames):\n",
    "        plt.subplot(rows, cols, idx+1)\n",
    "        frame = warped_frames[idx].numpy()\n",
    "        \n",
    "        frame = warped_frames[idx].numpy()\n",
    "        \n",
    "        # Transpose the array to shape [H, W, 3] for visualization\n",
    "        frame = frame.transpose(1, 2, 0)\n",
    "\n",
    "        # Clip the values to the range [0, 1]\n",
    "        frame = np.clip(frame, 0, 1)\n",
    "\n",
    "        # Normalize the frame for visualization\n",
    "#         frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
    "        \n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Frame {idx}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "# visualize_warped_frames(warped_frames_tensor)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_image(img):\n",
    "    \"\"\"\n",
    "    Visualize each frame.\n",
    "\n",
    "    Parameters:\n",
    "    - masks: A tensor of shape (num_frames, height, width) containing the masks\n",
    "    \"\"\"\n",
    "    \n",
    "    num_frames = img.shape[0]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title('image')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "        \n",
    "def visualize_images(masks):\n",
    "    \"\"\"\n",
    "    Visualize each frame.\n",
    "\n",
    "    Parameters:\n",
    "    - masks: A tensor of shape (num_frames, height, width) containing the masks\n",
    "    \"\"\"\n",
    "    \n",
    "    num_frames = masks.shape[0]\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(masks[i], cmap='gray')\n",
    "        plt.title(f'Mask for Frame {i+1}')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "def visualize_noise_frames(noise_frames):\n",
    "    \"\"\"\n",
    "    Visualize each frame in the noise_frames tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - noise_frames: A tensor of shape (num_frames, channels, height, width)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 텐서의 크기와 프레임 수를 가져옵니다.\n",
    "    num_frames = noise_frames.size(0)\n",
    "\n",
    "    # 각 프레임을 시각화합니다.\n",
    "    for i in range(num_frames):\n",
    "        frame = noise_frames[i].cpu().numpy().transpose(1, 2, 0)  # (channels, height, width) -> (height, width, channels)\n",
    "        \n",
    "        # 이미지를 표시합니다.\n",
    "        plt.imshow(frame)\n",
    "        plt.title(f\"Frame {i+1}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "def visualize_video(video_array):\n",
    "    \"\"\"\n",
    "    주어진 numpy.ndarray 동영상을 시각화하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "    - video_array: (num_frame, height, width, 3) shape의 numpy.ndarray 동영상\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    num_frames, height, width, _ = video_array.shape\n",
    "\n",
    "    for frame_idx in range(num_frames):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(video_array[frame_idx])\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Frame {frame_idx + 1}/{num_frames}\")\n",
    "        plt.show()\n",
    "#         input(\"Press Enter to continue to the next frame...\")\n",
    "\n",
    "# 예제\n",
    "# video = np.random.rand(5, 240, 320, 3)  # 임의의 동영상 데이터 생성\n",
    "# visualize_video_without_widgets(video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Motion Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def normalize_grid(grid):\n",
    "    # 그리드의 최대값과 최소값을 가져옵니다.\n",
    "    max_val = grid.max()\n",
    "    min_val = grid.min()\n",
    "\n",
    "    # 그리드 값을 [-1, 1] 범위로 정규화\n",
    "    normalized_grid = 2 * (grid - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "    return normalized_grid\n",
    "\n",
    "# 이 함수는 exp_noise를 각 GOP의 첫 번째 프레임에 설정하고, 나머지 프레임들에 대해 해당 프레임의 모션 벡터를 사용하여 노이즈를 변형합니다. \n",
    "# 결과적으로, 각 GOP의 모든 프레임에 대한 노이즈가 생성됩니다.\n",
    "def apply_motion_vector_to_iframe_tensor(iframe, motion_vector, scale_factor, grid_density):\n",
    "    C, H, W = iframe.shape\n",
    "#     print(iframe.shape)\n",
    "#     visualize_tensor_image(iframe)\n",
    "\n",
    "    # Create a dense grid\n",
    "    x = torch.linspace(0, W-1, W*grid_density).unsqueeze(0).repeat(H*grid_density, 1)\n",
    "    y = torch.linspace(0, H-1, H*grid_density).unsqueeze(1).repeat(1, W*grid_density)\n",
    "\n",
    "    # Resize motion vectors to match the dense grid\n",
    "    mv_x = F.interpolate(motion_vector[..., 0].unsqueeze(0).unsqueeze(0), size=(H*grid_density, W*grid_density), mode='bilinear', align_corners=True).squeeze()\n",
    "    mv_y = F.interpolate(motion_vector[..., 1].unsqueeze(0).unsqueeze(0), size=(H*grid_density, W*grid_density), mode='bilinear', align_corners=True).squeeze()\n",
    "\n",
    "    # Apply motion vectors to the grid\n",
    "    x_new = x - scale_factor * mv_x\n",
    "    y_new = y - scale_factor * mv_y\n",
    "\n",
    "    grid = torch.stack((x_new, y_new), dim=2).unsqueeze(0)\n",
    "    \n",
    "    # 그리드 값을 확인하고 필요한 경우 정규화\n",
    "#     print(\"Before normalization:\", grid.min(), grid.max())\n",
    "    grid = normalize_grid(grid)\n",
    "#     print(\"After normalization:\", grid.min(), grid.max())\n",
    "    \n",
    "    # Move the grid tensor to the same device as iframe\n",
    "    grid = grid.to(iframe.device)\n",
    "    \n",
    "    # Use grid_sample to warp the frame\n",
    "    warped_frame = F.grid_sample(iframe.unsqueeze(0), grid, mode='bilinear', padding_mode='reflection', align_corners=True).squeeze()\n",
    "    \n",
    "\n",
    "    # Resize the warped frame to the original resolution\n",
    "    warped_frame = F.interpolate(warped_frame.unsqueeze(0), size=(H, W), mode='bilinear', align_corners=True).squeeze()\n",
    "\n",
    "#     visualize_tensor_image(warped_frame)\n",
    "    return warped_frame\n",
    "\n",
    "\n",
    "def apply_motion_vectors_to_all_iframes(iframes, motion_vectors, scale_factor=1.0, grid_density=4):\n",
    "    \"\"\"\n",
    "    Apply motion vectors to all I-frames.\n",
    "\n",
    "    Parameters:\n",
    "    - iframes: Tensor of shape [num_gops, C, H, W]\n",
    "    - motion_vectors: Tensor of shape [total_frames, H, W, 2]\n",
    "\n",
    "    Returns:\n",
    "    - warped_frames: Tensor of shape [total_frames, C, H, W]\n",
    "    \"\"\"\n",
    "    \n",
    "    num_gops, C, H, W = iframes.shape\n",
    "    total_frames = motion_vectors.shape[0]\n",
    "    GOP_SIZE = total_frames // num_gops\n",
    "    \n",
    "    # Initialize the result tensor\n",
    "    warped_frames = torch.zeros((total_frames, C, H, W), device=iframes.device)\n",
    "\n",
    "    frame_counter = 0\n",
    "    for gop_idx in range(num_gops):\n",
    "        # Set the first frame of each GOP as the I-frame\n",
    "        warped_frames[frame_counter] = iframes[gop_idx]\n",
    "        frame_counter += 1\n",
    "        \n",
    "        for _ in range(GOP_SIZE - 1):\n",
    "            if frame_counter < total_frames:  # Ensure we don't exceed the total number of frames\n",
    "                mv = motion_vectors[frame_counter]\n",
    "                iframe = iframes[gop_idx]\n",
    "                \n",
    "                warped = apply_motion_vector_to_iframe_tensor(iframe, mv, scale_factor, grid_density)\n",
    "                warped_frames[frame_counter] = warped\n",
    "                frame_counter += 1\n",
    "\n",
    "    return warped_frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seclab_nahyun/anaconda3/envs/workspace/lib/python3.10/site-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.9.1` and `torch==1.12.1+cu113` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.\n",
      "  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "import torch\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.data.transforms import video\n",
    "from gluoncv.utils import split_and_load\n",
    "from gluoncv.data.dataloader import tsn_mp_batchify_fn\n",
    "import imageio\n",
    "import subprocess\n",
    "\n",
    "transform_post = video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "def transform_input_to_mxnet(input_tensor):\n",
    "#     # PyTorch 텐서를 MXNet NDArray로 변환\n",
    "#     input_ndarray = mgs_ = transform_post(input_tensor)\n",
    "#     data = input_tensor.reshape((-1,) + (args.new_length, 3, args.input_size, args.input_size))\n",
    "#     data = data.reshape((-1,) + (imgs.shape[0] , 3, 256, 256))\n",
    "\n",
    "    # 입력 텐서의 shape를 가져옵니다.\n",
    "    clip_length, num_channels, height, width = input_tensor.shape\n",
    "    \n",
    "    # 배치 차원과 깊이 차원을 추가합니다.\n",
    "    input_data = input_tensor.reshape((1, num_channels, clip_length, height, width))\n",
    "    \n",
    "    min_val = np.min(input_data, axis=(2, 3, 4), keepdims=True)\n",
    "    max_val = np.max(input_data, axis=(2, 3, 4), keepdims=True)\n",
    "    normalized_data = (input_data - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return normalized_data\n",
    "\n",
    "def check_video_with_ffmpeg(video_path):\n",
    "    \"\"\"\n",
    "    ffmpeg를 사용하여 비디오 파일의 유효성을 검사합니다.\n",
    "    \"\"\"\n",
    "    cmd = ['ffmpeg', '-v', 'error', '-i', video_path, '-f', 'null', '-']\n",
    "    proc = subprocess.Popen(cmd, stderr=subprocess.PIPE)\n",
    "    _, err = proc.communicate()\n",
    "    if proc.returncode != 0:\n",
    "        return False, err.decode('utf-8')\n",
    "    return True, None\n",
    "\n",
    "def validate_video_file(video_path):\n",
    "    \"\"\"\n",
    "    비디오 파일의 유효성을 검사하는 함수.\n",
    "    \"\"\"\n",
    "    is_valid, error_msg = check_video_with_ffmpeg(video_path)\n",
    "    if not is_valid:\n",
    "        return False, error_msg\n",
    "    \n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            cap.release()\n",
    "            return False, \"비디오 파일을 열 수 없습니다.\"\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            cap.release()\n",
    "            return False, \"비디오 프레임을 읽을 수 없습니다.\"\n",
    "        \n",
    "        cap.release()\n",
    "        return True, \"\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "\n",
    "def save_images_with_imageio(num_frame, original_images, adv_images, save_path):\n",
    "    for i in range(num_frame):\n",
    "        original_image = (original_images[i] * 255).astype('uint8')\n",
    "        adv_image = (adv_images[i] * 255).astype('uint8')\n",
    "        \n",
    "        original_image_path = f\"{save_path}/original_{i}.png\"\n",
    "        adv_image_path = f\"{save_path}/adv_{i}.png\"\n",
    "        \n",
    "        imageio.imwrite(original_image_path, original_image)\n",
    "        imageio.imwrite(adv_image_path, adv_image)\n",
    "        \n",
    "\n",
    "# transform_post = video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "def normalization(imgs):\n",
    "    batch_size = imgs.shape[0]\n",
    "    input_data = mx.nd.array(imgs)\n",
    "    data = np.stack(imgs, axis=0)\n",
    "    data = data.reshape((-1,) + (imgs.shape[0] , 3, 256, 256))\n",
    "    data = np.transpose(data, (0, 2, 1, 3, 4))\n",
    "    print(\"normalized imgs shape:\", imgs.shape)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def CWLoss(logits, target, kappa=0):\n",
    "    logits = F.softmax(logits, dim=1)\n",
    "\n",
    "    target_onehot = torch.zeros(1, 174).cuda()\n",
    "    target_onehot[0, target] = 1\n",
    "    real = (target_onehot * logits).sum(1)[0]\n",
    "    tmp_logit = ((1. - target_onehot) * logits - target_onehot*10000.)\n",
    "\n",
    "    other, other_class = logits.max(1)\n",
    "    sort_prob, sort_class = logits.sort()\n",
    "    second_logit = sort_pro|b[0][-2].unsqueeze(0)\n",
    "    second_class = sort_class[0][-2].unsqueeze(0)\n",
    "\n",
    "    return torch.clamp(torch.sum(logits)-second_logit, kappa), target.item(), real.item(), other.item(), other_class.item(), second_logit.item(), second_class.item()\n",
    "    #return torch.clamp(other-5*real, kappa), target.item(), real.item(), other.item(), other_class.item(), second_logit.item(), second_class.item()\n",
    "\n",
    "def Cross_Entropy(logits, target, kappa=0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    other, other_class = logits.max(1)\n",
    "    loss = criterion(logits, other_class.long())\n",
    "    #pdb.set_trace()\n",
    "    return loss, loss.item(), loss.item(), loss.item(), loss.item(), loss.item(), loss.item()\n",
    "\n",
    "\n",
    "def norm2(x):\n",
    "    assert len(x.shape) == 4\n",
    "    norm_vec = torch.sqrt(x.float().pow(2).sum(dim=[1,2,3])).view(-1, 1, 1, 1)\n",
    "    norm_vec += (norm_vec == 0).float()*1e-8\n",
    "    return norm_vec\n",
    "\n",
    "def _pert_loss(logits, ori_label, target_label, delta_motion):\n",
    "    cw_loss = CWLoss\n",
    "    #cw_loss = Cross_Entropy\n",
    "    loss, target, real, other, other_class, second_logit, second_class = cw_loss(logits, target_label)\n",
    "    loss = loss.squeeze(0)\n",
    "    return loss, target, real, other, other_class, second_logit, second_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Box Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coviar import get_num_frames\n",
    "from coviar import load\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import pdb\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import mxnet.ndarray as FF\n",
    "import random\n",
    "\n",
    "GOP_SIZE = 12\n",
    "\n",
    "def _perturbation_image(model,\n",
    "                      original_image,\n",
    "                      ori_label,\n",
    "                      video_path,\n",
    "                      save_path,\n",
    "                      transform_post,\n",
    "                      args,\n",
    "                      config,\n",
    "                      device):\n",
    "\n",
    "    original_image = original_image.to(device)\n",
    "\n",
    "#     total_frames = get_num_frames(video_path)\n",
    "    original_image_ = original_image.clone()\n",
    "    \n",
    "    num_frame, channel, height, width = original_image.shape\n",
    "    dim = height * width * channel\n",
    "    loop = 0\n",
    "    inner_loop = 0\n",
    "    success = False\n",
    "    num_query = 0\n",
    "    num_pframe = 0\n",
    "\n",
    "    max_query = 60000\n",
    "    base_exploration = 0.1\n",
    "    fd_eta = 0.1\n",
    "    online_lr = 0.1\n",
    "    flow_lr = 0.025\n",
    "    \n",
    "#     target_label = torch.tensor([-1], device=devi)ce\n",
    "    ori_label = ori_label.to(device)\n",
    "    target_label = torch.tensor([random.sample(range(101), 1)[0]]).to(device)\n",
    "    while target_label == ori_label:\n",
    "        target_label = torch.tensor([random.sample(range(101), 1)[0]]).to(device)\n",
    "    print('target_label', target_label, 'ori_label', ori_label)\n",
    "    \n",
    "    if num_frame % GOP_SIZE == 0:\n",
    "        num_iframe = num_frame // GOP_SIZE\n",
    "    else:\n",
    "        num_iframe = num_frame // GOP_SIZE + 1\n",
    "\n",
    "    prior = torch.zeros(num_frame, channel, height, width).to(device)\n",
    "    delta = torch.zeros(num_frame, channel, height, width).to(device)\n",
    "    est_grad = torch.zeros(num_frame, channel, height, width).to(device)\n",
    "    adv_img = torch.zeros(3, num_frame, channel, height, width).to(device)\n",
    "    iframe = torch.zeros(num_frame, height, width, channel).to(device)\n",
    "    noise_frames = torch.zeros(num_frame, channel, height, width).to(device)\n",
    "    noise_iframes = torch.zeros(num_frame, channel, height, width).to(device)\n",
    "    segmentation_result = torch.zeros(num_iframe, height, width).to(device)\n",
    "    dynamic_exploration = torch.zeros(height, width).to(device)\n",
    "\n",
    "    index_visual = torch.zeros(num_frame, 2, height, width).to(device)\n",
    "    index_motion = torch.zeros(num_frame, height, width, 2).to(device)\n",
    "    \n",
    "    print('original_image:', original_image.shape)# original_image: torch.Size([103, 3, 256, 256])\n",
    "    \n",
    "    segmentation_result = perform_segmentation_with_interval(original_image, interval=12, device=device)\n",
    "    print('segmentation', segmentation_result.shape)\n",
    "    mask = create_mask_for_stuff_pixels(segmentation_result)\n",
    "    print('mask', mask.shape)\n",
    "    \n",
    "    motion_vectors, weight_maps = compute_motion_vectors_and_weight_maps(video_path, GOP_SIZE=12, height=height, width=width, num_frame=num_frame)\n",
    "    print('motion_vectors', motion_vectors.shape, 'weight_maps', weight_maps.shape)\n",
    "    \n",
    "#     # I-frame과 각 프레임 간의 Optical Flow 계산\n",
    "#     motion_vectors = calculate_optical_flow(video_path, cap, gop_size=GOP_SIZE)\n",
    "#     cap.release()\n",
    "\n",
    "    # exploration 값을 동적으로 조절, dynamic_exploration_tensor: torch.Size([9, 3, 256, 256])\n",
    "    dynamic_exploration = base_exploration * weight_maps \n",
    "    print(dynamic_exploration.shape)\n",
    "    \n",
    "    while not (num_query > max_query):\n",
    "        pred_adv_logit = list()\n",
    "        start1 = time.time()\n",
    "            \n",
    "        gop_index = loop // GOP_SIZE\n",
    "              \n",
    "#         population_size = 50  # 후보 적대적 예제의 수\n",
    "#         sigma = 0.1  # 노이즈의 표준 편차\n",
    "        \n",
    "#         # NES를 사용하여 그래디언트 추정\n",
    "#         noises = torch.randn(population_size, num_frame, channel, height, width).to(device) * sigma\n",
    "\n",
    "\n",
    "        # 노이즈 프레임 생성: iframe에 대한 랜덤 노이즈 프레임(noise_frames)을 생성\n",
    "        noise_frames = torch.randn(1, 3, height, width).repeat(num_iframe, 1, 1, 1).to(device)\n",
    "        print('noise_frames.shape', noise_frames.shape, mask.shape)\n",
    "        \n",
    "        # masks를 noise_frames와 동일한 shape으로 확장 \n",
    "        expanded_masks = mask.unsqueeze(1).expand_as(noise_frames)\n",
    "\n",
    "        # noise_frames에 마스크를 적용\n",
    "        masked_noise_frames = noise_frames * expanded_masks.to(device)\n",
    "\n",
    "        # dynamic_exploration 값을 사용하여 노이즈 프레임의 크기를 조절 노이즈 프레임을 모션 벡터와 결합하여 적대적 섭동을 생성\n",
    "        dynamic_exploration_tensor = dynamic_exploration.clone().detach().to(device)\n",
    "        print('dynamic_exploration_tensor:', dynamic_exploration_tensor.shape, 'masked_noise_frames:', masked_noise_frames.shape)\n",
    "        iframe_noise = dynamic_exploration_tensor * masked_noise_frames\n",
    "        applied_noise = apply_motion_vectors_to_all_iframes(iframe_noise, motion_vectors)\n",
    "        print('iframe_noise', iframe_noise.shape, 'applied_noise', applied_noise.shape)\n",
    "        \n",
    "        q1 = prior + applied_noise\n",
    "        q2 = prior - applied_noise\n",
    "        visualize_warped_frames(q2)\n",
    "        visualize_warped_frames(original_image)\n",
    "\n",
    "        adv_img[0] = original_image + fd_eta*q1/norm2(q1)\n",
    "        adv_img[1] = original_image + fd_eta*q2/norm2(q2)\n",
    "        adv_img[2] = original_image\n",
    "#         normalized_image = adv_img[0] / 255.0\n",
    "#         visualize_noise_frames(normalized_image)\n",
    "        \n",
    "        losses = []\n",
    "        for i in range(3):\n",
    "#             perturbed_image = original_image + applied_noise[i]\n",
    "            adv_img_np = adv_img[i].clone().cpu().numpy()\n",
    "            batch_size = adv_img_np.shape[0]\n",
    "            print('adv_img_np', adv_img_np.shape, '// type', type(adv_img_np))\n",
    "#             adv_img_np = transform_post(adv_img_np)\n",
    "            data = np.stack(adv_img_np, axis=0)\n",
    "            print('data', data.shape, '// type', type(data))\n",
    "#             adv_img_tensor = torch.tensor(np.array(adv_img_np)).to(device)\n",
    "#             adv_img[i] = adv_img_tensor\n",
    "#             print('adv_img_tensor', adv_img_tensor.shape, '// type', adv_img_tensor.type())\n",
    "            data = transform_input_to_mxnet(data)\n",
    "            # 예측\n",
    "#             adv_img[i] = adv_img[i].to(device)\n",
    "            data = data.astype('float32', copy=False)\n",
    "            data = mx.nd.array(data)\n",
    "#             data =  torch.tensor(data)\n",
    "            print('data', data.shape, '// type', type(data))\n",
    "            pred = model(data)\n",
    "            print(pred)\n",
    "            \n",
    "            \n",
    "            \n",
    "            tmp_result = FF.mean(pred, axis=0, keepdims=True)\n",
    "            tmp_result = torch.from_numpy(tmp_result.asnumpy()).to(device)\n",
    "            print(tmp_result)\n",
    "            pred_classes = pred.argmax(axis=1).asnumpy().tolist()\n",
    "\n",
    "            # 결과 출력\n",
    "            for path, pred_class in zip(video_paths, pred_classes):\n",
    "                print(f\"Video: {path}, Predicted Class: {pred_class}\")\n",
    "            logits = model(perturbed_image)\n",
    "            print(logits.shape)\n",
    "            loss = criterion(logits, target_label)  # 손실 함수 정의 필요\n",
    "            losses.append(loss.item())\n",
    "        losses = torch.tensor(losses).to(device)\n",
    "        weighted_noises = (losses - losses.mean()) / (losses.std() + 1e-10)  # 정규화\n",
    "        est_grad = torch.mean(weighted_noises.view(population_size, 1, 1, 1, 1) * noises, dim=0)\n",
    "\n",
    "        # 그래디언트를 사용하여 적대적 이미지 업데이트\n",
    "        delta = online_lr * est_grad.sign()\n",
    "        original_image = torch.clamp(original_image + delta, 0, 1)\n",
    "\n",
    "\n",
    "#         l1, _, _, _, _, _, _ = _pert_loss(pred_adv_logit[0], ori_label, target_label, delta)\n",
    "#         l2, _, _, _, _, _, _ = _pert_loss(pred_adv_logit[1], ori_label, target_label, delta)\n",
    "#         loss, target, real, other, other_class, second_logit, second_class = _pert_loss(pred_adv_logit[2], ori_label, target_label, delta)\n",
    "\n",
    "#         num_query += 3\n",
    "# #         est_deriv = (l1-l2)/(fd_eta*dynamic_exploration*dynamic_exploration)\n",
    "# #         est_grad = est_deriv.item() * exp_noise\n",
    "# #         prior += online_lr * est_grad\n",
    "\n",
    "# #         original_image = original_image - flow_lr*prior.sign()\n",
    "# #         delta = original_image_ - original_image\n",
    "# #         tmp_norm = norm2(delta)\n",
    "# #         original_image = torch.max(torch.min(original_image, original_image_ + 0.03), original_image_ - 0.03)\n",
    "# #         original_image = torch.clamp(original_image, 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "        # 적대적 공격의 성공 여부 판단:\n",
    "        pred_adv_label = pred_adv_logit[2].argmax()\n",
    "        if (loop % 1000 ==0) or (loop == max_query) or pred_adv_label != ori_label:\n",
    "        #if (loop % 1000 ==0) or (loop == max_query) or pred_adv_label == target_label:\n",
    "            print('[T2]{:.3f}s for [{}]-th loop\\t'\n",
    "                  'Queries {:03d}\\t'\n",
    "                  'Overall loss {:.3f}\\t'\n",
    "                  'est_deriv {:.3f}\\t'\n",
    "                  'Target {}\\t'\n",
    "                  'Target logit {:.3f}\\t'\n",
    "                  'ori logit {:.3f}\\t'\n",
    "                  'ori class {}\\t'\n",
    "                  'second logit {:.3f}\\t'\n",
    "                  'second class {}\\t'.format(time.time() - start1, loop,\n",
    "                                            num_query, loss, est_deriv.item(), target,\n",
    "                                            real, other, other_class, second_logit, second_class))\n",
    "        loop += 1\n",
    "        if pred_adv_label != ori_label:\n",
    "        #if pred_adv_label == target_label:\n",
    "            #print('Predicted label is {}\\t'.format(pred_adv_label))\n",
    "            diff = adv_img[2] - original_image_\n",
    "            print('diff max {:.3f}, diff min {:.3f}'.format(diff.max(), diff.min()))\n",
    "            success = True\n",
    "            #save_images(num_frame, original_image_.cpu().permute(0,2,3,1).numpy(), adv_img[2].cpu().permute(0,2,3,1).numpy(), save_path)\n",
    "            break\n",
    "\n",
    "        if num_query >= max_query:\n",
    "            #save_images(num_frame, original_image_.cpu().permute(0,2,3,1).numpy(), adv_img[2].cpu().permute(0,2,3,1).numpy(), save_path)\n",
    "            break\n",
    "    return pred_adv_label, num_query, success\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_video\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# 커스텀 전처리 함수 정의\n",
    "def custom_transform(frame):\n",
    "    if isinstance(frame, torch.Tensor):  # 이미 텐서인 경우 스킵\n",
    "        return frame\n",
    "    return ToTensor()(frame)\n",
    "\n",
    "# UCF101Dataset 클래스 수정\n",
    "class UCF101Dataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.video_list = []  # 비디오 파일 목록\n",
    "        self.labels = []  # 레이블 목록\n",
    "        self.transform = transform\n",
    "\n",
    "        # 데이터 디렉토리에서 레이블 및 비디오 파일 수집\n",
    "        for label, class_folder in enumerate(os.listdir(data_dir)):\n",
    "            class_dir = os.path.join(data_dir, class_folder)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for video_file in os.listdir(class_dir):\n",
    "                    if video_file.endswith('.avi'):\n",
    "                        self.video_list.append(os.path.join(class_dir, video_file))\n",
    "                        self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_list[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 비디오 파일을 읽고 모든 프레임을 추출\n",
    "        video_frames, _, _ =  read_video(video_path, pts_unit='sec')\n",
    "        video_frames = video_frames.permute(0, 3, 1, 2)\n",
    "        if self.transform:\n",
    "            video_frames = [self.transform(frame) for frame in video_frames]\n",
    "        video_tensor = torch.stack(video_frames)\n",
    "\n",
    "\n",
    "        return video_tensor, video_path, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 로딩\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m모델 로딩\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "print('모델 로딩')\n",
    "loaded_model = tf.keras.models.load_model('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import os\n",
    "# import torch\n",
    "# import gluoncv\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.io import read_video\n",
    "# from torchvision.transforms import ToTensor\n",
    "# from gluoncv.model_zoo import get_model\n",
    "\n",
    "# def transform_input(input_tensor):\n",
    "\n",
    "#     # 입력 텐서의 shape를 가져옵니다.\n",
    "#     _, clip_length, height, width, num_channels = input_tensor.shape\n",
    "    \n",
    "#     # 배치 차원과 깊이 차원을 추가합니다.\n",
    "#     transformed_ndarray = input_tensor.reshape((1, num_channels, clip_length, height, width))\n",
    "    \n",
    "#     return transformed_ndarray\n",
    "\n",
    "# # 전처리 파이프라인 정의\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((224, 224)),  # 모델의 입력 크기에 따라 조정\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet 통계를 기반으로 정규화\n",
    "# ])\n",
    "\n",
    "\n",
    "# # 전처리 설정\n",
    "# transform_post = video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# # UCF101 데이터셋 로드\n",
    "# data_dir = '/data_1/seclab_nahyun/UCF101/UCF-101'\n",
    "# ucf101_dataset = UCF101Dataset(data_dir, transform=transform)\n",
    "\n",
    "# # DataLoader 설정\n",
    "# batch_size = 1\n",
    "# data_loader = DataLoader(ucf101_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # 메모리 정리\n",
    "# gc.collect()\n",
    "\n",
    "# # 모델 로딩\n",
    "# model = get_model(name='i3d_resnet50_v1_ucf101',\n",
    "#                   nclass=101,\n",
    "#                   pretrained=True,\n",
    "#                   num_segments=2)\n",
    "\n",
    "\n",
    "# # 모델 디바이스 설정\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # 예측 수행\n",
    "# model.cast('float32')\n",
    "# model.hybridize(static_alloc=True, static_shape=True)\n",
    "# count = 0\n",
    "# with torch.no_grad():\n",
    "#     for video_frames, video_path, label in data_loader:\n",
    "#         if video_frames is None:\n",
    "#             continue\n",
    "#         count += 1\n",
    "#         if count == 3:\n",
    "#             break\n",
    "            \n",
    "#         output_video_path = \"./resized_video.mp4\"  # 출력 비디오 파일 경로\n",
    "#         target_size = (256, 256)  # 목표 크기\n",
    "#         print(video_path[0])\n",
    "        \n",
    "# #         video_frames=tf.constant(video_frames.numpy())\n",
    "# #         video_frames = tf.transpose(video_frames, perm=[0, 1, 3, 4, 2])  # 차원 순서 변경\n",
    "# #         print(video_frames.shape)\n",
    "# #         predictions = loaded_model.predict(video_frames)\n",
    "# #         predicted_class_index = np.argmax(predictions)\n",
    "# #         print(predicted_class_index, label)\n",
    "        \n",
    "#         convert_resize_video(video_path[0], output_video_path, )\n",
    "#         torch.cuda.empty_cache()\n",
    "#         save_path = 'saved_images'  # 저장 경로 \n",
    "#         video_frames = video_frames.squeeze(0)  # 배치 차원 제거\n",
    "# #         video_path = video_path[0]\n",
    "#         _perturbation_image(model, video_frames, label, output_video_path, save_path, transform_post, None, None, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import remotezip as rz\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "from IPython import display\n",
    "from urllib import request\n",
    "from tensorflow_docs.vis import embed\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def list_files_from_zip_url(zip_url):\n",
    "  files = []\n",
    "  with rz.RemoteZip(zip_url) as zip:\n",
    "    for zip_info in zip.infolist():\n",
    "      files.append(zip_info.filename)\n",
    "  return files\n",
    "\n",
    "#파일 이름에서 클래스 이름을 검색하는 get_class 함수\n",
    "def get_class(fname):\n",
    "  return fname.split('_')[-3]\n",
    "\n",
    "\n",
    "#모든 파일(위의 files) 목록을 각 클래스에 대한 파일을 나열하는 사전으로 변환하는 get_files_per_class 함수\n",
    "def get_files_per_class(files):\n",
    "  files_for_class = collections.defaultdict(list)\n",
    "  for fname in files:\n",
    "    class_name = get_class(fname)\n",
    "    files_for_class[class_name].append(fname)\n",
    "  return files_for_class\n",
    "\n",
    "# 비디오를 훈련, 검증 및 테스트 세트로 분할하는 헬퍼 함수\n",
    "# 비디오는 zip 파일로 URL에서 다운로드되어 각각의 하위 디렉터리에 배치됨\n",
    "def download_from_zip(zip_url, to_dir, file_names):\n",
    "  with rz.RemoteZip(zip_url) as zip:\n",
    "    for fn in tqdm.tqdm(file_names):\n",
    "      class_name = get_class(fn)\n",
    "      zip.extract(fn, str(to_dir / class_name))\n",
    "      unzipped_file = to_dir / class_name / fn\n",
    "\n",
    "      fn = pathlib.Path(fn).parts[-1]\n",
    "      output_file = to_dir / class_name / fn\n",
    "      unzipped_file.rename(output_file)\n",
    "\n",
    "# 데이터의 하위 집합에 아직 배정되지 않은 나머지 데이터를 리턴하는 함수\n",
    "def split_class_lists(files_for_class, count):\n",
    "  split_files = []\n",
    "  remainder = {}\n",
    "  for cls in files_for_class:\n",
    "    split_files.extend(files_for_class[cls][:count])\n",
    "    remainder[cls] = files_for_class[cls][count:]\n",
    "  return split_files, remainder\n",
    "\n",
    "# UCF101 데이터세트의 하위 집합을 다운로드하고 훈련, 검증 및 테스트 세트로 분할하는 함수\n",
    "# The splits argument allows you to pass in a dictionary in which the key values are the name of subset (example: \"train\") and the number of videos you would like to have per class.\n",
    "\n",
    "def download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n",
    "  files = list_files_from_zip_url(zip_url)\n",
    "  for f in files:\n",
    "    path = os.path.normpath(f)\n",
    "    tokens = path.split(os.sep)\n",
    "    if len(tokens) <= 2:\n",
    "      files.remove(f) # Remove that item from the list if it does not have a filename\n",
    "\n",
    "  files_for_class = get_files_per_class(files)\n",
    "\n",
    "  classes = list(files_for_class.keys())[:num_classes]\n",
    "\n",
    "  for cls in classes:\n",
    "    random.shuffle(files_for_class[cls])\n",
    "\n",
    "  # Only use the number of classes you want in the dictionary\n",
    "  files_for_class = {x: files_for_class[x] for x in classes}\n",
    "\n",
    "  dirs = {}\n",
    "  for split_name, split_count in splits.items():\n",
    "    print(split_name, \":\")\n",
    "    split_dir = download_dir / split_name\n",
    "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
    "    download_from_zip(zip_url, split_dir, split_files)\n",
    "    dirs[split_name] = split_dir\n",
    "\n",
    "  return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_CLASSES = 101\n",
    "# FILES_PER_CLASS = 30\n",
    "# URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'\n",
    "# download_dir = pathlib.Path('/data_1/seclab_nahyun/UCF101_subset/')\n",
    "# subset_paths = download_ufc_101_subset(URL,\n",
    "#                                        num_classes = NUM_CLASSES,\n",
    "#                                        splits = {\"train\": 30, \"val\": 10, \"test\": 10},\n",
    "#                                        download_dir = download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': PosixPath('/data_1/seclab_nahyun/UCF101_subset/train'), 'val': PosixPath('/data_1/seclab_nahyun/UCF101_subset/val'), 'test': PosixPath('/data_1/seclab_nahyun/UCF101_subset/test')}\n",
      "Total videos: 5388\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "download_dir = pathlib.Path('/data_1/seclab_nahyun/UCF101_subset/')\n",
    "subset_paths = {'train': pathlib.Path('/data_1/seclab_nahyun/UCF101_subset/train'), 'val': pathlib.Path('/data_1/seclab_nahyun/UCF101_subset/val'), 'test': pathlib.Path('/data_1/seclab_nahyun/UCF101_subset/test')}\n",
    "print(subset_paths)\n",
    "\n",
    "# 데이터 다운로드 완료! 이제 UCF101 데이터세트의 하위 집합 복사을 만들자!\n",
    "# 다음 코드를 실행하여 모든 데이터 하위 집합 사이에 가지고 있는 총 비디오 수를 프린트\n",
    "\n",
    "video_count_train = len(list(download_dir.glob('train/*/*.avi')))\n",
    "video_count_val = len(list(download_dir.glob('val/*/*.avi')))\n",
    "video_count_test = len(list(download_dir.glob('test/*/*.avi')))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #12 frame 간격으로 프레임 저장\n",
    "# def format_frames(frame, output_size):\n",
    "#     frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "#     frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "#     return frame\n",
    "\n",
    "         \n",
    "# def frames_from_video_file(video_path, n_frames, output_size = (256, 256), frame_step = 12):\n",
    "#   \"\"\"\n",
    "#     Creates frames from each video file present for each category.\n",
    "\n",
    "#     Args:\n",
    "#       video_path: File path to the video.\n",
    "#       n_frames: Number of frames to be created per video file.\n",
    "#       output_size: Pixel size of the output frame image.\n",
    "\n",
    "#     Return:\n",
    "#       An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "#   \"\"\"\n",
    "#   # Read each video frame by frame\n",
    "#   result = []\n",
    "#   src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "#   video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "#   need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "#   if need_length > video_length:\n",
    "#     start = 0\n",
    "#   else:\n",
    "#     max_start = video_length - need_length\n",
    "#     start = random.randint(0, max_start + 1)\n",
    "\n",
    "#   src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "#   # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "#   ret, frame = src.read()\n",
    "#   result.append(format_frames(frame, output_size))\n",
    "\n",
    "#   for _ in range(n_frames - 1):\n",
    "#     for _ in range(frame_step):\n",
    "#       ret, frame = src.read()\n",
    "#     if ret:\n",
    "#       frame = format_frames(frame, output_size)\n",
    "#       result.append(frame)\n",
    "#     else:\n",
    "#       result.append(np.zeros_like(result[0]))\n",
    "#   src.release()\n",
    "#   result = np.array(result)[..., [2, 1, 0]]\n",
    "# #   print(result)\n",
    "\n",
    "#   return result\n",
    "\n",
    "# class FrameGenerator:\n",
    "#   def __init__(self, path, n_frames):\n",
    "#     \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "#       Args:\n",
    "#         path: Video file paths.\n",
    "#         classes: List of labels for classification.\n",
    "#     \"\"\"\n",
    "#     self.path = path\n",
    "#     self.n_frames = n_frames\n",
    "#     self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "#     self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "#   def get_files_and_class_names(self):\n",
    "#     video_paths = list(self.path.glob('*/*.avi'))\n",
    "#     classes = [p.parent.name for p in video_paths] \n",
    "#     return video_paths, classes\n",
    "\n",
    "#   def __call__(self):\n",
    "#     video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "#     pairs = list(zip(video_paths, classes))\n",
    "\n",
    "#     random.shuffle(pairs)\n",
    "\n",
    "#     for path, name in pairs:\n",
    "#       video_frames = frames_from_video_file(path, self.n_frames) \n",
    "#       label = self.class_ids_for_name[name] # Encode labels\n",
    "#       yield video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def format_frames(frame, output_size):\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    return frame\n",
    "\n",
    "\n",
    "# 이전에 정의한 format_frames 함수와 FrameGenerator 클래스를 그대로 사용합니다.\n",
    "def frames_from_video_file(video_path, output_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    동영상 파일에서 모든 프레임을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        video_path: 동영상 파일 경로.\n",
    "        output_size: 프레임 이미지의 크기.\n",
    "\n",
    "    Returns:\n",
    "        모든 프레임으로 이루어진 NumPy 배열과 video_path.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = src.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = format_frames(frame, output_size)\n",
    "        result.append(frame)\n",
    "\n",
    "    src.release()\n",
    "    result = np.array(result)[..., [2, 1, 0]]\n",
    "#     print(result)\n",
    "\n",
    "    return result, str(video_path)  # video_path도 반환합니다.\n",
    "\n",
    "# 이제 frames_from_video_file 함수를 사용하여 모든 프레임을 생성하도록 FrameGenerator 클래스를 수정합니다.\n",
    "class FrameGenerator:\n",
    "    def __init__(self, path):\n",
    "        \"\"\" 레이블링된 동영상 파일의 프레임을 생성합니다.\n",
    "\n",
    "        Args:\n",
    "            path: 동영상 파일 경로.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "        self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "    def get_files_and_class_names(self):\n",
    "        video_paths = list(self.path.glob('*/*.avi'))\n",
    "        classes = [p.parent.name for p in video_paths]\n",
    "        return video_paths, classes\n",
    "\n",
    "    def __call__(self):\n",
    "        video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "        pairs = list(zip(video_paths, classes))\n",
    "\n",
    "        random.shuffle(pairs)\n",
    "\n",
    "        for path, name in pairs:\n",
    "            video_frames, video_path = frames_from_video_file(path)\n",
    "            label = self.class_ids_for_name[name]  # 레이블 인코딩\n",
    "            yield video_frames, label, video_path  # video_path도 반환합니다.\n",
    "           \n",
    "        \n",
    "# Create the training set\n",
    "output_signature = (tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(), dtype=tf.int16),\n",
    "                    tf.TensorSpec(shape=(), dtype=tf.string))  # video_path를 추가\n",
    "batch_size = 1\n",
    "# Create the test set\n",
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test']), output_signature=output_signature)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# test_ds = test_ds.shuffle(1000).batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE) 이거 개오래걸림\n",
    "\n",
    "test_ds = test_ds.batch(batch_size).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ㅁㄴㅇㄹ')\n",
    "\n",
    "test_frames, test_labels, video_path = next(iter(test_ds))\n",
    "# count = 0\n",
    "# with tf.device(\"/GPU:0\"):\n",
    "#     for test_frames, test_labels, video_path in test_ds:\n",
    "#         if test_frames is None:\n",
    "#             continue\n",
    "#         count += 1\n",
    "#         if count == 2:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 모델 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# # 각 배치에 대한 예측 수행\n",
    "# print(test_frames.shape, test_labels, video_path)\n",
    "# with tf.device(\"/GPU:0\"):\n",
    "#     predictions = loaded_model.predict(test_frames)\n",
    "#     predicted_class_index = np.argmax(predictions)\n",
    "#     print(predicted_class_index)\n",
    "\n",
    "save_path = 'saved_images'  # 저장 경로 \n",
    "video_frames = tf.transpose(test_frames, perm=[0, 1, 4, 2, 3])\n",
    "video_frames = torch.tensor(video_frames[0].numpy())  # 배치 차원 제거\n",
    "label = torch.tensor(test_labels.numpy())\n",
    "# input_video_path = video_path.numpy()[0].decode()\n",
    "input_video_path = video_path[0].numpy().decode('utf-8')\n",
    "print(video_frames.shape, label, input_video_path)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "output_video_path = \"./resized_video.mp4\"  # 출력 비디오 파일 경로\n",
    "target_size = (256, 256)  # 목표 크기\n",
    "convert_resize_video(input_video_path, output_video_path)\n",
    "\n",
    "_perturbation_image(loaded_model, video_frames, label, output_video_path, save_path, None, None, None, device)\n",
    "\n",
    "## 거꾸로 해도 잘 됨!\n",
    "# video_frames=tf.constant(test_frames.numpy())\n",
    "# # video_frames = tf.transpose(video_frames, perm=[0, 1, 3, 4, 2])  # 차원 순서 변경\n",
    "# print(video_frames.shape)\n",
    "# with tf.device(\"/GPU:0\"):\n",
    "#     predictions = loaded_model.predict(video_frames)\n",
    "#     predicted_class_index = np.argmax(predictions)\n",
    "#     print(predicted_class_index, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "# !kill -9 130003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                                 Type                          Data/Info\n",
      "--------------------------------------------------------------------------------\n",
      "AUTOTUNE                                 int                           -1\n",
      "CWLoss                                   function                      <function CWLoss at 0x7f38e87e7130>\n",
      "Cross_Entropy                            function                      <function Cross_Entropy at 0x7f38e87e71c0>\n",
      "Dataset                                  type                          <class 'torch.utils.data.dataset.Dataset'>\n",
      "F                                        module                        <module 'torch.nn.functio<...>/torch/nn/functional.py'>\n",
      "FF                                       module                        <module 'mxnet.ndarray' f<...>net/ndarray/__init__.py'>\n",
      "FrameGenerator                           type                          <class '__main__.FrameGenerator'>\n",
      "GOP_SIZE                                 int                           12\n",
      "Image                                    module                        <module 'PIL.Image' from <...>e-packages/PIL/Image.py'>\n",
      "ModelBuilder                             type                          <class 'mit_semseg.models.models.ModelBuilder'>\n",
      "SegmentationModule                       type                          <class 'mit_semseg.models<...>dels.SegmentationModule'>\n",
      "T                                        module                        <module 'torchvision.tran<...>/transforms/__init__.py'>\n",
      "ToTensor                                 type                          <class 'torchvision.trans<...>rms.transforms.ToTensor'>\n",
      "UCF101Dataset                            type                          <class '__main__.UCF101Dataset'>\n",
      "apply_motion_vector_to_iframe_tensor     function                      <function apply_motion_ve<...>tensor at 0x7f3d6839e560>\n",
      "apply_motion_vectors_to_all_iframes      function                      <function apply_motion_ve<...>frames at 0x7f3d6839e5f0>\n",
      "argparse                                 module                        <module 'argparse' from '<...>/python3.10/argparse.py'>\n",
      "as_numpy                                 function                      <function as_numpy at 0x7f3c396b9bd0>\n",
      "async_copy_to                            function                      <function async_copy_to at 0x7f3c522be830>\n",
      "batch_size                               int                           1\n",
      "cap                                      VideoCapture                  < cv2.VideoCapture 0x7f3c523a2070>\n",
      "check_video_with_ffmpeg                  function                      <function check_video_wit<...>ffmpeg at 0x7f38e87e6dd0>\n",
      "collections                              module                        <module 'collections' fro<...>collections/__init__.py'>\n",
      "compute_motion_vectors_and_weight_maps   function                      <function compute_motion_<...>t_maps at 0x7f3c521e8790>\n",
      "convert_resize_video                     function                      <function convert_resize_video at 0x7f3d836d9990>\n",
      "copy                                     module                        <module 'copy' from '/hom<...>/lib/python3.10/copy.py'>\n",
      "create_mask_for_stuff_pixels             function                      <function create_mask_for<...>pixels at 0x7f3c521ebd00>\n",
      "custom_transform                         function                      <function custom_transform at 0x7f38e87e76d0>\n",
      "cv2                                      module                        <module 'cv2' from '/home<...>ackages/cv2/__init__.py'>\n",
      "display                                  module                        <module 'IPython.display'<...>ages/IPython/display.py'>\n",
      "download_dir                             PosixPath                     /data_1/seclab_nahyun/UCF101_subset\n",
      "download_from_zip                        function                      <function download_from_zip at 0x7f38aaaa9990>\n",
      "download_ufc_101_subset                  function                      <function download_ufc_10<...>subset at 0x7f38aaaa9ab0>\n",
      "embed                                    module                        <module 'tensorflow_docs.<...>rflow_docs/vis/embed.py'>\n",
      "format_frames                            function                      <function format_frames at 0x7f38aaaaa950>\n",
      "frames_from_video_file                   function                      <function frames_from_vid<...>o_file at 0x7f38aaaaa4d0>\n",
      "get_class                                function                      <function get_class at 0x7f38aae2a950>\n",
      "get_files_per_class                      function                      <function get_files_per_class at 0x7f38aaaa9900>\n",
      "get_model                                function                      <function get_model at 0x7f38e8b5cb80>\n",
      "get_num_frames                           builtin_function_or_method    <built-in function get_num_frames>\n",
      "glob                                     module                        <module 'glob' from '/hom<...>/lib/python3.10/glob.py'>\n",
      "height                                   int                           256\n",
      "imageio                                  module                        <module 'imageio' from '/<...>ges/imageio/__init__.py'>\n",
      "input_video_path                         str                           /data_1/seclab_nahyun/UCF<...>BaseballPitch_g11_c04.avi\n",
      "itertools                                module                        <module 'itertools' (built-in)>\n",
      "json                                     module                        <module 'json' from '/hom<...>on3.10/json/__init__.py'>\n",
      "list_files_from_zip_url                  function                      <function list_files_from<...>ip_url at 0x7f38e86fe290>\n",
      "load                                     builtin_function_or_method    <built-in function load>\n",
      "loaded_model                             Sequential                    <keras.engine.sequential.<...>object at 0x7f38e8440490>\n",
      "math                                     module                        <module 'math' from '/hom<...>310-x86_64-linux-gnu.so'>\n",
      "mx                                       module                        <module 'mxnet' from '/ho<...>kages/mxnet/__init__.py'>\n",
      "nd                                       module                        <module 'mxnet.ndarray' f<...>net/ndarray/__init__.py'>\n",
      "nn                                       module                        <module 'torch.nn' from '<...>es/torch/nn/__init__.py'>\n",
      "norm2                                    function                      <function norm2 at 0x7f38e87e7250>\n",
      "normalization                            function                      <function normalization at 0x7f38e87e7010>\n",
      "normalize_grid                           function                      <function normalize_grid at 0x7f3d6839e4d0>\n",
      "np                                       module                        <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "os                                       module                        <module 'os' from '/home/<...>ce/lib/python3.10/os.py'>\n",
      "output_signature                         tuple                         n=3\n",
      "output_video_path                        str                           ./resized_video.mp4\n",
      "pathlib                                  module                        <module 'pathlib' from '/<...>b/python3.10/pathlib.py'>\n",
      "pd                                       module                        <module 'pandas' from '/h<...>ages/pandas/__init__.py'>\n",
      "pdb                                      module                        <module 'pdb' from '/home<...>e/lib/python3.10/pdb.py'>\n",
      "perform_segmentation_with_interval       function                      <function perform_segment<...>terval at 0x7f3c521ebd90>\n",
      "pickle                                   module                        <module 'pickle' from '/h<...>ib/python3.10/pickle.py'>\n",
      "plt                                      module                        <module 'matplotlib.pyplo<...>es/matplotlib/pyplot.py'>\n",
      "random                                   module                        <module 'random' from '/h<...>ib/python3.10/random.py'>\n",
      "read_video                               function                      <function read_video at 0x7f3c396e3d90>\n",
      "request                                  module                        <module 'urllib.request' <...>n3.10/urllib/request.py'>\n",
      "rz                                       module                        <module 'remotezip' from <...>e-packages/remotezip.py'>\n",
      "save_images_with_imageio                 function                      <function save_images_wit<...>mageio at 0x7f38e87e6f80>\n",
      "shutil                                   module                        <module 'shutil' from '/h<...>ib/python3.10/shutil.py'>\n",
      "split_and_load                           function                      <function split_and_load at 0x7f38e8be2dd0>\n",
      "split_class_lists                        function                      <function split_class_lists at 0x7f38aaaa9a20>\n",
      "subprocess                               module                        <module 'subprocess' from<...>ython3.10/subprocess.py'>\n",
      "subset_paths                             dict                          n=3\n",
      "sys                                      module                        <module 'sys' (built-in)>\n",
      "test_ds                                  PrefetchDataset               <PrefetchDataset element_<...>e=tf.string, name=None))>\n",
      "test_frames                              EagerTensor                   tf.Tensor(\\n[[[[[0. 0. 0.<...>, 256, 3), dtype=float32)\n",
      "test_labels                              EagerTensor                   tf.Tensor([26], shape=(1,), dtype=int16)\n",
      "tf                                       module                        <module 'tensorflow' from<...>/tensorflow/__init__.py'>\n",
      "time                                     module                        <module 'time' (built-in)>\n",
      "torch                                    module                        <module 'torch' from '/ho<...>kages/torch/__init__.py'>\n",
      "torchvision                              module                        <module 'torchvision' fro<...>torchvision/__init__.py'>\n",
      "total_frames                             int                           57\n",
      "tqdm                                     module                        <module 'tqdm' from '/hom<...>ckages/tqdm/__init__.py'>\n",
      "transform_input_to_mxnet                 function                      <function transform_input<...>_mxnet at 0x7f3d6839ee60>\n",
      "transform_post                           VideoNormalize                VideoNormalize(\\n\\n)\n",
      "tsn_mp_batchify_fn                       function                      <function tsn_mp_batchify_fn at 0x7f38e8c25b40>\n",
      "user_scattered_collate                   function                      <function user_scattered_<...>ollate at 0x7f3c522be950>\n",
      "validate_video_file                      function                      <function validate_video_file at 0x7f38e87e6ef0>\n",
      "video                                    module                        <module 'gluoncv.data.tra<...>ata/transforms/video.py'>\n",
      "video_count_test                         int                           1094\n",
      "video_count_train                        int                           3200\n",
      "video_count_val                          int                           1094\n",
      "video_path                               EagerTensor                   tf.Tensor([b'/data_1/secl<...>shape=(1,), dtype=string)\n",
      "video_total                              int                           5388\n",
      "visualize_image                          function                      <function visualize_image at 0x7f3d6839e0e0>\n",
      "visualize_images                         function                      <function visualize_images at 0x7f3d6839e170>\n",
      "visualize_motion_vector                  function                      <function visualize_motio<...>vector at 0x7f3c521e8700>\n",
      "visualize_motion_vectors                 function                      <function visualize_motio<...>ectors at 0x7f3d6839dfc0>\n",
      "visualize_noise_frames                   function                      <function visualize_noise<...>frames at 0x7f3d6839e200>\n",
      "visualize_tensor_image                   function                      <function visualize_tenso<...>_image at 0x7f3d6839dbd0>\n",
      "visualize_video                          function                      <function visualize_video at 0x7f3d6839e290>\n",
      "visualize_warped_frames                  function                      <function visualize_warpe<...>frames at 0x7f3d6839e050>\n",
      "width                                    int                           256\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the training set\n",
    "# output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "#                     tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "# train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], 10),\n",
    "#                                           output_signature = output_signature)\n",
    "# # Create the validation set\n",
    "# val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], 10),\n",
    "#                                         output_signature = output_signature)\n",
    "# # Create the test set\n",
    "# test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], 50), output_signature=output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "# val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "# test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set of frames: (2, 10, 256, 256, 3)\n",
      "Shape of training labels: (2,)\n",
      "Shape of validation set of frames: (2, 10, 256, 256, 3)\n",
      "Shape of validation labels: (2,)\n"
     ]
    }
   ],
   "source": [
    "# train_ds = train_ds.batch(2)\n",
    "# val_ds = val_ds.batch(2)\n",
    "# test_ds = test_ds.batch(1)\n",
    "\n",
    "# train_frames, train_labels = next(iter(train_ds))\n",
    "# print(f'Shape of training set of frames: {train_frames.shape}')\n",
    "# print(f'Shape of training labels: {train_labels.shape}')\n",
    "\n",
    "# val_frames, val_labels = next(iter(val_ds))\n",
    "# print(f'Shape of validation set of frames: {val_frames.shape}')\n",
    "# print(f'Shape of validation labels: {val_labels.shape}')\n",
    "\n",
    "# test_frames, test_labels = next(iter(test_ds))\n",
    "# print(f'Shape of validation set of frames: {test_frames.shape}')\n",
    "# print(f'Shape of validation labels: {test_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1600/1600 [==============================] - 302s 143ms/step - loss: 1.9042 - accuracy: 0.5813 - val_loss: 0.8827 - val_accuracy: 0.7815\n",
      "Epoch 2/10\n",
      "1600/1600 [==============================] - 42s 26ms/step - loss: 0.4928 - accuracy: 0.8984 - val_loss: 0.6788 - val_accuracy: 0.8300\n",
      "Epoch 3/10\n",
      "1600/1600 [==============================] - 42s 26ms/step - loss: 0.2396 - accuracy: 0.9578 - val_loss: 0.5745 - val_accuracy: 0.8510\n",
      "Epoch 4/10\n",
      "1600/1600 [==============================] - 42s 26ms/step - loss: 0.1292 - accuracy: 0.9837 - val_loss: 0.5367 - val_accuracy: 0.8528\n",
      "Epoch 5/10\n",
      "1600/1600 [==============================] - 42s 26ms/step - loss: 0.0737 - accuracy: 0.9931 - val_loss: 0.5295 - val_accuracy: 0.8583\n",
      "Epoch 6/10\n",
      "1600/1600 [==============================] - 42s 26ms/step - loss: 0.0414 - accuracy: 0.9978 - val_loss: 0.5050 - val_accuracy: 0.8684\n",
      "Epoch 7/10\n",
      "1600/1600 [==============================] - 42s 26ms/step - loss: 0.0278 - accuracy: 0.9984 - val_loss: 0.5259 - val_accuracy: 0.8720\n",
      "Epoch 8/10\n",
      "1600/1600 [==============================] - 42s 26ms/step - loss: 0.0162 - accuracy: 0.9994 - val_loss: 0.5274 - val_accuracy: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f45104d8d30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net = tf.keras.applications.EfficientNetB0(include_top = False)\n",
    "# net.trainable = False\n",
    "\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Rescaling(scale=255),\n",
    "#     tf.keras.layers.TimeDistributed(net),\n",
    "#     tf.keras.layers.Dense(101),\n",
    "#     tf.keras.layers.GlobalAveragePooling3D()\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer = 'adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(train_ds, \n",
    "#           epochs = 10,\n",
    "#           validation_data = val_ds,\n",
    "#           callbacks = tf.keras.callbacks.EarlyStopping(patience = 2, monitor = 'val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "# # 모델을 학습시킨 후에 저장하려면 다음과 같이 save_model 함수를 호출합니다.\n",
    "# tf.keras.models.save_model(\n",
    "#     model,  # 저장할 모델 객체\n",
    "#     'my_model',  # 모델을 저장할 디렉토리 또는 파일 경로 지정\n",
    "#     overwrite=True,  # 같은 이름의 파일이 이미 있는 경우 덮어쓸지 여부\n",
    "#     include_optimizer=True  # 옵티마이저를 포함할지 여부 (True로 설정하면 학습 상태도 저장됨)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x7f3ae0539780>\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 31, Predicted Label: 71\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 2, Predicted Label: 2\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 94, Predicted Label: 94\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 71, Predicted Label: 71\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 53, Predicted Label: 21\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 69, Predicted Label: 47\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 93, Predicted Label: 47\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 82, Predicted Label: 71\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 9, Predicted Label: 9\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 80, Predicted Label: 80\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 82, Predicted Label: 82\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 59, Predicted Label: 98\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 4, Predicted Label: 8\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 94, Predicted Label: 94\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 35, Predicted Label: 47\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 86, Predicted Label: 8\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 9, Predicted Label: 9\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 25, Predicted Label: 25\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 92, Predicted Label: 21\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 73, Predicted Label: 73\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 80, Predicted Label: 80\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 36, Predicted Label: 36\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 66, Predicted Label: 66\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 13, Predicted Label: 13\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 90, Predicted Label: 78\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 58, Predicted Label: 58\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 17, Predicted Label: 71\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 7, Predicted Label: 8\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 32, Predicted Label: 30\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 74, Predicted Label: 47\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 17, Predicted Label: 17\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 27, Predicted Label: 8\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 68, Predicted Label: 68\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 43, Predicted Label: 43\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 18, Predicted Label: 18\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 39, Predicted Label: 71\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 39, Predicted Label: 39\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 65, Predicted Label: 65\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 56, Predicted Label: 56\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 5, Predicted Label: 5\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 90, Predicted Label: 47\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 12, Predicted Label: 12\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 84, Predicted Label: 84\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 69, Predicted Label: 47\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 27, Predicted Label: 4\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 82, Predicted Label: 82\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 4, Predicted Label: 4\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 51, Predicted Label: 14\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 9, Predicted Label: 9\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 98, Predicted Label: 98\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 4, Predicted Label: 4\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 49, Predicted Label: 49\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 81, Predicted Label: 81\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 87, Predicted Label: 87\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 60, Predicted Label: 60\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 6, Predicted Label: 78\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 5, Predicted Label: 8\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 58, Predicted Label: 58\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 0, Predicted Label: 19\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 71, Predicted Label: 71\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 18, Predicted Label: 8\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 71, Predicted Label: 71\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 46, Predicted Label: 46\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 38, Predicted Label: 13\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 93, Predicted Label: 93\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 26, Predicted Label: 26\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 1, Predicted Label: 1\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 80, Predicted Label: 80\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 62, Predicted Label: 62\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 0, Predicted Label: 0\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 93, Predicted Label: 93\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 78, Predicted Label: 78\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 15, Predicted Label: 8\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 89, Predicted Label: 14\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 45, Predicted Label: 45\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 76, Predicted Label: 47\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 97, Predicted Label: 97\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 37, Predicted Label: 30\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 63, Predicted Label: 63\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 100, Predicted Label: 42\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 44, Predicted Label: 71\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 85, Predicted Label: 85\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 10, Predicted Label: 10\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 24, Predicted Label: 13\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 21, Predicted Label: 21\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 95, Predicted Label: 4\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 23, Predicted Label: 22\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 2, Predicted Label: 2\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 10, Predicted Label: 10\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 39, Predicted Label: 71\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 72, Predicted Label: 72\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 19, Predicted Label: 19\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 5, Predicted Label: 8\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 84, Predicted Label: 84\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 83, Predicted Label: 32\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 99, Predicted Label: 99\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 8, Predicted Label: 8\n",
      "(1, 50, 224, 224, 3)\n",
      "Actual Label: 64, Predicted Label: 64\n",
      "(1, 50, 224, 224, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(video_frames\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     23\u001b[0m video_frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(video_frames)  \u001b[38;5;66;03m# Convert to NumPy array if necessary\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_single_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Print actual vs. predicted labels\u001b[39;00m\n\u001b[1;32m     27\u001b[0m print_actual_vs_predicted_labels(video_labels, predictions)\n",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m, in \u001b[0;36mpredict_single_video\u001b[0;34m(video_frames)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_single_video\u001b[39m(video_frames):\n\u001b[0;32m----> 6\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/keras/engine/training.py:1951\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1944\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1945\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1946\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1947\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTPUStrategy and AutoShardPolicy.FILE might lead to out-of-order \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1948\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1949\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m-> 1951\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1954\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1955\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1959\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1961\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/keras/engine/data_adapter.py:1399\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_coordinator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/keras/engine/data_adapter.py:1149\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1148\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/keras/engine/data_adapter.py:236\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    226\u001b[0m              x,\n\u001b[1;32m    227\u001b[0m              y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m              shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    234\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    235\u001b[0m   \u001b[38;5;28msuper\u001b[39m(TensorLikeDataAdapter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 236\u001b[0m   x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43m_process_tensorlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m   sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    238\u001b[0m       sample_weights, sample_weight_modes)\n\u001b[1;32m    240\u001b[0m   \u001b[38;5;66;03m# If sample_weights are not specified for an output use 1.0 as weights.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/keras/engine/data_adapter.py:1043\u001b[0m, in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n\u001b[1;32m   1041\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m-> 1043\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_convert_single_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mlist_to_tuple(inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/util/nest.py:914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/keras/engine/data_adapter.py:1038\u001b[0m, in \u001b[0;36m_process_tensorlike.<locals>._convert_single_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, np\u001b[38;5;241m.\u001b[39mfloating):\n\u001b[1;32m   1037\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mfloatx()\n\u001b[0;32m-> 1038\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_scipy_sparse(x):\n\u001b[1;32m   1040\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _scipy_sparse_to_sparse_tensor(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1559\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m   1498\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1559\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1565\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1565\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m      \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1695\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1691\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1692\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1695\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1698\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loaded_model = tf.keras.models.load_model('my_model')\n",
    "\n",
    "# print(loaded_model)\n",
    "# # Function to make predictions on a single video\n",
    "# def predict_single_video(video_frames):\n",
    "#     predictions = loaded_model.predict(video_frames)\n",
    "#     return predictions\n",
    "\n",
    "# # Function to print actual and predicted labels\n",
    "# def print_actual_vs_predicted_labels(video_labels, predictions):\n",
    "#     actual_label = video_labels.numpy()[0]\n",
    "#     predicted_label = np.argmax(predictions)\n",
    "#     print(f\"Actual Label: {actual_label}, Predicted Label: {predicted_label}\")\n",
    "\n",
    "# # Create a generator for the test dataset to process videos one by one\n",
    "# def single_video_generator(test_ds):\n",
    "#     for video_frames, label in test_ds:\n",
    "#         yield video_frames, label\n",
    "\n",
    "# # Iterate through the test dataset and make predictions for each video\n",
    "# for video_frames, video_labels in single_video_generator(test_ds):\n",
    "#     print(video_frames.shape)\n",
    "#     video_frames = np.array(video_frames)  # Convert to NumPy array if necessary\n",
    "#     predictions = predict_single_video(video_frames)\n",
    "    \n",
    "#     # Print actual vs. predicted labels\n",
    "#     print_actual_vs_predicted_labels(video_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user     :0           2023-10-06 18:08 (:0)\r\n",
      "seclab_nahyun pts/0        2023-10-25 22:55 (166.104.245.67)\r\n",
      "seclab_gagyeom pts/6        2023-10-24 17:48 (166.104.245.67)\r\n",
      "seclab_gagyeom pts/7        2023-10-24 17:49 (166.104.245.67)\r\n"
     ]
    }
   ],
   "source": [
    "!who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ace_nahyun/workspace/kinetics-dataset/test_ds/annotations/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 284\u001b[0m\n\u001b[1;32m    279\u001b[0m         correct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(pred \u001b[38;5;241m==\u001b[39m target)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(target)\n\u001b[0;32m--> 284\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mKineticsTest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/ace_nahyun/workspace/kinetics-dataset/test_ds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    288\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:2\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 204\u001b[0m, in \u001b[0;36mKineticsTest.__init__\u001b[0;34m(self, root, clip_len)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_len \u001b[38;5;241m=\u001b[39m clip_len\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/ace_nahyun/workspace/pytorch-resnet3d/data/kinetics_data.pth\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnnotations created!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    207\u001b[0m annotations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/ace_nahyun/workspace/pytorch-resnet3d/data/kinetics_data.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 242\u001b[0m, in \u001b[0;36mKineticsTest.parse_annotations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data, labels\n\u001b[1;32m    241\u001b[0m frame_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/frames/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot\n\u001b[0;32m--> 242\u001b[0m val_data, labels \u001b[38;5;241m=\u001b[39m \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m/annotations/test.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m annotations \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_data\u001b[39m\u001b[38;5;124m'\u001b[39m: val_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: labels}\n\u001b[1;32m    244\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(annotations, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/ace_nahyun/workspace/pytorch-resnet3d/data/kinetics_data.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 219\u001b[0m, in \u001b[0;36mKineticsTest.parse_annotations.<locals>.parse\u001b[0;34m(annotation_csv)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(annotation_csv):\n\u001b[0;32m--> 219\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    220\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m annotations]\n\u001b[1;32m    221\u001b[0m     clip_labels, yt_ids, start_times, end_times, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mannotations)\n",
      "File \u001b[0;32m~/anaconda3/envs/workspace/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ace_nahyun/workspace/kinetics-dataset/test_ds/annotations/test.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "import collections\n",
    "import torchnet as tnt\n",
    "import sys\n",
    "sys.path.append('./pytorch-resnet3d/')\n",
    "from models import resnet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import random\n",
    "from PIL import Image\n",
    "import numbers\n",
    "from utils import util\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('./semantic-segmentation-pytorch/')\n",
    "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
    "import time\n",
    "\n",
    "\n",
    "# visualize_video_frames_tf(tf.squeeze(test_frames, axis=0))\n",
    "\n",
    "\n",
    "# 모델 디바이스 설정\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "# # Load the pre-trained model\n",
    "# builder = ModelBuilder()\n",
    "# net_encoder = builder.build_encoder(\n",
    "#     arch='resnet50dilated',\n",
    "#     fc_dim=2048,\n",
    "#     weights='./semantic-segmentation-pytorch/ckpt/encoder_epoch_20.pth')\n",
    "# net_decoder = builder.build_decoder(\n",
    "#     arch='ppm_deepsup',\n",
    "#     fc_dim=2048,\n",
    "#     num_class=150,\n",
    "#     weights='./semantic-segmentation-pytorch/ckpt/decoder_epoch_20.pth',\n",
    "#     use_softmax=True)\n",
    "\n",
    "# crit = torch.nn.NLLLoss(ignore_index=-1)\n",
    "# segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
    "# segmentation_module.eval()\n",
    "# segmentation_module.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_cuda(batch, device):\n",
    "    _batch = {}\n",
    "    for k, v in batch.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            v = v.to(device)  # 모든 텐서를 cuda:2로 이동\n",
    "        elif isinstance(v, list) and isinstance(v[0], torch.Tensor):\n",
    "            v = [item.to(device) for item in v]  # 리스트 내의 텐서들을 cuda:2로 이동\n",
    "        _batch[k] = v\n",
    "    return _batch\n",
    "\n",
    "class GroupResize(object):\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        self.worker = torchvision.transforms.Resize(size, interpolation)\n",
    "        \n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "\n",
    "class GroupRandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        if not img_group:\n",
    "            return img_group  # 리스트가 비어 있으면 입력을 반환합니다.\n",
    "\n",
    "        w, h = img_group[0].size\n",
    "        th, tw = self.size\n",
    "\n",
    "        out_images = list()\n",
    "\n",
    "        x1 = random.randint(0, w - tw)\n",
    "        y1 = random.randint(0, h - th)\n",
    "\n",
    "        for img in img_group:\n",
    "            assert(img.size[0] == w and img.size[1] == h)\n",
    "            if w == tw and h == th:\n",
    "                out_images.append(img)\n",
    "            else:\n",
    "                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))\n",
    "\n",
    "        return out_images\n",
    "\n",
    "class GroupCenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.worker = torchvision.transforms.CenterCrop(size)\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        return [self.worker(img) for img in img_group]\n",
    "\n",
    "class GroupRandomHorizontalFlip(object):\n",
    "    def __call__(self, img_group):\n",
    "        if random.random() < 0.5:\n",
    "            img_group = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]\n",
    "        return img_group\n",
    "\n",
    "class GroupNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor): # (T, 3, 224, 224)\n",
    "        for b in range(tensor.size(0)):\n",
    "            for t, m, s in zip(tensor[b], self.mean, self.std):\n",
    "                t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "class LoopPad(object):\n",
    "\n",
    "    def __init__(self, max_len):\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        length = tensor.size(0)\n",
    "\n",
    "        if length==self.max_len:\n",
    "            return tensor\n",
    "\n",
    "        # repeat the clip as many times as is necessary\n",
    "        n_pad = self.max_len - length\n",
    "        pad = [tensor]*(n_pad//length)\n",
    "        if n_pad%length>0:\n",
    "            pad += [tensor[0:n_pad%length]]\n",
    "\n",
    "        tensor = torch.cat([tensor]+pad, 0)\n",
    "        return tensor\n",
    "\n",
    "# NOTE: Returns [0-255] rather than torchvision's [0-1]\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        self.worker = lambda x: F.to_tensor(x)*255\n",
    "\n",
    "    def __call__(self, img_group):\n",
    "        img_group = [self.worker(img) for img in img_group]\n",
    "        return torch.stack(img_group, 0)\n",
    "    \n",
    "def kinetics_mean_std():\n",
    "    mean = [114.75, 114.75, 114.75]\n",
    "    std = [57.375, 57.375, 57.375]\n",
    "    return mean, std\n",
    "\n",
    "def clip_transform(max_len):\n",
    "\n",
    "    mean, std = kinetics_mean_std()\n",
    "    transform = transforms.Compose([\n",
    "                GroupResize(256),\n",
    "                GroupCenterCrop(256),\n",
    "                ToTensor(),\n",
    "                GroupNormalize(mean, std),\n",
    "                LoopPad(max_len),\n",
    "        ])\n",
    "    return transform\n",
    "\n",
    "def extract_frames(video_path, num_frames):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 이미지로 변환\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "        frames.append(img_pil)\n",
    "\n",
    "        if len(frames) == num_frames:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "class KineticsTest(data.Dataset):\n",
    "\n",
    "    def __init__(self, root, clip_len):\n",
    "        super(KineticsTest, self).__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.clip_len = clip_len\n",
    "\n",
    "        if not os.path.exists('/home/ace_nahyun/workspace/pytorch-resnet3d/data/kinetics_data.pth'):\n",
    "            self.parse_annotations()\n",
    "            print('Annotations created!')\n",
    "\n",
    "        annotations = torch.load('/home/ace_nahyun/workspace/pytorch-resnet3d/data/kinetics_data.pth')\n",
    "        self.labels = annotations['labels']\n",
    "        self.test_data = annotations['val_data']\n",
    "        print('%d test clips' % len(self.test_data))\n",
    "        \n",
    "        self.clip_transform = clip_transform(self.clip_len)\n",
    "#         self.loader = lambda fl: Image.open(os.path.join(self.root, fl)).convert('RGB')\n",
    "        self.loader = lambda fl: os.path.join(self.root, fl)\n",
    "    \n",
    "    def parse_annotations(self):\n",
    "\n",
    "        def parse(annotation_csv):\n",
    "            annotations = open(annotation_csv, 'r').read().strip().split('\\n')[1:]\n",
    "            annotations = [line.split(',') for line in annotations]\n",
    "            clip_labels, yt_ids, start_times, end_times, _, _ = zip(*annotations)\n",
    "\n",
    "            labels = map(lambda l: l.strip('\"'), clip_labels)\n",
    "            labels = np.unique(list(labels)).tolist()\n",
    "\n",
    "            data = []\n",
    "            for yt_id, start, end, label in tqdm.tqdm(zip(yt_ids, start_times, end_times, clip_labels), total=len(yt_ids)):\n",
    "                label = label.strip('\"')\n",
    "                file_name = f\"{yt_id}_{int(start):06d}_{int(end):06d}.mp4\"\n",
    "                file_path = os.path.join(frame_dir, file_name)\n",
    "            \n",
    "                # 파일이 존재하는지 확인\n",
    "                if os.path.exists(file_path):\n",
    "                    data.append({'frames': file_path, 'label': labels.index(label)})\n",
    "                # else:\n",
    "                    # print(f\"파일을 찾을 수 없음: {file_path}\")\n",
    "\n",
    "            return data, labels\n",
    "\n",
    "\n",
    "        frame_dir = '%s/frames/' % self.root\n",
    "        val_data, labels = parse('%s/annotations/test.csv' % self.root)\n",
    "        annotations = {'val_data': val_data, 'labels': labels}\n",
    "        torch.save(annotations, '/home/ace_nahyun/workspace/pytorch-resnet3d/data/kinetics_data.pth')\n",
    "\n",
    "\n",
    "    def sample(self, video_path):\n",
    "        # video_path에서 직접 프레임을 추출하여 로드\n",
    "        frames = extract_frames(video_path, num_frames=self.clip_len)\n",
    "#         print(frames)\n",
    "\n",
    "        # 필요한 경우 중앙 정렬 등의 추가적인 프레임 선택 및 조정 로직 적용\n",
    "\n",
    "        # 이미지로드 및 변환\n",
    "#         imgs = [self.loader(frame) for frame in frames]\n",
    "\n",
    "        return frames\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        entry = self.test_data[index]\n",
    "        video_path = entry['frames']  # 비디오 경로 추가\n",
    "        frames = self.sample(video_path)\n",
    "        if frames is None or len(frames) == 0:\n",
    "            return self.__getitem__((index + 1) % len(self.test_data))  # 다음 인덱스로 이동\n",
    "        frames = self.clip_transform(frames)\n",
    "        frames = frames.permute(1, 0, 2, 3)  # (3, T, 224, 224)\n",
    "        instance = {'frames': frames, 'label': entry['label'], 'video_path': video_path}\n",
    "        return instance\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_data)\n",
    "\n",
    "def accuracy(output, target):\n",
    "    \"\"\"Computes the accuracy of the model\"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        assert pred.shape[0] == len(target)\n",
    "        correct = torch.sum(pred == target).item()\n",
    "    return correct / len(target)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = KineticsTest('/home/ace_nahyun/workspace/kinetics-dataset/test_ds', clip_len=32)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "save_path = 'saved_images'  # 저장 경로 \n",
    "    \n",
    "\n",
    "loaded_model = resnet.i3_res50(400).to(device)  # vanilla I3D ResNet50\n",
    "\n",
    "\n",
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    loaded_model.eval()\n",
    "    total_correct = 0\n",
    "    total_images = 0\n",
    "    loss_meters = collections.defaultdict(lambda: tnt.meter.AverageValueMeter())\n",
    "    \n",
    "\n",
    "    for idx, batch in enumerate(test_dataloader):\n",
    "        batch['frames'] = batch['frames'].requires_grad_(True)\n",
    "        batch = batch_cuda(batch, device)\n",
    "\n",
    "        pred, loss_dict = loaded_model(batch)\n",
    "        pred.requires_grad_(True)\n",
    "        loss_dict = {k: v.mean() for k, v in loss_dict.items() if v.numel() > 0}\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        for k, v in loss_dict.items():\n",
    "            loss_meters[k].add(v.item())\n",
    "            \n",
    "            \n",
    "            \n",
    "#         # 특정 클래스에 대한 로짓 및 그래디언트 계산\n",
    "#         target_label = batch['label'].item() \n",
    "#         target_logits = pred[range(pred.size(0)), target_label]\n",
    "#         target_logits.requires_grad_(True)\n",
    "#         loaded_model.zero_grad()\n",
    "#         target_logits.backward(retain_graph=True)\n",
    "\n",
    "#         # Grad-CAM 계산\n",
    "#         gradients = backward_feature[-1]\n",
    "#         b, k, u, v = gradients.shape\n",
    "\n",
    "#         alpha = gradients.mean(axis=(2, 3), keepdims=True)\n",
    "#         weights = alpha\n",
    "\n",
    "#         grad_cam_map = np.maximum((weights * activations).sum(axis=1), 0)\n",
    "#         grad_cam_map = grad_cam_map / np.max(grad_cam_map, axis=(1, 2), keepdims=True)\n",
    "\n",
    "#         # 시각화\n",
    "#         for frame_idx in range(grad_cam_map.shape[0]):\n",
    "#             heatmap = cv2.applyColorMap(np.uint8(255 * grad_cam_map[frame_idx]), cv2.COLORMAP_JET)\n",
    "#             heatmap = np.float32(heatmap) / 255\n",
    "#             frame = batch['frames'][0, :, frame_idx].cpu().numpy().transpose(1, 2, 0)\n",
    "#             frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
    "#             superimposed_img = heatmap + frame\n",
    "#             superimposed_img = np.clip(superimposed_img, 0, 1)\n",
    "#             plt.imshow(superimposed_img)\n",
    "#             plt.title(f\"Frame {frame_idx}\")\n",
    "#             plt.show()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        # 정확도 계산\n",
    "        acc = accuracy(pred, batch['label'])\n",
    "        total_correct += total_correct + acc * pred.size(0)\n",
    "        total_images += pred.size(0)\n",
    "\n",
    "        stats = ' | '.join(['%s: %.3f' % (k, v.value()[0]) for k, v in loss_meters.items()])\n",
    "        print('%d/%d.. %s | Accuracy: %.3f' % (idx, len(test_dataloader), stats, acc))\n",
    "        \n",
    "#         # 예측된 레이블과 실제 레이블 출력\n",
    "#         _, predicted_labels = torch.max(pred, 1)\n",
    "#         print(f\"Batch {idx}:\")\n",
    "#         print(f\"Predicted labels: {predicted_labels}\")\n",
    "#         print(f\"Actual labels: {batch['label']}\")\n",
    "        \n",
    "#         target_labels = batch['label'].to(device)\n",
    "#         target_logits = pred[range(pred.size(0)), target_labels[0]]\n",
    "#         print('target_labels: ', target_labels, 'target_logits', target_logits)\n",
    "#         target_logits.backward(retain_graph = True)\n",
    "        \n",
    "#         activations = feature_blobs[0].to(device) # (1, 512, 7, 7), forward activations\n",
    "#         gradients = backward_feature[0] # (1, 512, 7, 7), backward gradients\n",
    "#         b, k, u, v = gradients.size()\n",
    "\n",
    "#         alpha = gradients.view(b, k, -1).mean(2) # (1, 512, 7*7) => (1, 512), feature map k의 'importance'\n",
    "#         weights = alpha.view(b, k, 1, 1) # (1, 512, 1, 1)\n",
    "        \n",
    "        \n",
    "#         grad_cam_map = (weights*activations).sum(1, keepdim = True) # alpha * A^k = (1, 512, 7, 7) => (1, 1, 7, 7)\n",
    "#         grad_cam_map = F.relu(grad_cam_map) # Apply R e L U\n",
    "#         grad_cam_map = F.interpolate(grad_cam_map, size=(224, 224), mode='bilinear', align_corners=False) # (1, 1, 224, 224)\n",
    "#         map_min, map_max = grad_cam_map.min(), grad_cam_map.max()\n",
    "#         grad_cam_map = (grad_cam_map - map_min).div(map_max - map_min).data # (1, 1, 224, 224), min-max scaling\n",
    "        \n",
    "#         # grad_cam_map.squeeze() : (224, 224)\n",
    "#         grad_heatmap = cv2.applyColorMap(np.uint8(255 * grad_cam_map.squeeze().cpu()), cv2.COLORMAP_JET) # (224, 224, 3), numpy\n",
    "\n",
    "#         # Grad-CAM heatmap save\n",
    "#         cv2.imwrite(os.path.join(saved_loc, \"Grad_CAM_heatmap.jpg\"), grad_heatmap)\n",
    "\n",
    "#         grad_heatmap = np.float32(grad_heatmap) / 255\n",
    "\n",
    "#         grad_result = grad_heatmap + img\n",
    "#         grad_result = grad_result / np.max(grad_result)\n",
    "#         grad_result = np.uint8(255 * grad_result)\n",
    "\n",
    "#         # Grad-CAM Result save\n",
    "#         cv2.imwrite(os.path.join(saved_loc, \"Grad_Result.jpg\"), grad_result)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# #         print(batch['frames'].shape, batch['label'], batch['video_path'])\n",
    "# #         video_path = batch['video_path']\n",
    "# #         test_labels = batch['label'].to('cpu')\n",
    "# #         test_frames = batch['frames'].to('cpu') # [1, 3, 32, 256, 256]\n",
    "# #         video_frames = tf.transpose(test_frames, perm=[0,  2,  1, 3, 4]) # [1, 32, 3, 256, 256]\n",
    "# #         preprocessed_image = preprocess_images(video_frames[0])\n",
    "# #         #     print(preprocessed_image.shape, type(preprocessed_image))\n",
    "# #         video_frames = torch.tensor(preprocessed_image.numpy())\n",
    "\n",
    "# #         label = torch.tensor(test_labels.numpy())\n",
    "# #         input_video_path = video_path[0]\n",
    "# #         print(video_frames.shape, label, input_video_path)\n",
    "\n",
    "# #         output_video_path = \"/home/ace_nahyun/workspace/resized_video.mp4\"  # 출력 비디오 파일 경로\n",
    "# #         target_size = (256, 256)  # 목표 크기\n",
    "# #         convert_resize_video(input_video_path, output_video_path)\n",
    "        \n",
    "# #         video_frames=video_frames.to(device)\n",
    "\n",
    "# #         _perturbation_image(loaded_model, video_frames, label, output_video_path, save_path, None, None, None, device, segmentation_module)\n",
    "\n",
    "\n",
    "    overall_accuracy = total_correct / total_images\n",
    "    print('(test) Overall Accuracy: %.3f' % overall_accuracy)\n",
    "\n",
    "    # 종료 시간 기록 및 실행 시간 계산\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"코드 실행 시간: {elapsed_time}초\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readme.md  data  eval.py  models  utils\n"
     ]
    }
   ],
   "source": [
    "!ls ./pytorch-resnet3d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_kernel",
   "language": "python",
   "name": "workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
